{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb51e830-ee22-4fd9-81e6-a56821b35263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1b41d61-0065-433f-9405-a6fa1c1e410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for langchain\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "# from langchain_chroma import Chroma\n",
    "from langchain.vectorstores import FAISS\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79be252f-ad21-4da4-8010-73bca5ee890a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62f931ca-499d-4458-a194-ffd9332624d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for langchain, plotly and Chroma\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d90e8881-85e7-4739-833f-c8e55967a367",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL = Ollama(model=\"llama3.2\")\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "db_name = \"vector-database\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1a58298-27ce-45ef-96e5-c43b1595fc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = glob.glob(\"example/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92541fa7-d5ca-419d-8b72-12bfe1054884",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_loader_kwargs = {'encoding': 'utf-8'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0f18d07f-3d47-44da-af4b-c9b110c35aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'https://arxiv.org/pdf/2405.10825', 'title': 'Large Language Model (LLM) for Telecommunications: A Comprehensive Survey on Principles, Key Techniques, and Opportunities', 'time': '2023-10-17T14:00:32.000Z', 'tags': ['Large Language Model', 'Telecom', 'Article', '5G'], 'authors': ['Hao Zhou', 'Chengming Hu', 'Ye Yuan', 'Yufei Cui', 'Yili Jin']}\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "with jsonlines.open(\"articles-Copy1.json\",\"r\") as documents:\n",
    "    print(next(iter(documents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "95df87e8-e17b-48cf-9b96-ea2511b23b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "folders = glob.glob(\"example/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e6749ea0-4434-4fd8-9987-a1e2df2838e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_documents = []\n",
    "\n",
    "with jsonlines.open(\"articles-Copy1.json\", \"r\") as metadata_file:\n",
    "    for file_path, metadata in zip(folders, metadata_file):\n",
    "        try:\n",
    "            pdf_loader = PyPDFLoader(file_path)\n",
    "            pdf_documents = pdf_loader.load()\n",
    "\n",
    "            for pdf_doc in pdf_documents:\n",
    "            # Metadata'yı dökümana ekle\n",
    "                pdf_doc.metadata.update({\n",
    "                    \"url\": metadata.get(\"url\"),\n",
    "                    \"tags\":\", \".join(metadata.get(\"tags\")),\n",
    "                    \"title\": metadata.get(\"title\"),\n",
    "                    \"authors\":\", \".join(metadata.get(\"authors\"))\n",
    "                }\n",
    "                )\n",
    "\n",
    "            # Dökümanı listeye ekle\n",
    "                langchain_documents.append( Document(\n",
    "                        page_content=pdf_doc.page_content,\n",
    "                        metadata=pdf_doc.metadata\n",
    "                    )\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file {file_path}: {e}\")\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8dbaaabd-9ab6-4276-9b6a-bbc951660fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29,\n",
       " [Document(metadata={'source': 'example/5G Core Network_Study Paper_v8.pdf', 'page': 0, 'url': 'https://arxiv.org/pdf/2405.10825', 'tags': 'Large Language Model, Telecom, Article, 5G', 'title': 'Large Language Model (LLM) for Telecommunications: A Comprehensive Survey on Principles, Key Techniques, and Opportunities', 'authors': 'Hao Zhou, Chengming Hu, Ye Yuan, Yufei Cui, Yili Jin'}, page_content='MOBILE  DIVISION  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nSTUDY PAPER ON \\n \\n5G CORE NETWORK \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n©TEC \\n \\n \\n \\n \\nTELECOMMUNICATION ENGINEERING CENTRE \\nKHURSHID LAL BHAWAN, JANPATH \\nNEW DELHI - 110001 \\nINDIA \\n \\n ')])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(langchain_documents), langchain_documents[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8c113f17-aef5-4a14-9617-b8d9d09245ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Qdrant\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "31a2295e-64b2-42c8-a552-51c54edb74d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from langchain_ollama import OllamaEmbeddings\\n\\n# Load environment variables in a file called .env\\n\\nload_dotenv()\\nos.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\\nembeddings = OpenAIEmbeddings()\\n\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "embeddings = OpenAIEmbeddings()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "82b28acd-da9d-4c55-907c-728c238bb4a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "366"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 200, chunk_overlap = 20, separators=[])\n",
    "chunks = text_splitter.split_documents(langchain_documents)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aebab834-b05c-4193-b0cd-becb173d4e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 366 documents\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'def format_docs(docs):\\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\\n\\nchain = {\"docs\": format_docs} | prompt | model | StrOutputParser()\\n\\nquestion = \"Summarize this document\"\\n\\ndocs = vectorstore.similarity_search(question)\\n\\nchain.invoke(docs)\\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"llama3.2\")\n",
    "\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
    "\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")\n",
    "#vectorstore = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "# Convert loaded documents into strings by concatenating their content\n",
    "# and ignoring metadata\n",
    "\n",
    "\"\"\"def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "chain = {\"docs\": format_docs} | prompt | model | StrOutputParser()\n",
    "\n",
    "question = \"Summarize this document\"\n",
    "\n",
    "docs = vectorstore.similarity_search(question)\n",
    "\n",
    "chain.invoke(docs)\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e627aad5-fe2c-4adb-b8c5-48587c113328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vectors have 3,072 dimensions\n"
     ]
    }
   ],
   "source": [
    "# Get one vector and find how many dimensions it has\n",
    "\n",
    "collection = vectorstore._collection\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"The vectors have {dimensions:,} dimensions\")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "89cd5e4a-30eb-47c2-bdf6-f1599a7b08d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a5e97f69-9925-4e3b-a469-54670aad9df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nmodel = \"ollama-3.2\"\\n# create a new Chat \\nllm = ChatOllama(temperature=0.7, model_name=MODEL)\\n\\n# set up the conversation memory for the chat\\nmemory = ConversationBufferMemory(memory_key=\\'chat_history\\', return_messages=True)\\n\\n# the retriever is an abstraction over the VectorStore that will be used during RAG\\nretriever = vectorstore.as_retriever()\\n\\n# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory\\nconversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)\\n'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "retriever = vectorstore.as_retriever\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever)\n",
    "\"\"\"\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "#PROMPT_TEMPLATE = \"\"\"\n",
    "#Answer the question based only on the following context:\n",
    "\n",
    "#{context}\n",
    "\n",
    "#---\n",
    "\n",
    "#Answer the question based on the above context: {question}\n",
    "#\"\"\"\n",
    "#query_text = \"What is Time Sensitive Networking architecture\"\n",
    "#results = vectorstore.similarity_search_with_score(query_text,k = 9) # The most important part\n",
    "retriever = vectorstore.as_retriever()\n",
    "#context_text = \"\\n\\n\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "llm = ChatOllama(model=\"llama3.2\", format=\"json\", temperature=0)\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance \n",
    "    of a retrieved document to a user question. If the document contains keywords related to the user question, \n",
    "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a float number score  between 0 to 1 to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "    Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "question = \"What is Network Function?\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))\n",
    "#prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "#prompt = prompt_template.format(context = context_text,question=query_text)\n",
    "\n",
    "\n",
    "\n",
    "#response = model.invoke(prompt)\n",
    "#docs = retriever.invoke(question)\n",
    "\n",
    "\"\"\"\n",
    "model = \"ollama-3.2\"\n",
    "# create a new Chat \n",
    "llm = ChatOllama(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3aeb4a55-d1c5-4e4c-affe-9065a30035f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There is no mention of \"Time Sensitive Networking\" in the provided context. The context only discusses 4G, NF, VNF, Network Function, Service-Based Interfaces, Management & Maintenance, Load analytics, Non-Public Network (NPN), mobile edge computing, and spectrum usage in non-standalone mode, but it does not mention Time Sensitive Networking.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64f14d1-e0a8-4d27-bbd4-ca4c10c762f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f5615994-484d-4f49-93c7-09171ce88e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"store = Qdrant.from_documents(\n",
    "    langchain_documents,\n",
    "    embeddings,\n",
    "    path=\"/tmp/ai_qdrant\",\n",
    "    collection_name=\"AI-Embeddings\",\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89198ab1-8e05-4dd5-8d69-a547f44175c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'authors': 'Hao Zhou, Chengming Hu, Ye Yuan, Yufei Cui, Yili Jin', 'page': 7, 'source': 'example/Rapor-RAGEvaluation.pdf', 'tags': 'Large Language Model, Telecom, Article, 5G', 'title': 'Large Language Model (LLM) for Telecommunications: A Comprehensive Survey on Principles, Key Techniques, and Opportunities', 'url': 'https://arxiv.org/pdf/2405.10825'}, page_content='Step 2: For each GT Statement, check if it’s can be attributed to the retrieved \\ncontext.\\nStatement 1: No\\nStatement 2: Yes\\nStep 3: Use the formula to compute the metric:\\nContext recall : 1/2 = 0.5\\nRAG EVALUATION TOOLS\\nRAGAS\\nRagas provides a set of evaluation metrics that can be used to measure the performance \\nof your LLM application. These metrics are designed to help you objectively measure the \\nperformance of your application. Metrics are available for different applications and tasks, \\nsuch as RAG and Agentic workflows.\\nEach metric are essentially paradigms that are designed to evaluate a particular aspect of \\nthe application. LLM Based metrics might use one or more LLM calls to arrive at the score \\nor result. One can also modify or write your own metrics using ragas.\\nRAG METRICS\\n• Context Precision\\n• Context Recall\\n• Context Entities Recall\\n• Noise Sensivity\\n• Response Relevancy\\n• Faithfulness\\n• Multimodal Faithfulness'),\n",
       "  1.6322420354392266)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search_with_score(\n",
    "    query=\"AI and authors\",\n",
    "    k=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1e09b0af-0e07-43ca-8838-1130c22fb0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search_with_score(\n",
    "    query=\"authors\",\n",
    "    filter={\"authors\": \"Yili Jin\"},\n",
    "    k=1\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bab78aad-24dd-423d-a282-db17e149497c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_83196/2562771045.py:5: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(\n",
      "/tmp/ipykernel_83196/2562771045.py:5: DeprecationWarning: callback_manager is deprecated. Please use callbacks instead.\n",
      "  llm = Ollama(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama3.2\",\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "64ce112a-8daf-4b35-b1f6-e3aaf680478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5143090a-f9fa-4315-ad8f-fe38010738ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_chain_with_filter(llm, filter={}):\n",
    "    template = \"\"\"You are a bot that answers user questions using only the context provided.\n",
    "    If you don't know the answer, simply state that you don't know.\n",
    "    {context}\n",
    "    Question: {input}\"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"context\", \"input\"])\n",
    "    retriever = store.as_retriever(search_kwargs={'filter': filter})\n",
    "    llm_with_prompt = create_stuff_documents_chain(llm, prompt)\n",
    "    return create_retrieval_chain(retriever, llm_with_prompt)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5386df27-fcbe-4407-985d-fa53cf20391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = retrieval_chain_with_filter(llm).invoke({\n",
    "    \"input\": \"\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "596141af-0e12-4813-8e0c-a5d8e722c83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: fastembed 0.4.2\n",
      "Uninstalling fastembed-0.4.2:\n",
      "  Would remove:\n",
      "    /home/yasar/anaconda3/envs/llms/lib/python3.11/site-packages/fastembed-0.4.2.dist-info/*\n",
      "    /home/yasar/anaconda3/envs/llms/lib/python3.11/site-packages/fastembed/*\n",
      "Proceed (Y/n)? ^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall fastembed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75675abe-a038-43a1-b471-c8d3f8567ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca84146-8706-4316-88d4-c53b07db6477",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers --quiet\n",
    "%pip install intel-extension-for-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd5078d-f4bf-4f07-863d-af7dbcf70757",
   "metadata": {},
   "source": [
    "# Intel Weight-Only Quantization (Vector Compressing)\n",
    "## Weight-Only Quantization for Huggingface Models with Intel Extension for Transformers Pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ffc48b05-f596-43b7-bfaa-51b21d0a869a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'neural_compressor.conf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mintel_extension_for_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WeightOnlyQuantConfig\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mweight_only_quantization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WeightOnlyQuantPipeline\n\u001b[1;32m      4\u001b[0m conf \u001b[38;5;241m=\u001b[39m WeightOnlyQuantConfig(weight_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.11/site-packages/intel_extension_for_transformers/transformers/__init__.py:19\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#!/usr/bin/env python\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     20\u001b[0m     WEIGHTS_NAME,\n\u001b[1;32m     21\u001b[0m     AutoDistillationConfig,\n\u001b[1;32m     22\u001b[0m     BenchmarkConfig,\n\u001b[1;32m     23\u001b[0m     DistillationConfig,\n\u001b[1;32m     24\u001b[0m     DynamicLengthConfig,\n\u001b[1;32m     25\u001b[0m     FlashDistillationConfig,\n\u001b[1;32m     26\u001b[0m     NASConfig,\n\u001b[1;32m     27\u001b[0m     Provider,\n\u001b[1;32m     28\u001b[0m     PrunerV2,\n\u001b[1;32m     29\u001b[0m     PruningConfig,\n\u001b[1;32m     30\u001b[0m     QuantizationConfig,\n\u001b[1;32m     31\u001b[0m     TFDistillationConfig,\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistillation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     34\u001b[0m     SUPPORTED_DISTILLATION_CRITERION_MODE,\n\u001b[1;32m     35\u001b[0m     DistillationCriterionMode,\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmixture\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto_distillation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoDistillation\n",
      "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.11/site-packages/intel_extension_for_transformers/transformers/config.py:21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menum\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Enum\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mneural_compressor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     22\u001b[0m     Distillation_Conf, Pruner, Pruning_Conf, Quantization_Conf\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mneural_compressor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdotdict\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DotDict, deep_set\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Metric\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'neural_compressor.conf'"
     ]
    }
   ],
   "source": [
    "from intel_extension_for_transformers.transformers import WeightOnlyQuantConfig\n",
    "from langchain_community.llms.weight_only_quantization import WeightOnlyQuantPipeline\n",
    "\n",
    "conf = WeightOnlyQuantConfig(weight_dtype=\"nf4\")\n",
    "hf = WeightOnlyQuantPipeline.from_model_id(\n",
    "    model_id=\"google/flan-t5-large\",\n",
    "    task=\"text2text-generation\",\n",
    "    quantization_config=conf,\n",
    "    pipeline_kwargs={\"max_new_tokens\": 10},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef160ea-295a-4595-91a2-33246a8b5bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from intel_extension_for_transformers.transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "\n",
    "model_id = \"google/flan-t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=10\n",
    ")\n",
    "hf = WeightOnlyQuantPipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4283a256-9b57-4c1b-aeaa-84c08964973b",
   "metadata": {},
   "source": [
    "# RAG CACHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd23117b-53f8-4bde-a01e-51004b3bd976",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain-mongodb: Python package to use MongoDB as a vector store, semantic cache, chat history store etc. in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c1f840-446b-4df4-9ae0-774d63be2be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = prompt | hf\n",
    "\n",
    "question = \"What is electroencephalography?\"\n",
    "\n",
    "print(chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c2bd72e-a429-449a-a0b6-f4f997603d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a046550f-42cd-4ce1-a15e-6220b3c6bf23",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectordb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are the approaches to Task Decomposition?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatOpenAI(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      6\u001b[0m retriever_from_llm \u001b[38;5;241m=\u001b[39m MultiQueryRetriever\u001b[38;5;241m.\u001b[39mfrom_llm(\n\u001b[0;32m----> 7\u001b[0m     retriever\u001b[38;5;241m=\u001b[39m\u001b[43mvectordb\u001b[49m\u001b[38;5;241m.\u001b[39mas_retriever(), llm\u001b[38;5;241m=\u001b[39mllm\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectordb' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectordb.as_retriever(), llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe54966a-9269-46e6-b9c5-9f3d02577825",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
