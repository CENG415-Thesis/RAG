 1/1: 1-1
 1/2: 1+1
 2/1: print("Hello")
 2/2: print("new")
 2/3: print('new')
 3/1:
myname = "Yasar"
a = myname[0]
print(a)
 3/2:
a = myname[-4]
print(a)
 3/3:
a = myname[-4]
print(a)
 3/4:
myname = "Hello"
a = myname[0]
print(a)
 3/5:
a = myname[-4]
print(a)
 3/6: "I'm going on a run "
 3/7: print("Hello")
 3/8: print('Hello \n World')
 3/9: print('Hello \t World')
3/10: len("")
3/11: len('')
 4/1: len('Hello')
 4/2: len('I am')
 4/3: mystring = "Hello World"
 4/4: mystring
 4/5: mystring[0]
 4/6: len(mystring)
 4/7: mystring(8)
 4/8: mystring[8]
 4/9: mystring = "abcdefghjk"
4/10: mystring[2:]
4/11: mystring[:3]
4/12: mystring = "abcdefghijk"
4/13: mystring[2:]
4/14: mystring[3:8]
4/15: mystring[3:7]
4/16: mystring[1:3]
4/17: mystring[::]
4/18: mystring[::2]
4/19: mystring[::3]
4/20: len(mystring)
4/21: for i in range:
4/22: for i in range(10):
4/23:
for i in range(10):
    print(mystring[i])
4/24:
for i in range(10):
    print(mystring[i] + "\t")
4/25:
for i in range(10):
    print(i)
4/26: clear
 5/1: mystring + "yasar"
 5/2: len('Hello')
 5/3: len('I am')
 5/4: mystring = "Hello World"
 5/5: mystring
 5/6: mystring[0]
 5/7: len(mystring)
 5/8: mystring(8)
 5/9: mystring + "yasar"
5/10: for i in range(12):
5/11:
for i in range(12):
    print("Yasar + 1")
4/27: mystring[::-1]
4/28: mystring[::-1]
 6/1:
name = "Sam"
name[0] = "P"
 6/2: last_letters = name[1:]
 6/3: last_letters
 6/4: 'P' + last_letters
 6/5: name = name + name
 6/6: name
 6/7: name = name + "a"
 6/8: name
 6/9: x = "Hello World"
6/10: x.upper()
6/11: len(x.upper())
6/12: type(x)
6/13: len(x.split)
6/14: len(x.split())
6/15: type(x.split())
 7/1: print("This is a string {}".format("YASAR"))
 7/2: print("The {} {} {}".format("Yasar","M","B"))
 7/3: print("The {2} {1} {0} ".format("B","M","Y"))
 7/4: print("The {0} {0} {0}".format("Y"))
 7/5: print("The {q} {b} {f}".format(f="fox",b = "brown", q = "quick"))
 7/6: print(f)
 7/7: result = 100/777
 7/8: result = 100/777
 7/9: result = 100/777
7/10: result
7/11: print("The result was {}".format(result))
7/12: print("The result was {r:10.3f}".format(result))
7/13: print("The result was {r:1.3f}".format(result))
7/14: print("The result was {r:1.3f}".format(r = result))
7/15: print("The result was {r:10.3f}".format(r = result))
7/16: print("The result was {r:0.3f}".format(r = result))
7/17:
print("The result was {r:10.3f}".format(r = result))
# The 10 is for whitespace interval(space)
7/18:
print("The result was {r:1.3f}".format(r = result))
# The 10 is for whitespace interval(space)
7/19: result = 104.123456
7/20: print("The result was {}".format(result))
7/21:
print("The result was {r:1.3f}".format(r = result))
# The 10 is for whitespace interval(space)
7/22:
print("The result was {r:1.2f}".format(r = result))
# The 10 is for whitespace interval(space)
7/23:
print("The result was {r:0.2f}".format(r = result))
# The 10 is for whitespace interval(space)
7/24:
print("The result was {r:.2f}".format(r = result))
# The 10 is for whitespace interval(space)
7/25:
print("The result was {r:2.2f}".format(r = result))
# The 10 is for whitespace interval(space)
7/26:
print("The result was {r:3.2f}".format(r = result))
# The 10 is for whitespace interval(space)
7/27:
print("The result was {r:10.2f}".format(r = result))
# The 10 is for whitespace interval(space)
7/28:
print("The result was {r:0.2f}".format(r = result))
# The 10 is for whitespace interval(space)
7/29:
print("The result was {r:1.2f}".format(r = result))
# The 10 is for whitespace interval(space)
7/30: name = "Jose"
7/31: print('Hello, his name is {}'.format(name))
7/32: print(f'Hello, his name is {name}')
 8/1: my_list = [1,2,3]
 8/2: my_list = ['STRING',100,23.2]
 8/3: len(my_list)
 8/4: mylist = ['one','two','three']
 8/5: mylist[0]
 8/6: mylist[1:]
 8/7:
another_list = ['four','five']
my_list  + another_list
 8/8:
another_list = ['four','five']
mylist  + another_list
 8/9:
another_list = ['four','five']
 new-list = mylist  + another_list
8/10:
another_list = ['four','five']
new-list = mylist  + another_list
8/11:
another_list = ['four','five']
new_list = mylist  + another_list
8/12: new_list
8/13: new_list[0] = 'ONE ALL CAPS'
8/14: new_list.append('six')
8/15: new_list
8/16: new_list.append('seven')
8/17: new_list
8/18: new_list.pop()
8/19: new_list
8/20: popped_item = new_list.pop()
8/21: new_list
8/22: popped_item = new_list.pop()
8/23: new_list
8/24: new_list.pop(0)
8/25: new_list
8/26: new_list.pop(-1)
8/27: new_list
8/28:
new_list = ['a', 'e', 'x', 'b', 'c']
num_list = [4,1,8,3]
8/29: new_list.sort()
8/30: new_list
8/31: num_list
8/32: num_list.sort()
8/33: num_list
8/34: num_list.reverse()
8/35: num_list
 9/1:

my_dict = {'key1':'value1', 'key2':'value2'}
 9/2: my_dict
 9/3: my_dict['key1']
 9/4: prices_lookup = {'apple':2.99, 'oranges':1.99,'milk':5.80 }
 9/5: prices_lookup['apple']
 9/6: d = {'k1':123,'k2':[0,1,2],'k3':{'insideKey':100}}
 9/7: d['k2']
 9/8: d['k3']
 9/9: d['k3']['insideKey']
9/10: d = {'key1':['a','b','c']}
9/11: d['key1'][2].upper()
9/12: d
9/13: d['key1'][2] = d['key1'][2].upper()
9/14: d
9/15: d = {'k1': 100 ,'k2': 200 , 'k3':300}
9/16: d['k1'] = 'NEW VALUE'
9/17: d
9/18: d = {'k1': 100 ,'k2': 200 , 'k3':300}
9/19: d.keys()
9/20: d.values()
9/21: d.items()
10/1: t = (1,2,3)
10/2: type(t)
10/3: type(mylist)
10/4: mylist = [1,2,3]
10/5: type(mylist)
10/6: len(t)
10/7: len(mylist)
10/8: t = ('one',2)
10/9: t[0]
10/10: t[-1]
10/11: t = ('a','a','b')
10/12: t.count('a')
10/13: t.index('a')
10/14: t.index('b')
10/15: t
10/16: mylist
10/17: mylist[0] = 'new'
10/18: mylist
10/19: t[0] = 1
10/20: t[0] = 'new'
10/21: t[0]
10/22: t[0] = 'new'
10/23: t = (1,2,[1,2])
10/24: t[2]
10/25: t[2][0] = 9
10/26: t
11/1: myset = set()
11/2: myset
11/3: myset.add(1)
11/4: myset
11/5: myset.add(2)
11/6: myset.add(2)
11/7: myset
11/8: # AynÄ± matematikteki setler gibi
11/9: #CASTING
11/10: mylist = [1,1,1,1,1,1,1,2,2,2,2,2,2,3,3,3,3,3,3,3]
11/11: set(mylist)
11/12: mylist
12/1: True
12/2: true
12/3: False
12/4: type(False)
12/5: 1 > 2
12/6: 1 == 1
12/7: b = None
12/8: b
13/1: #To create a file which in the Jupyter
13/2: %%writefile myfile.txt
13/3: %%writefile myfile.txt
13/4: %%writefile myfile.txt
13/5:
%%writefile myfile.txt
Hello this is a text file
This is the second line
This is the third line
13/6: myfile = open('myfile.txt')
13/7: myfile = open('whoops_wrong.txt')
13/8: pwd
13/9: myfile = open('myfile.txt')
13/10: myfile.read()
13/11: myfile.read()
13/12: myfile.seek(0)
13/13: myfile.read()
13/14: myfile.seek(5)
13/15: myfile.read()
13/16: myfile.seek(0)
13/17: myfile.readlines()
13/18: len(myfile.readlines)
13/19: len(myfile.readlines())
13/20: myfile.seek(0)
13/21: len(myfile.readlines())
13/22: myfile.close()
13/23: with open('myfile.txt') as my_new_file:
13/24:
with open('myfile.txt') as my_new_file:
    contents = my_new_file.read()
13/25: contents
13/26: type(contents)
13/27:
#Shift + tab for more informations
with open('myfile.txt',mode='r') as myfile:
    contents = myfile.read()
13/28: %%writefile my_new_file.txt
13/29:
%%writefile my_new_file.txt
ONE ON FIRST
TWO ON SECOND
THREE ON THIRD
13/30:
with open('my_new_file.txt',mode = 'r',) as f:
    print(f.read)
13/31:
with open('my_new_file.txt',mode = 'r') as f:
    print(f.read)
13/32:
with open('my_new_file.txt',mode = 'r') as f:
    print(f.read())
13/33:
with open('my_new_file.txt',mode = 'a') as f:
    f.write('FOUR ON FOURTH')
13/34:
with open('my_new_file.txt',mode = 'r') as f:
    print(f.read())
13/35:
with open('my_new_file.txt',mode = 'a') as f:
    f.write('FIVE ON FIFTH')
13/36:
with open('my_new_file.txt',mode = 'r') as f:
    print(f.read())
13/37:
with open('my_new_file.txt',mode = 'a') as f:
    f.write('\nFIVE ON FIFTH')
13/38:
with open('my_new_file.txt',mode = 'r') as f:
    print(f.read())
13/39:
with open('my_new_file.txt',mode = 'a') as f:
    f.write('\nFIVE ON FIFTH')
13/40:
with open('my_new_file.txt',mode = 'a') as f:
    f.write('\nFIVE ON FIFTH')
13/41:
with open('my_new_file.txt',mode = 'r') as f:
    print(f.read())
13/42:
with open('my_new_file.txt',mode = 'r') as f:
    print(f.read())
13/43:
with open('YMBRANDOM.txt',mpde = 'w') as f:
    f.write('I created this file.')
13/44:
with open('YMBRANDOM.txt',mode = 'w') as f:
    f.write('I created this file.')
13/45:
with open('YMBRANDOM.txt',mode = 'r') as f:
    print(f.read())
13/46:
with open('YMBRANDOM.txt',mode = 'W') as f:
    print(f.read())
13/47:
with open('YMBRANDOM.txt',mode = 'w') as f:
    print(f.read())
13/48:
with open('YMBRANDOM.txt',mode = 'a') as f:
    print(f.read())
13/49:
with open('YMBRANDOM.txt',mode = 'w') as f:
    print(f.read())
13/50:
with open('YMBRANDOM.txt',mode = 'r') as f:
    print(f.read())
14/1:
s ='hello'
# Reverse the string using slicing

s[:-1]
14/2:
s ='hello'
# Reverse the string using slicing

s[::-1]
14/3:
s = 'hello'
# Print out 'e' using indexing

s[1]
14/4:
s ='hello'
# Print out the 'o'

# Method 1:
s[4]
14/5:
# Method 2:

s[-1]
14/6:
list3 = [1,2,[3,4,'hello']]

list3[2][2] = 'goodbye'
list3
14/7:
list4 = [5,3,4,6,1]
list4.sort()
14/8:
list4 = [5,3,4,6,1]
list4.sort()
14/9:
list4 = [5,3,4,6,1]
list4.sort()
list4
14/10:
d = {'simple_key':'hello'}
# Grab 'hello'
d['simple_key']
14/11:
d = {'k1':{'k2':'hello'}}
# Grab 'hello'
d['k1']['k2']
14/12:
# Getting a little tricker
d = {'k1':[{'nest_key':['this is deep',['hello']]}]}

#Grab hello

d['k1'][1]
14/13:
# Getting a little tricker
d = {'k1':[{'nest_key':['this is deep',['hello']]}]}

#Grab hello

d['k1']
14/14:
# Getting a little tricker
d = {'k1':[{'nest_key':['this is deep',['hello']]}]}

#Grab hello

d['k1'][0]['nest_key'][1]
14/15:
# This will be hard and annoying!
d = {'k1':[1,2,{'k2':['this is tricky',{'tough':[1,2,['hello']]}]}]}

d['k1']
14/16:
# This will be hard and annoying!
d = {'k1':[1,2,{'k2':['this is tricky',{'tough':[1,2,['hello']]}]}]}

d['k1'][2]
14/17:
# This will be hard and annoying!
d = {'k1':[1,2,{'k2':['this is tricky',{'tough':[1,2,['hello']]}]}]}

d['k1'][2]['k2']
14/18:
# This will be hard and annoying!
d = {'k1':[1,2,{'k2':['this is tricky',{'tough':[1,2,['hello']]}]}]}

d['k1'][2]['k2'][1]
14/19:
# This will be hard and annoying!
d = {'k1':[1,2,{'k2':['this is tricky',{'tough':[1,2,['hello']]}]}]}

d['k1'][2]['k2'][1]['tough']
14/20:
# This will be hard and annoying!
d = {'k1':[1,2,{'k2':['this is tricky',{'tough':[1,2,['hello']]}]}]}

d['k1'][2]['k2'][1]['tough'][2]
14/21:
list5 = [1,2,2,33,4,4,11,22,3,3,2]

set(list5)
14/22:
# Answer before running cell
2 > 3
14/23:
# Answer before running cell
3 <= 2
14/24:
# Answer before running cell
3 == 2.0
14/25:
# Answer before running cell
3.0 == 3
14/26:
# Answer before running cell
4**0.5 != 2
14/27:
# two nested lists
l_one = [1,2,[3,4]]
l_two = [1,2,{'k1':4}]

# True or False?
l_one[2][0] >= l_two[2]['k1']
#False
15/1:
if 3 > 2:
    print('ITS TRUE!')
15/2:
hungry = True

if hungry:
    print('FEED ME!')
15/3:
hungry = True

if not hungry:
    print('i am okey')
else:
    print('feed me')
15/4:
loc = 'Bank'

if loc == 'Auto Shop':
    print('Cars are cool')

elif loc == 'Store':
    print('Welcome to the store!')

elif loc == 'Bank':
    print('Money is cool')

else:
    print('I do not know much.')
15/5:
name = 'Sammy'

if name == 'Frankie':
    print("Hello" + name)

elif name == 'Sammy':
    print("Hello Sammy")
16/1: mylist = [1..10]
17/1: mylist = [1,2,3,4,5,6,7,8,9,10]
17/2:
for num in mylist:
    print(num)
17/3:
for jelly in mylist:
    print(jelly)
17/4:
for jelly in mylist:
    print(hello)
17/5:
for jelly in mylist:
    print("hello")
17/6:
for jelly in mylist:
    if jelly % 2 == 0:
        print(jelly)
17/7:
for jelly in mylist:
    if jelly % 2 == 0:
        print(jelly)
    else:
        print(f'{} is odd number',jelly)
17/8:
for jelly in mylist:
    if jelly % 2 == 0:
        print(jelly)
    else:
        print(f'{jelly} is odd number')
17/9:
tup = (1,2,3)

for item in tup:
    print(item)
17/10: mylist = [(1,2),(3,4),(5,6),(7,8)]
17/11: len(mylist)
17/12:
for (a,b) in mylist:
    print(a,b)
17/13:
for (a,b) in mylist:
    print(a)
17/14:
for (a,b) in mylist:
    print(a)
    print(b)
17/15:
for (a,b) in mylist:
    print(a)
   # print(b)
17/16: d = {'k1':1,'k2':2,'k3':3}
17/17:
for item in d:
    print(item)
17/18:
for item,price in d:
    print(price)
17/19:
for item in d.items:
    print(item)
17/20:
for item in d.items():
    print(item)
17/21:
for key,value in d.items():
    print(item)
17/22:
for key,value in d.items():
    print(key,value)
17/23:
for key,value in d.items():
    print(value)
17/24:
for value in d.values():
    print(value)
18/1: x = 0
18/2:
x = 0

while x < 5:
    print(f'The current value of x is {x}')
18/3:
x = 0

while x < 5:
    print(f'The current value of x is {x}')
    x +=1
18/4:
x = 0

while x < 5:
    print(f'The current value of x is {x}')
    x +=1
else:
    print(f'X is not less than {x}')
18/5:
x = 50

while x < 5:
    print(f'The current value of x is {x}')
    x +=1
else:
    print(f'X is not less than {x}')
18/6:
x = [1,2,3]

for item in x:
    # comment
    pass

print('End of my script')
18/7: mystring = 'Sammy'
18/8:
for letter in mystring:
    print(letter)
18/9:
for letter in mystring:
    if letter == 'a':
        continue
    print(letter)
18/10:
x = 5

while x < 5:
    
    if x == 2:
        break
    
    print(x)
    x+=1
18/11:
x = 0

while x < 5:
    
    if x == 2:
        break
    
    print(x)
    x+=1
19/1: mylist = [1,2,3]
19/2:
for num in range(0,11,2):
    print(num)
19/3: list(range(0,11,2))
19/4:
index_count = 0

for letter in 'abcde':
    print('At index {} , the letter is {}'.format(index_count,letter))
19/5:
index_count = 0

for letter in 'abcde':
    print('At index {} , the letter is {}'.format(index_count,letter))
    index_count+=1
19/6:

word = 'abcde'

for item in enumerate(word):
    print(item)
19/7: print(enumerate(word))
19/8:

word = 'abcde'

for index, letter in enumerate(word):
    print(item)
19/9:

word = 'abcde'

for index, letter in enumerate(word):
    print(index)
    print(letter)
    print('\n')
19/10:
mylist1 = [1,2,3,4]
mylist2 = ['a','b','c','d']

for item in zip(mylist1,mylist2):
    print(item)
19/11:
mylist1 = [1,2,3,4]
mylist2 = ['a','b','c','d']
mylist3 = [100,200,300]

for item in zip(mylist1,mylist2,mylist3):
    print(item)
19/12: list(zip(mylist1,mylist2,mylist3))
19/13: 'x' in [1,2]
19/14: x in 'xss'
19/15: 'x' in 'xss'
19/16: 'mykey' in {'mykey':345}
19/17: 'mykey' in {'mykey':345}.values()
19/18: mylist = [10,20,30,40]
19/19: min(mylist)
19/20: max(mylist)
19/21: from random import
19/22: from random import shuffle
19/23: mylist = [1,2,3,4,5,6,7,8,9,10]
19/24: shuffle(mylist)
19/25: mylist
19/26: shuffle(mylist)
19/27: mylist
19/28: shuffle(mylist)
19/29: mylist
19/30: from random import randint
19/31: randint(0,100)
19/32: randint(0,100)
19/33: randint(0,100)
19/34: randint(0,10)
19/35: input('Enter a number here:')
19/36: result =  input('What is your name:')
19/37: result
20/1: mystring = 'hello'
20/2:
mylist = []

for letter in mystring:
    mylist.append(letter)
20/3: mylist
20/4: mylist = [letter for letter in mystring]
20/5: mylist
20/6: mylist = [x for x in range(0,11)]
20/7: mylist
20/8: mylist = [x**2 for x in range(0,11)]
20/9: mylist
20/10: mylist = [ x%2 == 0 for x in range(0,11)]
20/11: mylist
20/12: mylist = [ x for x in range(0,11) if x % 2 == 0]
20/13: mylist
20/14: celcius = [0,10,20,34.5]
20/15: celcius = [0,10,20,34.5]
20/16:
celcius = [0,10,20,34.5]
fahrenheit = [(9/5)*temp + 32] for temp in celcius
20/17:
celcius = [0,10,20,34.5]
fahrenheit = [(9/5)*temp + 32] for temp in celcius]
20/18:
celcius = [0,10,20,34.5]
fahrenheit = [(9/5)*temp + 32) for temp in celcius]
20/19:
celcius = [0,10,20,34.5]
fahrenheit = [((9/5)*temp + 32) for temp in celcius]
20/20: fahrenheit
20/21:
fahrenheit = []

for temp in celcius:
    fahrenheit.append(((9/5)*temp + 32))
20/22: fahrenheit
20/23: results = [x if x % 2 == 0 else 'ODD' for x in range(0,11)]
20/24: results
20/25: results = [x if x % 2 == 0 else continue for x in range(0,11)]
20/26: results = [x if x % 2 == 0 else pass for x in range(0,11)]
20/27: results = [x if x % 2 == 0 else 0 for x in range(0,11)]
20/28: results = [x if x % 2 == 0 else 0 for x in range(0,11)]
20/29: results
20/30: results = [x if x % 2 == 0 else 'a' for x in range(0,11)]
20/31: results
20/32:
mylist = []

for x in [2,4,6]:
    for y in [100,200,300]:
        mylist.append(x*y)
20/33: mylist
20/34: mylist = [x*y for x in [2,4,6] for y in [1,10,1000]]
20/35: mylist
21/1: list a = st.split('s')
21/2: st.split('s')
21/3: st = 'Print only the words that start with s in this sentence'
21/4: st.split('s')
21/5:  list a = st.split()
21/6: st = 'Print only the words that start with s in this sentence'
21/7: a = st.split()
21/8:
a = st.split()
type(a)
21/9:
a = st.split()

for word in a:
    if word[0] == 's':
        print(word)
21/10:
for i in range(10):
    if i % 2 == 0:
21/11:
for i in range(10):
    if i % 2 == 0:
        print(i)
21/12:
#Code in this cell
[x for x in range(0,50) if (x % 3 == 0)]
21/13:
splitted = st.split()

for word in splitted:
    if len(word) % 2 == 0:
        print(word)
21/14:
for i in range(0,100):
    if ((i % 3) == 0 && (i % 5) != 0):
        print("Fizz")
    elif ((i % 5) == 0 && (i % 3) != 0):
        print("Buzz")
    elif ((i % 5) == 0 && (i % 3) == 0):
        print("FizzBuzz")
    else:
        print(i)
21/15:
for i in range(0,100):
    if ((i % 3) == 0 and (i % 5) != 0):
        print("Fizz")
    elif ((i % 5) == 0 and (i % 3) != 0):
        print("Buzz")
    elif ((i % 5) == 0 and (i % 3) == 0):
        print("FizzBuzz")
    else:
        print(i)
21/16: [x[0] for x in st.split()]
21/17: st = 'Create a list of the first letters of every word in this string'
21/18: [x[0] for x in st.split()]
21/19:
for i in range(11):
    if i % 2 == 0:
        print(i)
22/1:
def say_hello():
    print("Hello")
22/2: say_hello
22/3: say_hello()
22/4:
def  say_hello(name):
    print("Hello" + name)
22/5: say_hello("Yasar")
22/6:
def  say_hello(name):
    print("Hello " + name)
22/7: say_hello("Yasar")
22/8: def say_hello(name= 'Default')
22/9:
def say_hello(name= 'Default'):
    print("Hello " + name)
22/10: say_hello()
22/11:
def add_num(num1,num2):
    return num1 + num2
22/12: add_num(5,4)
22/13: result add_num(5,4)
22/14: result= add_num(5,4)
22/15: result= add_num(5,4)
22/16: result
22/17:
def print_result(a,b):
    print(a + b)
22/18: print_result(23,23)
22/19: def return_result(a,b):
22/20:
def return_result(a,b):
    return a + b
23/1:
def even_check(number):
    result = number % 2 == 0
    return result
23/2: even_check(29)
23/3: even_check(20)
23/4:
def even_check(number):
    return number % 2 == 0
23/5: even_check(1)
23/6: even
23/7: even
23/8: even_check(10)
23/9: # RETURN TRUE IF ANY NUMBER IS EVEN INSIDE A LIST
23/10:
def any_even(a):
    for number in a:
        if(number % 2 == 0 )
            return true;
            break
23/11:
def any_even(a):
    for number in a:
        if(number % 2 == 0 ):
            return true
            break
23/12:
def any_even(a):
    isEven = False
    for number in a:
        if(number % 2 == 0 ):
            return not isEven
            break
         else:
            return isEven
23/14:
def any_even(a):
    isEven = False
    for number in a:
        if(number % 2 == 0 ):
            return not isEven
            break
        else:
            return isEven
23/15: any_even(2)
23/16: any_even([5,7,8])
23/17:
def any_even(a):
    isEven = False
    for number in a:
        if(number % 2 == 0 ):
            isEven = True
            break
    
    return isEven
23/18: any_even([5,7,8])
23/19: any_even([5,7,9])
23/20: any_even([,8,5,7,9])
23/21: any_even([0,8,5,7,9])
23/22: any_even([8,5,7,9])
23/23: OR YOU CAN DO JUST LIKE THAT
23/24:
def any_even_list(num_list):
    for number in num_list:
        if number % 2 == 0
             return True
        else:
            pass
23/25:
def any_even_list(num_list):
    for number in num_list:
        if number % 2 == 0:
             return True
        else:
            pass
23/26: any_even_list([1,2,3,4])
23/27:
def return_all_evens(my_list):
    even_list = []

    for number in my_list:
        if number % 2 == 0:
            even_list.append(number)
        else:
            pass
    
    
    return even_list
23/28: return_all_evens([1,2,344,5,6,6,7,7,4,2])
23/29:
def return_all_evens(my_list):
    even_list = []

    for number in my_list:
        if number % 2 == 0:
            even_list.append(number)
        else:
            pass
    

    even_list = list(set(even_list))
    
    return even_list
23/30: return_all_evens([1,2,344,5,6,6,7,7,4,2])
23/31:
def return_all_evens(my_list):
    even_list = []

    for number in my_list:
        if number % 2 == 0:
            even_list.append(number)
        else:
            pass
    

    even_list = list(set(even_list))
    even_list.sort()
    
    return even_list
23/32: return_all_evens([1,2,344,5,6,6,7,7,4,2])
23/33:
def return_all_evens(my_list):
    even_list = []

    for number in my_list:
        if number % 2 == 0:
            even_list.append(number)
        else:
            pass
    

    even_list = list(set(even_list))
    even_list.sort()[::-1]
    
    return even_list
23/34: return_all_evens([1,2,344,5,6,6,7,7,4,2])
23/35:
def return_all_evens(my_list):
    even_list = []

    for number in my_list:
        if number % 2 == 0:
            even_list.append(number)
        else:
            pass
    

    even_list = list(set(even_list))
    even_list.sort()
    even_list[::-1]
    
    return even_list
23/36: return_all_evens([1,2,344,5,6,6,7,7,4,2])
23/37:
def return_all_evens(my_list):
    even_list = []

    for number in my_list:
        if number % 2 == 0:
            even_list.append(number)
        else:
            pass
    

    even_list = list(set(even_list))
    even_list.sort()
    even_list[::-1]
    
    return even_list
23/38: return_all_evens([1,2,344,5,6,6,7,7,4,2])
23/39:
def return_all_evens(my_list):
    even_list = []

    for number in my_list:
        if number % 2 == 0:
            even_list.append(number)
        else:
            pass
    

    even_list = list(set(even_list))
    even_list.sort()
    even_list[::-1]
    
    return even_list
23/40: return_all_evens([1,2,344,5,6,6,7,7,4,2])
23/41:
def return_all_evens(my_list):
    even_list = []

    for number in my_list:
        if number % 2 == 0:
            even_list.append(number)
        else:
            pass
    

    even_list = list(set(even_list))
    even_list.sort()
    even_list[::-1]
    
    return even_list
23/42: return_all_evens([1,2,344,5,6,6,7,7,4,2])
23/43:
def return_all_evens(my_list):
    even_list = []

    for number in my_list:
        if number % 2 == 0:
            even_list.append(number)
        else:
            pass
    

    even_list = list(set(even_list))
    even_list.sort()
    even_list[::1]
    
    return even_list
23/44: return_all_evens([1,2,344,5,6,6,7,7,4,2])
23/45:
def return_all_evens(my_list):
    even_list = []

    for number in my_list:
        if number % 2 == 0:
            even_list.append(number)
        else:
            pass
    

    even_list = list(set(even_list))
    even_list.sort()
    even_list[-1]
    
    return even_list
23/46: return_all_evens([1,2,344,5,6,6,7,7,4,2])
23/47:
def return_all_evens(my_list):
    even_list = []

    for number in my_list:
        if number % 2 == 0:
            even_list.append(number)
        else:
            pass
    

    even_list = list(set(even_list))
    even_list.sort()
    even_list = even_list[::-1]
    
    return even_list
23/48: return_all_evens([1,2,344,5,6,6,7,7,4,2])
24/1: stock_prices = [('APPL',200),('GOOG',400),('MSFT',100)]
24/2:
for item in stock_prices:
    print(item)
24/3:
for ticker,price in stock_prices:
    print(ticker)
24/4:
for ticker,price in stock_prices:
    print(price + (0.1)*price)
24/5: work_hours = [('Abby',100),('Billy',400),('Cassie',800)]
24/6:
def employee_check(work_hours):
        current_name = ""
        current_max = 0
    for name,hours in work_hours:
        if hours > current_max:
            current_max = hours
            current_name = name
    return name
24/8:
def employee_check(work_hours):
    current_name = ""
    current_max = 0
    for name,hours in work_hours:
        if hours > current_max:
            current_max = hours
            current_name = name
    return name
24/9: employee_check(work_hours)
24/10:
def employee_check(work_hours):
    current_name = ""
    current_max = 0
    for name,hours in work_hours:
        if hours > current_max:
            current_max = hours
            current_name = name
    return (name, work_hours)
24/11: employee_check(work_hours)
24/12:
def employee_check(work_hours):
    current_name = ""
    current_max = 0
    for name,hours in work_hours:
        if hours > current_max:
            current_max = hours
            current_name = name
    return (name, hours)
24/13: employee_check(work_hours)
24/14: name, hours = employee_check(work_hours)
24/15: name
24/16: hours
25/1: example = [1,2,3,4,5,6,7]
25/2: from random import shuffle
25/3: shuffle(example)
25/4: example
25/5:
def shuffle_list(mylist)
    shuffle(mylist)
    return mylist
25/6:
def shuffle_list(mylist):
    shuffle(mylist)
    return mylist
25/7: result = shuffle_list(example)
25/8: result
25/9: mylist = [' ','O',' ']
25/10: shuffle_list(mylist)
27/1:
def player_guess():
    guess = ''
    guess = int(input("Pick a number: 0,1 or 2"))
    
    while guess not in ['0','1','2']:
        guess = input("Pick a number: 0,1 or 2")
    
    return int(guess)
27/2: player_guess()
27/3: myindex = player_guess()
27/4:
def player_guess():
    guess = ''
    
    while guess not in ['0','1','2']:
        guess = input("Pick a number: 0,1 or 2")
    
    return int(guess)
27/5:
def player_guess():
    guess = ''
    
    while guess not in ['0','1','2']:
        guess = input("Pick a number: 0,1 or 2")
    
    return int(guess)
27/6: player_guess()
27/7: myindex = player_guess()
27/8:
def player_guess():
    guess = ''
    
    while guess not in ['0','1','2']:
        guess = input("Pick a number: 0,1 or 2\n")
    
    return int(guess)
27/9: player_guess()
27/10: myindex = player_guess()
27/11: example = [1,2,3,4,5,6,7]
27/12: from random import shuffle
27/13: shuffle(example)
27/14: example
27/15:
def shuffle_list(mylist):
    shuffle(mylist)
    return mylist
27/16: result = shuffle_list(example)
27/17: result
27/18: mylist = [' ','O',' ']
27/19: shuffle_list(mylist)
27/20:
def player_guess():
    guess = ''
    
    while guess not in ['0','1','2']:
        guess = input("Pick a number: 0,1 or 2\n")
    
    return int(guess)
27/21: player_guess()
27/22: myindex = player_guess()
27/23:
if result[myindex] == 'O':
    print("Well done")
else:
    print("You are wrong")
27/24: result = shuffle_list(example)
27/25: mylist = [' ','O',' ']
27/26:
def player_guess():
    guess = ''
    
    while guess not in ['0','1','2']:
        guess = input("Pick a number: 0,1 or 2\n")
    
    return int(guess)
27/27: player_guess()
27/28: myindex = player_guess()
27/29:
if result[myindex] == 'O':
    print("Well done")
else:
    print("You are wrong")
27/30: result = shuffle_list(example)
27/31: mylist = [' ','O',' ']
27/32:
def player_guess():
    guess = ''
    
    while guess not in ['0','1','2']:
        guess = input("Pick a number: 0,1 or 2\n")
    
    return int(guess)
27/33: myindex = player_guess()
27/34:
if result[myindex] == 'O':
    print("Well done")
else:
    print("You are wrong")
27/35: example = [1,2,3,4,5,6,7]
27/36: from random import shuffle
27/37: shuffle(example)
27/38: example
27/39:
def shuffle_list(mylist):
    shuffle(mylist)
    return mylist
27/40: result = shuffle_list(example)
27/41: mylist = [' ','O',' ']
27/42:
def player_guess():
    guess = ''
    
    while guess not in ['0','1','2']:
        guess = input("Pick a number: 0,1 or 2\n")
    
    return int(guess)
27/43: myindex = player_guess()
27/44:
if result[myindex] == 'O':
    print("Well done")
else:
    print("You are wrong")
27/45:
def player_guess():
    guess = ''
    
    while guess not in ['0','1','2']:
        guess = input("Pick a number: 0,1 or 2\n")
    
    return int(guess)
27/46: result = shuffle_list(mylist)
27/47: myindex = player_guess()
27/48:
if result[myindex] == 'O':
    print("Well done")
else:
    print("You are wrong")
27/49:
def player_guess():
    guess = ''
    
    while guess not in ['0','1','2']:
        guess = input("Pick a number: 0,1 or 2\n")
    
    return int(guess)
27/50: result = shuffle_list(mylist)
27/51: myindex = player_guess()
27/52:
if result[myindex] == 'O':
    print("Well done")
else:
    print("You are wrong")
27/53:
def shuffle_list(mylist):
    shuffle(mylist)
    return mylist
27/54: result = shuffle_list(example)
27/55: mylist = [' ','O',' ']
27/56:
def player_guess():
    guess = ''
    
    while guess not in ['0','1','2']:
        guess = input("Pick a number: 0,1 or 2\n")
    
    return int(guess)
27/57: result = shuffle_list(mylist)
27/58: myindex = player_guess()
27/59:
if result[myindex] == 'O':
    print("Well done")
else:
    print("You are wrong")
27/60:
if result[myindex] == 'O':
    print("Well done")
else:
    print("You are wrong")
    print(result)
27/61:
def player_guess():
    guess = ''
    
    while guess not in ['0','1','2']:
        guess = input("Pick a number: 0,1 or 2\n")
    
    return int(guess)
27/62: result = shuffle_list(mylist)
27/63: myindex = player_guess()
27/64:
if result[myindex] == 'O':
    print("Well done")
else:
    print("You are wrong")
    print(result)
29/1:
def myfunc(a,b):
    #Returns 5% of the sum of a and b
    return sum((a,b))*0.05
29/2: myfunc(40,60)
29/3:
def myfunc(a,b,c=0,d=0,e=0):
    #Returns 5% of the sum of a and b
    return sum((a,b,c,d,e))*0.05
29/4: myfunc(40,60)
29/5: myfunc(40,60,100,100)
29/6:
def myfunc(*args):
    return sum(args) * 0.05
29/7: myfunc(10,20,102,23,23,23,3)
29/8:
# **kwargs return dictionary
def myfunc(**kwargs):
    if 'fruit' in kwargs:
        print('My fruit of choice is {}'.format(kwargs['fruit'])
    else:
        print('I did not find any fruit here')
29/9:
# **kwargs return dictionary
def myfunc(**kwargs):
    if 'fruit' in kwargs:
        print('My fruit of choice is {}'.format(kwargs['fruit']))
    else:
        print('I did not find any fruit here')
29/10: myfunc(fruit = 'apple')
29/11:
# **kwargs return dictionary
def myfunc(**kwargs):
    print(kwargs)
    if 'fruit' in kwargs:
        print('My fruit of choice is {}'.format(kwargs['fruit']))
    else:
        print('I did not find any fruit here')
29/12: myfunc(fruit = 'apple', veggie = 'lettuce')
29/13:
def myfunc(*args, **kwargs):
    print('I would like {} {}'.format(args[0],kwargs['food']))
29/14:
def myfunc(*args, **kwargs):
    print('I would like {} {}'.format(args[0],kwargs['food']))
29/15: myfunc(10,20,30, fruit ="orange",food = "eggs", animal = "dogs")
29/16:
def myfunc(*args):
    print(args) # Any other keyword you want
    return sum(args) * 0.05
29/17:
def myfunc(*spam): # Any other keyword you want
    return sum(spam) * 0.05
29/18: myfunc(10,20,102,23,23,23,3)
29/19:
def myfunc(*args):
    print(args) # Any other keyword you want
    return sum(args) * 0.05
29/20:
def myfunc(*spam): # Any other keyword you want
    return sum(spam) * 0.05
29/21:
def myfunc(*spam):
    print(spam)# Any other keyword you want
    return sum(spam) * 0.05
29/22: myfunc(10,20,102,23,23,23,3)
31/1:
def lesser_of_two_evens(a,b):
    if a % 2 == 0 and b % 2 == 0:
        return min(a,b)
    else:
        return max(a,b)
31/2:
# Check
lesser_of_two_evens(2,4)
31/3:
# Check
lesser_of_two_evens(2,5)
31/4:
def animal_crackers(text):
    text_list = text.split(" ")
    if text_list[0] == text_list[1]:
        return True
    else:
        return False
31/5:
# Check
animal_crackers('Levelheaded Llama')
31/6:
def animal_crackers(text):
    text_list = text.split("")
    if text_list[0][0] == text_list[1][0]:
        return True
    else:
        return False
31/7:
# Check
animal_crackers('Levelheaded Llama')
31/8:
# Check
animal_crackers('Crazy Kangaroo')
31/9:
def animal_crackers(text):
    text_list = text.split("")
    if text_list[0][0] == text_list[1][0]:
        return True
    else:
        return False
31/10:
# Check
animal_crackers('Levelheaded Llama')
31/11:
def animal_crackers(text):
    text_list = text.split(" ")
    if text_list[0][0] == text_list[1][0]:
        return True
    else:
        return False
31/12:
# Check
animal_crackers('Levelheaded Llama')
31/13:
# Check
animal_crackers('Crazy Kangaroo')
31/14:
def makes_twenty(n1,n2):
    if n1 + n2 == 20:
        return True
    else:
        return False
31/15:
# Check
makes_twenty(20,10)
31/16:
def makes_twenty(n1,n2):
    if n1 + n2 == 20 or n1 == 20 or n2 == 20:
        return True
    else:
        return False
31/17:
# Check
makes_twenty(20,10)
31/18:
# Check
makes_twenty(2,3)
31/19:
def old_macdonald(name):
    out_list = []
    out_string = ""
    out_list[0] = name[:3].capitalize()
    out_list[1] = name[3:].capitalize()
    out_string = out_list[0] + out_list[1]
31/20:
# Check
old_macdonald('macdonald')
31/21:
def old_macdonald(name):
    out_list = []
    out_string = ""
    out_list[0] = name[:3]
    out_list[1] = name[3:]
    out_string = out_list[0].capitalize() + out_list[1].capitalize()
31/22:
# Check
old_macdonald('macdonald')
31/23:
def old_macdonald(name):
    out_list = []
    out_string = ""
    out_list[0] = name[:3]
    out_list[1] = name[3:]
    out_string = out_list[0].capitalize() + out_list[1].capitalize()
    return out_string
31/24:
# Check
old_macdonald('macdonald')
31/25:
def old_macdonald(name):
    out_list = []
    out_string = ""
    out_list[0] = name[:3]
    
    return outlist
31/26:
# Check
old_macdonald('macdonald')
31/27:
def old_macdonald(name):
    
    out_string = ""
    out_string +=name[:3].capitalize()
    out_string +=name[3:].capitalize()
    
    return out_string
31/28:
# Check
old_macdonald('macdonald')
31/29:
def master_yoda(text):
    text_list =  text.split(" ")
    return " ".join(text_list[::-1]
31/30:
def master_yoda(text):
    text_list =  text.split(" ")
    return " ".join(text_list[::-1])
31/31:
# Check
master_yoda('I am home')
31/32:
# Check
master_yoda('We are ready')
31/33:
# Check
has_33([1, 3, 3])
31/34:
def has_33(nums):
    if "33" in " ".join(nums):
        return True
    else:
        return False
31/35:
# Check
has_33([1, 3, 3])
31/36:
def has_33(nums):
    if "33" in (" ".join(nums)):
        return True
    else:
        return False
31/37:
# Check
has_33([1, 3, 3])
31/38:
def has_33(nums):
    if "33" in str("".join(nums)):
        return True
    else:
        return False
31/39:
# Check
has_33([1, 3, 3])
31/40:
def has_33(nums):
    if "33" in str(" ".join(nums)):
        return True
    else:
        return False
31/41:
# Check
has_33([1, 3, 3])
31/42:
def has_33(nums):
    if "33" in str("".join(nums)):
        return True
    else:
        return False
31/43:
# Check
has_33([1, 3, 3])
31/44:
def has_33(nums):
    return "".join(nums)
31/45:
# Check
has_33([1, 3, 3])
31/46:
# Check
has_33([1, 3, 3])
31/47:
def has_33(nums):
   catted = ""
   for number in nums:
        catted +=str(number)
 
    if '33' in catted:
        return True
    else:
        return False
31/49:
def has_33(nums):
   catted = ""
   for number in nums:
        catted +=str(number)
 
 if '33' in catted:  
        return True
    else:
        return False
31/51:
def has_33(nums):
   catted = ""
 for number in nums:
    catted +=str(number)
 
 if '33' in catted:  
    return True
 else:
    return False
31/53:
def has_33(nums):
   catted = ""
 for number in range(len(nums)):
    catted +=str(number)
 
 if '33' in catted:  
    return True
 else:
    return False
31/55:
def has_33(nums):
   catted = ""
 for number in range(len(nums)):
    catted +=str(nums[number])
 
 if '33' in catted:  
    return True
 else:
    return False
31/57:
def has_33(nums):
   catted = ""

 for number in range(len(nums)):
    catted +=str(nums[number])
 
 if '33' in catted:  
    return True
 else:
    return False
31/59:
def has_33(nums):
   catted = ""

 for number in range(0,len(nums):
    catted +=str(nums[number])
 
 if '33' in catted:  
    return True
 else:
    return False
31/61:
def has_33(nums):
    for number in range(len(nums)):
31/62:
def has_33(nums):
    catted = ""
    for number in range(len(nums)):
       catted += str(nums[number])
    
    if "33" in catted:
        return True
    else:
        return False
31/63:
# Check
has_33([1, 3, 3])
31/64:
# Check
has_33([1, 3, 1, 3])
31/65:
# Check
has_33([3, 1, 3])
31/66:
def blackjack(a,b,c):
    if sum(a,b,c) <= 21:
        return sum
    else if sum(a,b,c) > 21 and 11 in list(a,b,c) == True:
        return sum(a,b,c) - 10
    else:
        return "BUST"
31/67:
def blackjack(a,b,c):
    if sum(a,b,c) <= 21:
        return sum
    elif sum(a,b,c) > 21 and 11 in list(a,b,c) == True:
        return sum(a,b,c) - 10
    else:
        return "BUST"
31/68:
# Check
blackjack(5,6,7)
31/69:
def blackjack(a,b,c):
    if sum(args) <= 21:
        return sum
    elif sum(args) > 21 and 11 in list(a,b,c) == True:
        return sum(args) - 10
    else:
        return "BUST"
31/70:
# Check
blackjack(5,6,7)
31/71:
def blackjack(a,b,c):
    d = a + b
    if sum(d,c) <= 21:
        return sum
    elif sum(d,c) > 21 and 11 in list(a,b,c) == True:
        return sum(d,c) - 10
    else:
        return "BUST"
31/72:
# Check
blackjack(5,6,7)
31/73:
def blackjack(a,b,c):
    d = a + b
    if sum(d,c) <= 21:
        return sum
    elif sum(d,c) > 21 and 11 in list[a,b,c] == True:
        return sum(d,c) - 10
    else:
        return "BUST"
31/74:
# Check
blackjack(5,6,7)
31/75:
def blackjack(a,b,c):
    d = a + b
    if sum(d,c) <= 21:
        return sum
    elif sum(d,c) > 21 and (11 in tuple(a,b,c)):
        return sum(d,c) - 10
    else:
        return "BUST"
31/76:
# Check
blackjack(5,6,7)
31/77:
def blackjack(a,b,c):
    list_ = [a,b,c]
    d = a + b
    if sum(d,c) <= 21:
        return sum
    elif sum(d,c) > 21 and (11 in list_):
        return sum(d,c) - 10
    else:
        return "BUST"
31/78:
# Check
blackjack(5,6,7)
31/79:
def blackjack(a,b,c):
    list_ = [a,b,c]
    d = a + b
    if sum(d,c) < 21:
        return sum
    elif sum(d,c) > 21 and (11 in list_):
        return sum(d,c) - 10
    else:
        return "BUST"
31/80:
# Check
blackjack(5,6,7)
31/81:
def blackjack(a,b,c):
    list_ = [a,b,c]
    d = a + b
    if sum(d,c) <= 21:
        return sum
    elif sum(d,c) > 21 and (11 in list_):
        return sum(d,c) - 10
    else:
        return "BUST"
31/82:
# Check
blackjack(5,6,7)
31/83:
# Check
blackjack(9,9,9)
31/84:
def blackjack(a,b,c):
    list_[0] = a
    list_[1] = b
    list_[2] = c
    d = a + b
    if sum(d,c) <= 21:
        return sum
    elif sum(d,c) > 21 and (11 in list_):
        return sum(d,c) - 10
    else:
        return "BUST"
31/85:
# Check
blackjack(5,6,7)
31/86:
# Check
blackjack(9,9,9)
31/87:
# Check
blackjack(9,9,11)
31/88:
def blackjack(a,b,c):
  return a + b + c
31/89:
# Check
blackjack(5,6,7)
31/90:
def blackjack(a,b,c):
      sum = a + b + c
    if sum <=21:
        return sum
    else:
        if a == 11 or b = 11 or c == 11:
            return sum - 10
        else:
            return "BUST"
31/92:
def blackjack(a,b,c):
    sum = a + b + c
    if sum <=21:
        return sum
    else:
        if a == 11 or b = 11 or c == 11:
            return sum - 10
        else:
            return "BUST"
31/93:
def blackjack(a,b,c):
    sum = a + b + c
    if sum <=21:
        return sum
    else:
        if a == 11 or b == 11 or c == 11:
            return sum - 10
        else:
            return "BUST"
31/94:
# Check
blackjack(5,6,7)
31/95:
# Check
blackjack(9,9,9)
31/96:
# Check
blackjack(9,9,11)
31/97:
def summer_69(arr):
        
    out = ""
    for i in range(len(arr)):
        out +=str(arr[i])
    if 6 not in arr:
        return sum(arr)
    else:
        return sum(arr)- sum(arr[arr.index(6):arr.index(9)])
31/98:
# Check
summer_69([1, 3, 5])
31/99:
# Check
summer_69([4, 5, 6, 7, 8, 9])
31/100:
# Check
summer_69([2, 1, 6, 9, 11])
31/101:
def summer_69(arr):
        arr.append(0)
    out = ""
    for i in range(len(arr)):
        out +=str(arr[i])
    if 6 not in arr:
        return sum(arr)
    else:
        return sum(arr)- sum(arr[arr.index(6):arr.index(9)])
31/103:
def summer_69(arr):
    arr.append(0)
    out = ""
    for i in range(len(arr)):
        out +=str(arr[i])
    if 6 not in arr:
        return sum(arr)
    else:
        return sum(arr)- sum(arr[arr.index(6):arr.index(9)])
31/104:
# Check
summer_69([1, 3, 5])
31/105:
# Check
summer_69([4, 5, 6, 7, 8, 9])
31/106:
# Check
summer_69([2, 1, 6, 9, 11])
31/107:
def summer_69(arr):
    arr.append(0)
    out = ""
    for i in range(len(arr)):
        out +=str(arr[i])
    if 6 not in arr:
        return sum(arr)
    else:
        return sum(arr)- sum(arr[arr.index(6):(arr.index(9) + 1)])
31/108:
# Check
summer_69([1, 3, 5])
31/109:
# Check
summer_69([4, 5, 6, 7, 8, 9])
31/110:
# Check
summer_69([2, 1, 6, 9, 11])
31/111:
def summer_69(arr):
    # arr.append(0)
    out = ""
    for i in range(len(arr)):
        out +=str(arr[i])
        
        
    if 6 not in arr:
        return sum(arr)
    else:
       # return sum(arr)- sum(arr[arr.index(6):(arr.index(9) + 1)])
        for i in range(len(arr[arr.index(6):])) 
            while (arr[i] != 9)
                arr.pop(i)
            else:
                arr.pop(i)
                break
31/112:
# Check
summer_69([1, 3, 5])
31/113:
def summer_69(arr):
    # arr.append(0)
    out = ""
    for i in range(len(arr)):
        out +=str(arr[i])
        
        
    if 6 not in arr:
        return sum(arr)
    else:
       # return sum(arr)- sum(arr[arr.index(6):(arr.index(9) + 1)])
        for i in range(len(arr[arr.index(6):])) 
            while (arr[i] != 9):
                arr.pop(i)
            else:
                arr.pop(i)
                break
31/114:
def summer_69(arr):
    # arr.append(0)
    out = ""
    for i in range(len(arr)):
        out +=str(arr[i])
        
        
    if 6 not in arr:
        return sum(arr)
    else:
       # return sum(arr)- sum(arr[arr.index(6):(arr.index(9) + 1)])
        for i in range(len(arr[arr.index(6):])):
            while (arr[i] != 9):
                arr.pop(i)
            else:
                arr.pop(i)
                break
31/115:
# Check
summer_69([1, 3, 5])
31/116:
# Check
summer_69([4, 5, 6, 7, 8, 9])
31/117:
# Check
summer_69([2, 1, 6, 9, 11])
31/118:
def summer_69(arr):
    # arr.append(0)
    out = ""
    for i in range(len(arr)):
        out +=str(arr[i])
        
        
    if 6 not in arr:
        return sum(arr)
    else:
       # return sum(arr)- sum(arr[arr.index(6):(arr.index(9) + 1)])
        for i in range(len(arr[arr.index(6):])):
            while (arr[i] != 9):
                arr.pop(i)
            else:
                arr.pop(i)
                break
        return sum(arr)
31/119:
# Check
summer_69([1, 3, 5])
31/120:
# Check
summer_69([4, 5, 6, 7, 8, 9])
31/121:
# Check
summer_69([2, 1, 6, 9, 11])
31/122:
def summer_69(arr):
    # arr.append(0)
    out = ""
    for i in range(len(arr)):
        out +=str(arr[i])
        
        
    if 6 not in arr:
        return sum(arr)
    else:
       # return sum(arr)- sum(arr[arr.index(6):(arr.index(9) + 1)])
        for i in range(len(arr[arr.index(6):])):
            while (arr[i] != 9):
                arr.pop(i)
            else:
                arr.pop(i)
                break
        return sum(arr)
31/123:
# Check
summer_69([1, 3, 5])
31/124:
# Check
summer_69([4, 5, 6, 7, 8, 9])
31/125:
# Check
summer_69([2, 1, 6, 9, 11])
31/126:
def summer_69(arr):
    # arr.append(0)
    out = ""
    for i in range(len(arr)):
        out +=str(arr[i])
        
        
    if 6 not in arr:
        return sum(arr)
    else:
       # return sum(arr)- sum(arr[arr.index(6):(arr.index(9) + 1)])
        for i in range(len(arr[arr.index(6):])):
            while (arr[arr.index(6):][i] != 9):
                arr.pop(i + arr.index(6))
            else:
                arr.pop(i + arr.index(6))
                break
        return sum(arr)
31/127:
# Check
summer_69([1, 3, 5])
31/128:
# Check
summer_69([4, 5, 6, 7, 8, 9])
31/129:
def summer_69(arr):
    # arr.append(0)
    out = ""
    for i in range(len(arr)):
        out +=str(arr[i])
        
        
    if 6 not in arr:
        return sum(arr)
    else:
       # return sum(arr)- sum(arr[arr.index(6):(arr.index(9) + 1)])
        for i in range(len(arr[arr.index(6):])):
            temp = arr.index(6)
            while (arr[arr.index(6):][i] != 9):
                arr.pop(i + temp)
            else:
                arr.pop(i + temp)
                break
        return sum(arr)
31/130:
def summer_69(arr):
    # arr.append(0)
    out = ""
    for i in range(len(arr)):
        out +=str(arr[i])
        
        
    if 6 not in arr:
        return sum(arr)
    else:
       # return sum(arr)- sum(arr[arr.index(6):(arr.index(9) + 1)])
        for i in range(len(arr[arr.index(6):])):
            temp = arr.index(6)
            while (arr[arr.index(6):][i] != 9):
                arr.pop(i + temp)
            else:
                arr.pop(i + temp)
                break
        return sum(arr)
31/131:
def summer_69(arr):
    # arr.append(0)
    out = ""
    for i in range(len(arr)):
        out +=str(arr[i])
        
        
    if 6 not in arr:
        return sum(arr)
    else:
       # return sum(arr)- sum(arr[arr.index(6):(arr.index(9) + 1)])
        for i in range(len(arr[arr.index(6):])):
            temp = arr.index(6)
            while (arr[arr.index(6):][i] != 9):
                arr.pop(i + temp)
            else:
                arr.pop(i + temp)
                break
        return sum(arr)
31/132:
# Check
summer_69([1, 3, 5])
31/133:
# Check
summer_69([4, 5, 6, 7, 8, 9])
31/134:
# Check
summer_69([2, 1, 6, 9, 11])
31/135:
def summer_69(arr):
    arr.append(0)
    
    if 6 not in arr:
        return sum(arr)
    else:
        return sum(arr)- sum(arr[arr.index(6):(arr.index(9) + 1)])
31/136:
# Check
summer_69([1, 3, 5])
31/137:
# Check
summer_69([4, 5, 6, 7, 8, 9])
31/138:
# Check
summer_69([2, 1, 6, 9, 11])
31/139:
def spy_game(nums):
    out = ""
    for element in nums:
        if element != 0 or element != 7:
            nums.remove(element)

        if [0,0,7] in nums:
            return True
        else:
            return False
31/140:
# Check
spy_game([1,2,4,0,0,7,5])
31/141:
# Check
spy_game([1,0,2,4,0,5,7])
31/142:
# Check
spy_game([1,7,2,0,4,5,0])
31/143:
def spy_game(nums):
    out = ""
    for element in nums:
        if element != 0 or element != 7:
            nums.remove(element)

    print(nums)
31/144:
# Check
spy_game([1,2,4,0,0,7,5])
31/145:
# Check
spy_game([1,0,2,4,0,5,7])
31/146:
# Check
spy_game([1,7,2,0,4,5,0])
31/147:
def spy_game(nums):
    out = ""
    for element in nums:
        if element == 0 or element == 7:
            pass
        else:
            nums.remove(element)
31/148:
# Check
spy_game([1,2,4,0,0,7,5])
31/149:
# Check
spy_game([1,0,2,4,0,5,7])
31/150:
# Check
spy_game([1,7,2,0,4,5,0])
31/151:
def spy_game(nums):
    out = ""
    for element in nums:
        if element == 0 or element == 7:
            pass
        else:
            nums.remove(element)
            
    
    print(nums)
31/152:
# Check
spy_game([1,2,4,0,0,7,5])
31/153:
# Check
spy_game([1,0,2,4,0,5,7])
31/154:
# Check
spy_game([1,7,2,0,4,5,0])
31/155:
def spy_game(nums):
    out = ""
    for element in nums:
        out += str(element)
        
    zeros_list = []
    seven_index = 0 
    for i in range(len(out)):
         if out[i] == '0':
            zeros_list.append(out[i])
         
         elif out[i] == 7:
                seven_index = i
         
         else:
            pass
    
    is_seven_last = True

        if (max(zeros_list) > 7)
            is_seven_last == False
        
        else:
            pass
        
        
        
    return is_seven_last
31/156:
# Check
spy_game([1,2,4,0,0,7,5])
31/157:
def spy_game(nums):
    out = ""
    for element in nums:
        out += str(element)
        
    zeros_list = []
    seven_index = 0 
    for i in range(len(out)):
         if out[i] == '0':
            zeros_list.append(out[i])
         
         elif out[i] == 7:
                seven_index = i
         
         else:
            pass
    
    is_seven_last = True

        if (max(zeros_list) > 7):
            is_seven_last == False
        
        else:
            pass
        
        
        
    return is_seven_last
31/158:
def spy_game(nums):
    out = ""
    for element in nums:
        out += str(element)
        
    zeros_list = []
    seven_index = 0 
    for i in range(len(out)):
         if out[i] == '0':
            zeros_list.append(out[i])
         
         elif out[i] == 7:
                seven_index = i
         
         else:
            pass
    
        is_seven_last = True

        if (max(zeros_list) > 7):
            is_seven_last == False
        
        else:
            pass
        
        
        
    return is_seven_last
31/160:
def spy_game(nums):
    out = ""
    for element in nums:
        out += str(element)
        
    zeros_list = []
    seven_index = 0 
    for i in range(len(out)):
         if out[i] == '0':
            zeros_list.append(out[i])
         
         elif out[i] == 7:
                seven_index = i
         
         else:
            pass
    
        
    is_seven_last = True
    if (max(zeros_list) > 7):
            is_seven_last == False
        
    else:
        pass
        
        
        
    return is_seven_last
31/161:
# Check
spy_game([1,2,4,0,0,7,5])
31/162:
def spy_game(nums):
    out = ""
    for element in nums:
        out += str(element)
        
    zeros_list_index = []
    seven_index = 0 
    for i in range(len(out)):
         if out[i] == '0':
            zeros_list_index.append(i)
         
         elif out[i] == 7:
                seven_index = i
         
         else:
            pass
    
        
    is_seven_last = Tr
    if (max(zeros_list_index) > 7):
            is_seven_last == False
        
    else:
        pass
        
        
        
    return is_seven_last
31/163:
# Check
spy_game([1,2,4,0,0,7,5])
31/164:
def spy_game(nums):
    out = ""
    for element in nums:
        out += str(element)
        
    zeros_list_index = []
    seven_index = 0 
    for i in range(len(out)):
         if out[i] == '0':
            zeros_list_index.append(i)
         
         elif out[i] == 7:
                seven_index = i
         
         else:
            pass
    
        
    is_seven_last = True
    if (max(zeros_list_index) > 7):
            is_seven_last == False
        
    else:
        pass
        
        
        
    return is_seven_last
31/165:
# Check
spy_game([1,2,4,0,0,7,5])
31/166:
# Check
spy_game([1,0,2,4,0,5,7])
31/167:
# Check
spy_game([1,7,2,0,4,5,0])
31/168:
def spy_game(nums):
    out = ""
    for element in nums:
        out += str(element)
        
    zeros_list_index = []
    seven_index = 0 
    for i in range(len(out)):
         if out[i] == '0':
            zeros_list_index.append(i)
         
         elif out[i] == 7:
                seven_index = i
         
         else:
            pass
    
        
    is_seven_last = True
    if (max(zeros_list_index) > seven_index):
            is_seven_last == False
        
    else:
        pass
        
        
        
    return is_seven_last
31/169:
# Check
spy_game([1,2,4,0,0,7,5])
31/170:
# Check
spy_game([1,0,2,4,0,5,7])
31/171:
# Check
spy_game([1,7,2,0,4,5,0])
31/172:
def spy_game(nums):
    out = ""
    for element in nums:
        out += str(element)
        
    zeros_list_index = []
    seven_index = 0 
    for i in range(len(out)):
         if out[i] == '0':
            zeros_list_index.append(i)
         
         elif out[i] == 7:
                seven_index = i
         
         else:
            pass
    
        
    is_seven_last = True
    if (max(zeros_list_index) > seven_index):
            is_seven_last == False
        
    else:
        pass
        
        
        
    return seven_index
31/173:
# Check
spy_game([1,2,4,0,0,7,5])
31/174:
# Check
spy_game([1,0,2,4,0,5,7])
31/175:
# Check
spy_game([1,7,2,0,4,5,0])
31/176:
def spy_game(nums):
    out = ""
    for element in nums:
        out += str(element)
        
    zeros_list_index = []
    seven_index = 0 
    for i in range(len(out)):
         if out[i] == '0':
            zeros_list_index.append(i)
         
         elif out[i] == 7:
                seven_index = i
         
         else:
            pass
    
        
    is_seven_last = True
    if (max(zeros_list_index) > seven_index):
            is_seven_last == False
        
    else:
        pass
        
        
    print(out)   
    return seven_index
31/177:
# Check
spy_game([1,2,4,0,0,7,5])
31/178:
# Check
spy_game([1,0,2,4,0,5,7])
31/179:
# Check
spy_game([1,7,2,0,4,5,0])
31/180:
def spy_game(nums):
    out = ""
    for element in nums:
        out += str(element)
        
    zeros_list_index = []
    seven_index = 0 
    for i in range(len(out)):
         if out[i] == '0':
            zeros_list_index.append(i)
         
         elif out[i] == '7':
                seven_index = i
         
         else:
            pass
    
        
    is_seven_last = True
    if (max(zeros_list_index) > seven_index):
            is_seven_last == False
        
    else:
        pass
        
        
    return seven_index
31/181:
# Check
spy_game([1,2,4,0,0,7,5])
31/182:
# Check
spy_game([1,0,2,4,0,5,7])
31/183:
# Check
spy_game([1,7,2,0,4,5,0])
31/184:
def spy_game(nums):
    out = ""
    for element in nums:
        out += str(element)
        
    zeros_list_index = []
    seven_index = 0 
    for i in range(len(out)):
         if out[i] == '0':
            zeros_list_index.append(i)
         
         elif out[i] == '7':
                seven_index = i
         
         else:
            pass
    
        
    is_seven_last = True
    if (max(zeros_list_index) > seven_index):
            is_seven_last == False
        
    else:
        pass
        
        
    return is_seven_last
31/185:
# Check
spy_game([1,2,4,0,0,7,5])
31/186:
# Check
spy_game([1,0,2,4,0,5,7])
31/187:
# Check
spy_game([1,7,2,0,4,5,0])
31/188:
def spy_game(nums):
    out = ""
    for element in nums:
        out += str(element)
        
    zeros_list_index = []
    seven_index = 0 
    for i in range(len(out)):
         if out[i] == '0':
            zeros_list_index.append(i)
         
         elif out[i] == '7':
                seven_index = i
         
         else:
            pass
    
        
    is_seven_last = True
    if (max(zeros_list_index) > seven_index):
            is_seven_last == False
        
    else:
        pass
        
        
    return seven
31/189:
def spy_game(nums):
    out = ""
    for element in nums:
        out += str(element)
        
    zeros_list_index = []
    seven_index = 0 
    for i in range(len(out)):
         if out[i] == '0':
            zeros_list_index.append(i)
         
         elif out[i] == '7':
                seven_index = i
         
         else:
            pass
    
        
    is_seven_last = True
    if (max(zeros_list_index) > seven_index):
            is_seven_last == False
        
    else:
        pass
        
        
    return seven_index
31/190:
# Check
spy_game([1,2,4,0,0,7,5])
31/191:
# Check
spy_game([1,0,2,4,0,5,7])
31/192:
# Check
spy_game([1,7,2,0,4,5,0])
31/193:
def spy_game(nums):
    out = ""
    for element in nums:
        out += str(element)
        
    zeros_list_index = []
    seven_index = 0 
    for i in range(len(out)):
         if out[i] == '0':
            zeros_list_index.append(i)
         
         elif out[i] == '7':
                seven_index = i
         
         else:
            pass
    
        
    is_seven_last = True
    if (max(zeros_list_index) > seven_index):
            is_seven_last == False
        
    else:
        pass
        
    print(zeros_list_index)   
    return seven_index
31/194:
# Check
spy_game([1,2,4,0,0,7,5])
31/195:
# Check
spy_game([1,0,2,4,0,5,7])
31/196:
# Check
spy_game([1,7,2,0,4,5,0])
31/197:
def spy_game(nums):
    out = ""
    for element in nums:
        out += str(element)
        
    zeros_list_index = []
    seven_index = 0 
    for i in range(len(out)):
         if out[i] == '0':
            zeros_list_index.append(i)
         
         elif out[i] == '7':
                seven_index = i
         
         else:
            pass
    
        
   
    if (max(zeros_list_index) > seven_index):
        return False
        
    else:
        return True
31/198:
# Check
spy_game([1,2,4,0,0,7,5])
31/199:
# Check
spy_game([1,0,2,4,0,5,7])
31/200:
# Check
spy_game([1,7,2,0,4,5,0])
31/201:
def count_primes(num):
    prime_list = [2]
    non_prime_list = [1]
    for i in range(3,num + 1):
        isPrime = True
        for prime in prime_list:
            if i % prime == 0:
                isPrime = False:
            else:
                pass
            
            if (isPrime):
                prime_list.append(i)
            else:
                pass
            
            
            return prime_list.count()
31/202:
# Check
count_primes(100)
31/203:
def count_primes(num):
    prime_list = [2]
    non_prime_list = [1]
    for i in range(3,num + 1):
        isPrime = True
        for prime in prime_list:
            if i % prime == 0:
                isPrime = False
            else:
                pass
            
            if (isPrime):
                prime_list.append(i)
            else:
                pass
            
            
            return prime_list.count()
31/204:
# Check
count_primes(100)
31/205:
def count_primes(num):
    prime_list = [2]
    non_prime_list = [1]
    for i in range(3,num + 1):
        isPrime = True
        for prime in prime_list:
            if i % prime == 0:
                isPrime = False
            else:
                pass
            
            if (isPrime):
                prime_list.append(i)
            else:
                pass
            
            
            return len(prime_list)
31/206:
# Check
count_primes(100)
31/207:
def count_primes(num):
    prime_list = [2]
    non_prime_list = [1]
    for i in range(3,num + 1):
        isPrime = True
        for prime in prime_list:
            if i % prime == 0:
                isPrime = False
            else:
                break
            
            if (isPrime):
                prime_list.append(i)
            else:
                pass
            
            
            return len(prime_list)
31/208:
# Check
count_primes(100)
31/209:
def count_primes(num):
    prime_list = [2]
    non_prime_list = [1]
    for i in range(3,num + 1):
        isPrime = True
        for prime in prime_list:
            if i % prime == 0:
                isPrime = False
            else:
                pass
            
            if (isPrime):
                prime_list.append(i)
            else:
                pass
            
            
            return (prime_list)
31/210:
# Check
count_primes(100)
31/211:
def count_primes(num):
    prime_list = [2]
    non_prime_list = [1]
    for i in range(3,num + 1):
        isPrime = True
        for prime in prime_list:
            if i % prime == 0:
                isPrime = False
                continue
            else:
                pass
            
            if (isPrime):
                prime_list.append(i)
            else:
                pass
            
            
            return (prime_list)
31/212:
# Check
count_primes(100)
31/213:
def count_primes(num):
    prime_list = [2]
    non_prime_list = [1]
    for i in range(3,num + 1):
        isPrime = True
        for prime in prime_list:
            if i % prime == 0:
                isPrime = False
                continue
            else:
                pass
            
            if (isPrime):
                prime_list.append(i)
            else:
                pass
            
            
            return (prime_list)
31/214:
# Check
count_primes(100)
31/215:
def count_primes(num):
    prime_list = [2]
    non_prime_list = [1]
    for i in range(3,num + 1):
        isPrime = True
        for prime in prime_list:
            if i % prime == 0:
                isPrime = False
                
            else:
                pass
            
        if (isPrime):
            prime_list.append(i)
        else:
            pass
            
            
    return (prime_list)
31/216:
# Check
count_primes(100)
31/217:
def count_primes(num):
    prime_list = [2]
    non_prime_list = [1]
    for i in range(3,num + 1):
        isPrime = True
        for prime in prime_list:
            if i % prime == 0:
                isPrime = False
                
            else:
                pass
            
        if (isPrime):
            prime_list.append(i)
        else:
            pass
                
            
    return len(prime_list)
31/218:
# Check
count_primes(100)
31/219:
def paper_doll(text):
    temp = ""
    for i in text:
        temp += i*3
        
    return temp
31/220:
# Check
paper_doll('Hello')
31/221:
# Check
paper_doll('Mississippi')
31/222:
def animal_crackers(text):
    text_list = text.lower.split(" ")
    if text_list[0][0] == text_list[1][0]:
        return True
    else:
        return False
31/223:
# Check
animal_crackers('Levelheaded Llama')
31/224:
def animal_crackers(text):
    text_list = text.lower().split(" ")
    if text_list[0][0] == text_list[1][0]:
        return True
    else:
        return False
31/225:
# Check
animal_crackers('Levelheaded Llama')
31/226:
# Check
animal_crackers('Crazy Kangaroo')
31/227:
def almost_there(n):
    return (abs(100 -n) <= 10) or (abs(200 - n) <= 10)
31/228:
# Check
almost_there(104)
31/229:
# Check
almost_there(150)
31/230:
# Check
almost_there(209)
31/231:
def count_primes(num):
    prime_list = [2]
    non_prime_list = [1]
    for i in range(3,num + 1,2):
        isPrime = True
        for prime in prime_list:
            if i % prime == 0:
                isPrime = False
                
            else:
                pass
            
        if (isPrime):
            prime_list.append(i)
        else:
            pass
                
            
    return len(prime_list)
31/232:
# Check
count_primes(100)
31/233:
def print_big(letter):
 patterns = {1:'  *  ',2:' * * ',3:'*   *',4:'*****',5:'**** ',6:'   * ',7:' *   ',8:'*   * ',9:'*    '}
    alphabet = {'A':[1,2,4,3,3],'B':[5,3,5,3,5],'C':[4,9,9,9,4],'D':[5,3,3,3,5],'E':[4,9,4,9,4]}
    for pattern in alphabet[letter.upper()]:
        print(patterns[pattern])
31/234:
def print_big(letter):
    patterns = {1:'  *  ',2:' * * ',3:'*   *',4:'*****',5:'**** ',6:'   * ',7:' *   ',8:'*   * ',9:'*    '}
    alphabet = {'A':[1,2,4,3,3],'B':[5,3,5,3,5],'C':[4,9,9,9,4],'D':[5,3,3,3,5],'E':[4,9,4,9,4]}
    for pattern in alphabet[letter.upper()]:
        print(patterns[pattern])
31/235: print_big('a')
31/236: print_big('b')
31/237: print_big('c')
31/238: print_big('d')
31/239: print_big('e')
33/1:
def square(num):
    return num**2
33/2: my_nums = [1,2,3,4,5]
33/3:  map(square, my_nums)
33/4:
for item in map(square, my_nums):
    print(item)
33/5: list(map(square,my_nums))
33/6:
def splicer(mystring):
    return mystring.split()
33/7: splicer("HELLO WPRLDSASD ASDASDA")
33/8: names = ["Andy", "Eve","Sally"]
33/9: type(map())
33/10: type(map(splicer,names))
33/11: list(map(splicer,names))
33/12: (map(splicer,names))
33/13: list((map(splicer,names))
33/14: list((map(splicer,names)))
33/15:
def splicer(mystring):
    if len(mystring) % 2 == 0:
        return 'EVEN'
    else:
        return mystring[0]
33/16: names = ["Andy", "Eve","Sally"]
33/17: list((map(splicer,names)))
33/18: Map fonskiyonlarÄ±nÄ±, bir fonksiyona aynÄ± anda uygulanacak Ã§ok eleman
33/19:
def check_even(num):
    return num % 2 == 0
33/20: mynums = [1,2,3,4,5,6]
33/21: filter(check_even,my_nums)
33/22:
for n in filter(check_even,my_nums):
    print(n)
33/23: mynums = [1,2,3,4,5,6]
33/24: list(filter(check_even,my_nums))
33/25:
for n in filter(check_even,my_nums):
    print(n)
33/26:
def check_even(num):
    return num % 2 == 0
33/27: mynums = [1,2,3,4,5,6]
33/28: list(filter(check_even,mynums))
33/29:
for n in filter(check_even,my_nums):
    print(n)
33/30:
for n in filter(check_even,mynums):
    print(n)
33/31:
def check_even(num):
    a = num % 3
    return a
33/32: mynums = [1,2,3,4,5,6]
33/33: list(filter(check_even,mynums))
33/34:
for n in filter(check_even,mynums):
    print(n)
33/35:
def check_even(num):
    a = num % 5
    return a
33/36: mynums = [1,2,3,4,5,6]
33/37: list(filter(check_even,mynums))
33/38:
for n in filter(check_even,mynums):
    print(n)
33/39:
def check_even(num):
    a = num % 6
    return a
33/40: mynums = [1,2,3,4,5,6]
33/41: list(filter(check_even,mynums))
33/42:
for n in filter(check_even,mynums):
    print(n)
33/43:
def check_even(num):
    a = num % 2
    return a
33/44: mynums = [1,2,3,4,5,6]
33/45: list(filter(check_even,mynums))
33/46:
for n in filter(check_even,mynums):
    print(n)
33/47:
def check_even(num):
    
    return num % 2 == 0
33/48: mynums = [1,2,3,4,5,6]
33/49: list(filter(check_even,mynums))
33/50:
for n in filter(check_even,mynums):
    print(n)
33/51: square(3)
33/52: square = lambda num : num **2
33/53: square(2)
33/54: list(map(lambda num:num**2,mynums))
33/55: filter(lambda num: return num % 2 == 0,mynums)
33/56: filter(lambda num: num % 2 == 0,mynums)
33/57: list(filter(lambda num: num % 2 == 0,mynums))
33/58: names
33/59: list(map(lambda x:x[0],names))
33/60: list(map(lambda x:x[::-1],names))
35/1:
x = 25

def printer():
    x = 50
    return x
35/2: print(x)
35/3: print(printer())
34/1:
x = 50

def func():
    global x
    print('This function is now using the global x!')
    print('Because of global x is: ', x)
    x = 2
    print('Ran func(), changed global x to', x)

print('Before calling func(), x is: ', x)
func()
print('Value of x (outside of func()) is: ', x)
34/2:
x = 50

def func():
    global x
    print('This function is now using the global x!')
    print('Because of global x is: ', x)
    x = 2
    print('Ran func(), changed global x to', x)

print('Before calling func(), x is: ', x)
func()
print('Value of x (outside of func()) is: ', x)
36/1:
import math
def vol(rad):
    return (4/3)*math.pi*(rad**3)
36/2:
# Check
vol(2)
36/3:
import math
def vol(rad):
    return (4/3)*3*(rad**3)
36/4:
# Check
vol(2)
36/5:
import math
def vol(rad):
    return (4/3)*math.pi*(rad*rad*rad)
36/6:
# Check
vol(2)
36/7:
import math
def vol(rad):
    return (4/3)*3*(rad*rad*rad)
36/8:
# Check
vol(2)
36/9:
def ran_check(num,low,high):
    return num in range(low, high + 1)
36/10:
# Check
ran_check(5,2,7)
36/11: ran_bool(3,1,10)
36/12:
def ran_bool(num,low,high):
    return num in range(low, high + 1)
36/13: ran_bool(3,1,10)
36/14:
def ran_check(num,low,high):
    if num in range(low, high + 1):
        return f'{num} is between {low} and {high}'
36/15:
# Check
ran_check(5,2,7)
36/16:
def up_low(s):
    up = 0
    low = 0
36/17:
def up_low(s):
    up = 0
    low = 0
    for letter in s:
        if letter.isupper():
            up+=1
        elif letter.islower():
            low+=1
    print("No. of Upper case characters : ",up)
    print("No. of Lower case Characters : ",low)
36/18:
s = 'Hello Mr. Rogers, how are you this fine Tuesday?'
up_low(s)
36/19:
def up_low(s):
    up = 0
    low = 0
    for letter in s:
        if letter.isupper():
            up+=1
        elif letter.islower():
            low+=1
    print("No. of Upper case characters : ",up)
    print("No. of Lower case Characters : ",low)
36/20:
s = 'Hello Mr. Rogers, how are you this fine Tuesday?'
up_low(s)
36/21:
def unique_list(lst):
    set(list(lst))
36/22: unique_list([1,1,1,1,2,2,3,3,3,3,4,5])
36/23:
def unique_list(lst):
    return set(list(lst))
36/24:
def unique_list(lst):
    return set(list(lst))
36/25:
def unique_list(lst):
    return set(list(lst))
36/26: unique_list([1,1,1,1,2,2,3,3,3,3,4,5])
36/27:
def unique_list(lst):
    return list(set(lst))
36/28: unique_list([1,1,1,1,2,2,3,3,3,3,4,5])
36/29:
def multiply(numbers):  
    sum = 1
    for i in range(len(numbers)):
        sum *= numbers[i]
        i+=1
36/30: multiply([1,2,3,-4])
36/31:
def multiply(numbers):  
    sum = 1
    for i in range(len(numbers)):
        sum *= numbers[i]
        i+=1
    return sum
36/32: multiply([1,2,3,-4])
36/33:
def palindrome(s):
    new_s = s.replace(" ", "")
    return s == new_s
36/34: palindrome('helleh')
36/35:
def palindrome(s):
    new_s = s.replace(" ", "")
    return s == new_s
36/36: palindrome('hellh')
36/37:
def palindrome(s):
    new_s = s.replace(" ", "")
    return s[::-1] == new_s
36/38: palindrome('helleh')
36/39:
def palindrome(s):
    new_s = s.replace(" ", "")
    return s[::-1] == new_s
36/40: palindrome('helle')
36/41:
def palindrome(s):
    new_s = s.replace(" ", "")
    return s[::-1] == new_s
36/42: palindrome('nurses run')
36/43:
def palindrome(s):
    new_s = s.replace(" ", "")
    return new_s[::-1] == new_s
36/44: palindrome('nurses run')
36/45:
def palindrome(s):
    new_s = s.replace(" ", "")
    return new_s[::-1] == new_s
36/46: palindrome('helleh')
36/47:
import string

def ispangram(str1, alphabet=string.ascii_lowercase):
    print(list(str1))
36/48: ispangram("The quick brown fox jumps over the lazy dog")
36/49:
import string

def ispangram(str1, alphabet=string.ascii_lowercase):
    list(str1).sort()
36/50: ispangram("The quick brown fox jumps over the lazy dog")
36/51:
import string

def ispangram(str1, alphabet=string.ascii_lowercase):
    return list(str1).sort()
36/52: ispangram("The quick brown fox jumps over the lazy dog")
36/53:
import string

def ispangram(str1, alphabet=string.ascii_lowercase):
    return list(str1).sort()
36/54: ispangram("The quick brown fox jumps over the lazy dog")
36/55: string.ascii_lowercase
36/56:
import string

def ispangram(str1, alphabet=string.ascii_lowercase): 
    return set(list(str1).sort())
36/57: ispangram("The quick brown fox jumps over the lazy dog")
36/58:
import string

def ispangram(str1, alphabet=string.ascii_lowercase): 
    list(str1).sort()
    return set(list(str1))
36/59: ispangram("The quick brown fox jumps over the lazy dog")
36/60:
import string

def ispangram(str1, alphabet=string.ascii_lowercase): 
    list(str1).sort() 
    return set(list(str1)) set(string.ascii_lowercase)
36/61:
import string

def ispangram(str1, alphabet=string.ascii_lowercase): 
    list(str1).sort() 
    return set(list(str1))in set(string.ascii_lowercase)
36/62: ispangram("The quick brown fox jumps over the lazy dog")
36/63:
import string

def ispangram(str1, alphabet=string.ascii_lowercase): 
    list(str1).sort() 
   #return set(list(str1))in set(string.ascii_lowercase)
    return set(string.ascii_lowercase)
36/64: ispangram("The quick brown fox jumps over the lazy dog")
36/65:
import string

def ispangram(str1, alphabet=string.ascii_lowercase): 
    list(str1).sort() 
   #return set(list(str1))in set(string.ascii_lowercase)
    return {'a'} in set(string.ascii_lowercase)
36/66: ispangram("The quick brown fox jumps over the lazy dog")
36/67:
import string

def ispangram(str1, alphabet=string.ascii_lowercase): 
    list(str1).sort() 
   #return set(list(str1))in set(string.ascii_lowercase)
    return 'a'in set(string.ascii_lowercase)
36/68: ispangram("The quick brown fox jumps over the lazy dog")
36/69:
import string

def ispangram(str1, alphabet=string.ascii_lowercase): 
    list(str1).sort() 
    return set(list(str1))in (string.ascii_lowercase)
36/70: ispangram("The quick brown fox jumps over the lazy dog")
36/71:
import string

def ispangram(str1, alphabet=string.ascii_lowercase): 
    return str1.sort()
36/72: ispangram("The quick brown fox jumps over the lazy dog")
36/73:
import string

def ispangram(str1, alphabet=string.ascii_lowercase): 
    emp = []
    for letter in str1:
        emp.append(letter)
    emp.sort()
    return emp
36/74: ispangram("The quick brown fox jumps over the lazy dog")
36/75:
import string

def ispangram(str1, alphabet=string.ascii_lowercase): 
    emp = []
    for letter in str1:
        emp.append(letter)
    emp.sort()
    emp = list(set(emp))
    emp_str = ""
    return emp
36/76:
import string

def ispangram(str1, alphabet=string.ascii_lowercase): 
    emp = []
    for letter in str1:
        emp.append(letter)
    emp.sort()
  # emp = list(set(emp))
    emp_str = ""
    return emp
36/77: ispangram("The quick brown fox jumps over the lazy dogg")
36/78:
import string

def ispangram(str1, alphabet=string.ascii_lowercase): 
    emp = []
    for letter in str1:
        emp.append(letter)
    emp.sort()
    emp = list(set(emp))
    emp_str = ""
    return emp
36/79: ispangram("The quick brown fox jumps over the lazy dogg")
36/80:
import string

def ispangram(str1, alphabet=string.ascii_lowercase): 
    emp = []
    for letter in str1:
        emp.append(letter)
    emp.sort()
    emp = list(set(emp))
    emp_str = ""
    return emp
36/81: ispangram("The quick brown fox jumps over the lazy dogg")
36/82:
import string

def ispangram(str1, alphabet=string.ascii_lowercase): 
    emp = []
    for letter in str1:
        emp.append(letter)
    emp = list(set(emp))
    emp.sort()
    emp_str = ""
    return emp
36/83: ispangram("The quick brown fox jumps over the lazy dogg")
36/84:
import string

def ispangram(str1, alphabet=string.ascii_lowercase): 
    emp = []
    for letter in str1:
        emp.append(letter)
    emp = list(set(emp))
    
    emp_str = ""
    
    for letter in emp:
        if letter.isalpha() and letter.islower()
            pass
        else:
            emp.remove(letter)
    emp.sort()
    return emp == list(string.ascii_lowercase)
36/85:
import string

def ispangram(str1, alphabet=string.ascii_lowercase): 
    emp = []
    for letter in str1:
        emp.append(letter)
    emp = list(set(emp))
    
    emp_str = ""
    
    for letter in emp:
        if letter.isalpha() and letter.islower():
            pass
        else:
            emp.remove(letter)
    emp.sort()
    return emp == list(string.ascii_lowercase)
36/86: ispangram("The quick brown fox jumps over the lazy dogg")
36/87:
import string

def ispangram(str1, alphabet=string.ascii_lowercase): 
    emp = []
    for letter in str1:
        emp.append(letter)
    emp = list(set(emp))
    
    emp_str = ""
    
    for letter in emp:
        if letter.isalpha() and letter.islower():
            pass
        else:
            emp.remove(letter)
    emp.sort()
    return emp == list(string.ascii_lowercase)
36/88: ispangram("The quick brown fox jumps over the lazy do")
36/89:
import string

def ispangram(str1, alphabet=string.ascii_lowercase): 
    emp = []
    for letter in str1:
        emp.append(letter)
    emp = list(set(emp))
    
    emp_str = ""
    
    for letter in emp:
        if letter.isalpha() and letter.islower():
            pass
        else:
            emp.remove(letter)
    emp.sort()
    return emp
36/90: ispangram("The quick brown fox jumps over the lazy dog)
36/91:
import string

def ispangram(str1, alphabet=string.ascii_lowercase): 
    emp = []
    for letter in str1:
        emp.append(letter)
    emp = list(set(emp))
    
    emp_str = ""
    
    for letter in emp:
        if letter.isalpha() and letter.islower():
            pass
        else:
            emp.remove(letter)
    emp.sort()
    return emp
36/92: ispangram("The quick brown fox jumps over the lazy dog")
37/1:
import string

def ispangram(str1, alphabet=string.ascii_lowercase): 
    emp = []
    for letter in str1:
        emp.append(letter)
    emp = list(set(emp))
    
    emp_str = ""
    
    for letter in emp:
        if letter.isalpha() and letter.islower():
            pass
        else:
            emp.remove(letter)
    emp.sort()
    return emp == list(string.ascii_lowercase)
37/2: ispangram("The quick brown fox jumps over the lazy dog")
37/3: string.ascii_lowercase
37/4:
import string

def ispangram(str1, alphabet=string.ascii_lowercase): 
    emp = []
    for letter in str1:
        emp.append(letter)
    emp = list(set(emp))
    
    emp_str = ""
    
    for letter in emp:
        if letter.isalpha() and letter.islower():
            pass
        else:
            emp.remove(letter)
    emp.sort()
    return emp == list(string.ascii_lowercase)
37/5: ispangram("The quick brown fox jumps over the lazy dg")
37/6:
import string

def ispangram(str1, alphabet=string.ascii_lowercase): 
    emp = []
    for letter in str1:
        emp.append(letter)
    emp = list(set(emp))
    
    emp_str = ""
    
    for letter in emp:
        if letter.isalpha() and letter.islower():
            pass
        else:
            emp.remove(letter)
    emp.sort()
    return emp == list(string.ascii_lowercase)
37/7: ispangram("The quick brown fox umps over the lazy dog")
37/8:
import string

def ispangram(str1, alphabet=string.ascii_lowercase): 
    emp = []
    str1 = str1.lower()
    for letter in str1:
        emp.append(letter)
    emp = list(set(emp))
    
    emp_str = ""
    
    for letter in emp:
        if letter.isalpha() and letter.islower():
            pass
        else:
            emp.remove(letter)
    emp.sort()
    return emp == list(string.ascii_lowercase)
37/9: ispangram("The quick brown fox jumps over the lazy dog")
37/10: string.ascii_lowercase
37/11:
import string

def ispangram(str1, alphabet=string.ascii_lowercase): 
    emp = []
   # str1 = str1.lower()
    for letter in str1:
        emp.append(letter)
    emp = list(set(emp))
    
    emp_str = ""
    
    for letter in emp:
        if letter.isalpha() and letter.islower():
            pass
        else:
            emp.remove(letter)
    emp.sort()
    return emp == list(string.ascii_lowercase)
37/12: ispangram("The quick brown Fox jumps over the lazy dog")
37/13:
import string

def ispangram(str1, alphabet=string.ascii_lowercase): 
    emp = []
    str1 = str1.lower()
    for letter in str1:
        emp.append(letter)
    emp = list(set(emp))
    
    emp_str = ""
    
    for letter in emp:
        if letter.isalpha() and letter.islower():
            pass
        else:
            emp.remove(letter)
    emp.sort()
    return emp == list(string.ascii_lowercase)
37/14: ispangram("The quick brown Fox jumps over the lazy dog")
38/1: print([1,2,3])
38/2:
def display(row1,row2, row3):
    print(row1)
38/3:
row1 = [' ',' ',' ']
row2 = [' ',' ',' ']
row3 = [' ',' ',' ']

display(row1,row2,row3)
38/4:
def display(row1,row2,row3):
    print(row1)
    print(row2)
    print(row3)
38/5:
row1 = [' ',' ',' ']
row2 = [' ',' ',' ']
row3 = [' ',' ',' ']

display(row1,row2,row3)
38/6:
row1 = [' ',' ',' ']
row2 = [' ',' ',' ']
row3 = [' ',' ',' ']

display(row1,row2,row3)
38/7: row2[1] = 'X'
38/8: display(row1,row2,row3)
38/9: input("Please enter a value: ")
38/10: result = input("Please enter a value: ")
38/11: type(result)
38/12: result_int = int(result)
38/13: type(result_int)
41/1:
from IPython.display import clear_output

def display_board(board):
    for i in range(10):
    clear_output(wait=True)
    print("Hello World!")
41/2:
from IPython.display import clear_output

def display_board(board):
    for i in range(10):
        clear_output(wait=True)
        print("Hello World!")
41/3:
from IPython.display import clear_output

def display_board(board):
    
    for i in range(10):
        i = 0
        clear_output(wait=True)
        print("Hello World!")
        ++i
41/4:
from IPython.display import clear_output

def display_board(board):
    
    for i in range(10):
        i = 0
        clear_output(wait=True)
        print("Hello World!")
        ++i
41/5:
from IPython.display import clear_output

def display_board(board):
    
    for i in range(10):
        i = 0
        clear_output(wait=False)
        print("Hello World!")
        ++i
41/6:
from IPython.display import clear_output

def display_board(board):
    
    for i in range(10):
        i = 0
        clear_output(wait=False)
        print("Hello World!")
        ++i
41/7:
from IPython.display import clear_output

def display_board(board):
    print(board)
41/8:
test_board = ['#','X','O','X','O','X','O','X','O','X']
display_board(test_board)
41/9:
print("__|__|__")
print("__|__|__")
print("  |  |  ")
41/10:
def player_input():
    list1 = ["__","|","__","|","__"]
    list2 = ["__","|","__","|","__"]
    list3 = ["__","|","__","|","__"]
    row1 = ""
    row2 = ""
    row3 = ""
   
    for i in range(len(list1)):
        row1 +=list1[i]:
        row2 +=list2[i]:
        row2 +=list3[i];
    
    player_choice = input("make a choice between X or O")
    valid_list = ['X','0']
    
    while player_choice not in valid_list:
        player_choice = input("make a choice between X or O")
    else:
        print("Chose the location of {}".format(player_choice))
        print("Like:")
        print(row1,row2,row3)
    pass
41/11:
def player_input():
    list1 = ["__","|","__","|","__"]
    list2 = ["__","|","__","|","__"]
    list3 = ["__","|","__","|","__"]
    row1 = ""
    row2 = ""
    row3 = ""
   
    for i in range(len(list1)):
        row1 +=list1[i]
        row2 +=list2[i]
        row2 +=list3[i]
    
    player_choice = input("make a choice between X or O")
    valid_list = ['X','0']
    
    while player_choice not in valid_list:
        player_choice = input("make a choice between X or O")
    else:
        print("Chose the location of {}".format(player_choice))
        print("Like:")
        print(row1,row2,row3)
    pass
41/12:
def player_input():
    list1 = ["__","|","__","|","__"]
    list2 = ["__","|","__","|","__"]
    list3 = ["__","|","__","|","__"]
    row1 = ""
    row2 = ""
    row3 = ""
   
    for i in range(len(list1)):
        row1 +=list1[i]
        row2 +=list2[i]
        row2 +=list3[i]
    
    player_choice = input("make a choice between X or O")
    valid_list = ['X','0']
    
    while player_choice not in valid_list:
        player_choice = input("make a choice between X or O")
    else:
        print("Chose the location of {}".format(player_choice))
        print("Like:")
        print(row1,row2,row3)
    pass
41/13: player_input()
42/1: player_input()
42/2:
def player_input():
    list1 = ["__","|","__","|","__"]
    list2 = ["__","|","__","|","__"]
    list3 = ["__","|","__","|","__"]
    row1 = ""
    row2 = ""
    row3 = ""
   
    for i in range(len(list1)):
        row1 +=list1[i]
        row2 +=list2[i]
        row2 +=list3[i]
    
    player_choice = input("make a choice between X or O")
    valid_list = ['X','0']
    
    while player_choice not in valid_list:
        player_choice = input("make a choice between X or O")
    else:
        print("Chose the location of {}".format(player_choice))
        print("Like:")
        print(row1,row2,row3)
    pass
42/3:
def player_input():
    list1 = ["__","|","__","|","__"]
    list2 = ["__","|","__","|","__"]
    list3 = ["__","|","__","|","__"]
    row1 = ""
    row2 = ""
    row3 = ""
   
    for i in range(len(list1)):
        row1 +=list1[i]
        row2 +=list2[i]
        row2 +=list3[i]
    
    player_choice = input("make a choice between X or O")
    valid_list = ['X','0']
    
    while player_choice not in valid_list:
        player_choice = input("make a choice between X or O")
    else:
        print("Chose the location of {}".format(player_choice))
        print("Like:")
        print(row1,row2,row3)
    pass
42/4: player_input()
43/1:
def player_input():
    list1 = ["__","|","__","|","__"]
    list2 = ["__","|","__","|","__"]
    list3 = ["__","|","__","|","__"]
    row1 = ""
    row2 = ""
    row3 = ""
   
    for i in range(len(list1)):
        row1 +=list1[i]
        row2 +=list2[i]
        row2 +=list3[i]
    
    player_choice = input("make a choice between X or O")
    valid_list = ['X','O']
    
    while player_choice not in valid_list:
        player_choice = input("make a choice between X or O")
    else:
        print("Chose the location of {}".format(player_choice))
        print("Like:")
        print(row1,row2,row3)
    pass
43/2: player_input()
43/3:
def player_input():
    list1 = ["__","|","__","|","__"]
    list2 = ["__","|","__","|","__"]
    list3 = ["__","|","__","|","__"]
    row1 = ""
    row2 = ""
    row3 = ""
   
    for i in range(len(list1)):
        row1 +=list1[i]
        row2 +=list2[i]
        row2 +=list3[i]
    
    player_choice = input("make a choice between X or O")
    valid_list = ['X','O']
    
    while player_choice not in valid_list:
        player_choice = input("make a choice between X or O")
    else:
        print("Chose the location of {}".format(player_choice))
        print("Like:")
        print(row1)
        print(row2)
        print(row3)
    pass
43/4: player_input()
43/5:
def player_input():
    list1 = ["__","|","__","|","__"]
    list2 = ["__","|","__","|","__"]
    list3 = ["__","|","__","|","__"]
    row1 = ""
    row2 = ""
    row3 = ""
   
    for i in range(len(list1)):
        row1 +=list1[i]
        row2 +=list2[i]
        row2 +=list3[i]
    
    player_choice = input("make a choice between X or O")
    valid_list = ['X','O']
    
    while player_choice not in valid_list:
        player_choice = input("make a choice between X or O")
    else:
        print("Chose the location of {}".format(player_choice))
        print("Like:")
        print(row1)
        print(row2)
        print(row3)
    pass
43/6: player_input()
43/7:
def player_input():
    list1 = ["__","|","__","|","__"]
    list2 = ["__","|","__","|","__"]
    list3 = ["__","|","__","|","__"]
    row1 = ""
    row2 = ""
    row3 = ""
   
    for i in range(len(list1)):
        row1 +=list1[i]
        row2 +=list2[i]
        row3 +=list3[i]
    
    player_choice = input("make a choice between X or O")
    valid_list = ['X','O']
    
    while player_choice not in valid_list:
        player_choice = input("make a choice between X or O")
    else:
        print("Chose the location of {}".format(player_choice))
        print("Like:")
        print(row1)
        print(row2)
        print(row3)
    pass
43/8: player_input()
43/9:
def player_input():
    list1 = ["__","|","__","|","__"]
    list2 = ["__","|","__","|","__"]
    list3 = ["  ","|","  ","|","  "]
    row1 = ""
    row2 = ""
    row3 = ""
   
    for i in range(len(list1)):
        row1 +=list1[i]
        row2 +=list2[i]
        row3 +=list3[i]
    
    player_choice = input("make a choice between X or O")
    valid_list = ['X','O']
    
    while player_choice not in valid_list:
        player_choice = input("make a choice between X or O")
    else:
        print("Chose the location of {}".format(player_choice))
        print("Like:")
        print(row1)
        print(row2)
        print(row3)
    pass
43/10: player_input()
43/11:
def player_input():
    list1 = ["__","|","__","|","__"]
    list2 = ["__","|","__","|","__"]
    list3 = ["  ","|","  ","|","  "]
    row1 = ""
    row2 = ""
    row3 = ""
   
    for i in range(len(list1)):
        row1 +=list1[i]
        row2 +=list2[i]
        row3 +=list3[i]
    
    player_choice = input("make a choice between X or O")
    valid_list = ['X','O']
    
    while player_choice not in valid_list:
        player_choice = input("make a choice between X or O")
    else:
        print("Chose the location of {}".format(player_choice))
        print("Like:")
        
    listA = print(" 7 ","|"," 8 ","|"," 9 ")
    listB = print(" 6 ","|"," 5 ","|"," 4 ")
    listC = print(" 1 ","|"," 2 ","|"," 3 ")
    pass
43/12: player_input()
43/13:
test_board = ['#','X','O\n','X','O','X','O','X','O','X']
display_board(test_board)
43/14:
from IPython.display import clear_output

def display_board(board):
    print(board)
43/15:
test_board = ['#','X','O\n','X','O','X','O','X','O','X']
display_board(test_board)
43/16:
from IPython.display import clear_output


def display_board(board):
    list1 = ["__","|","__","|","__"]
    list2 = ["__","|","__","|","__"]
    list3 = ["  ","|","  ","|","  "]
    board = list1 + list2 + list3
    print(board)
43/17:
test_board = ['#','X','O\n','X','O','X','O','X','O','X']
display_board(test_board)
43/18:
def player_input():
    
    player_choice = input("make a choice between X or O")
   
    while player_choice not in valid_list:
        player_choice = input("make a choice between X or O")
    else:
        
    valid_list = ['X','O']
    
    
    print("Choose the location of {}".format(player_choice))
    print("Like:")
        
        listA = print(" 7 ","|"," 8 ","|"," 9 ")
        listB = print(" 4 ","|"," 5 ","|"," 6")
        listC = print(" 1 ","|"," 2 ","|"," 3 ")
        
        player_index = int(input("1 to 9:\n"))
        
        while player_index not in (range(1,10)):
            print("Unvalid input please retry: \n")
43/19:
def player_input():
    valid_list = ['X','O']
    player_choice = input("make a choice between X or O")
   
    while player_choice not in valid_list:
        player_choice = input("make a choice between X or O")
    else:
    
        print("Choose the location of {}".format(player_choice))
        print("Like:")
        
        listA = print(" 7 ","|"," 8 ","|"," 9 ")
        listB = print(" 4 ","|"," 5 ","|"," 6")
        listC = print(" 1 ","|"," 2 ","|"," 3 ")
        
        player_index = int(input("1 to 9:\n"))
        
        while player_index not in (range(1,10)):
            print("Unvalid input please retry: \n")
43/20: player_input()
43/21:
def player_input():
    valid_list = ['X','O']
    player_choice = input("make a choice between X or O")
   
    while player_choice not in valid_list:
        player_choice = input("make a choice between X or O")
    else:
    
        print("Choose the location of {}".format(player_choice))
        print("Like:")
        
        print(" 7 ","|"," 8 ","|"," 9 ")
        print(" 4 ","|"," 5 ","|"," 6 ")
        print(" 1 ","|"," 2 ","|"," 3 ")
        
        player_index = int(input("1 to 9:\n"))
        
        
        
        while player_index not in (range(1,10)):
            print("Unvalid input please retry: \n")
        else:
        
            row1 = ""
            row2 = ""
            row3 = ""
   
        for i in range(len(list1)):
            row1 +=listA[i]
            row2 +=listB[i]
            row3 +=listC[i]
43/22: player_input()
43/23:
def player_input():
    valid_list = ['X','O']
    player_choice = input("make a choice between X or O")
   
    while player_choice not in valid_list:
        player_choice = input("make a choice between X or O")
    else:
    
        print("Choose the location of {}".format(player_choice))
        print("Like:")
        
        print(" 7 ","|"," 8 ","|"," 9 ")
        print(" 4 ","|"," 5 ","|"," 6 ")
        print(" 1 ","|"," 2 ","|"," 3 ")
        
        player_index = int(input("1 to 9:\n"))
        
        
        
        while player_index not in (range(1,10)):
            print("Unvalid input please retry: \n")
        else:
        
            row1 = ""
            row2 = ""
            row3 = ""
   
        for i in range(len(list1)):
            row1 +=listA[i]
            row2 +=listB[i]
            row3 +=listC[i]
43/24: player_input()
43/25:
def player_input():
    valid_list = ['X','O']
    player_choice = input("make a choice between X or O")
   
    while player_choice not in valid_list:
        player_choice = input("make a choice between X or O")
    else:
    
        print("Choose the location of {}".format(player_choice))
        print("Like:")
        
        print(" 7 ","|"," 8 ","|"," 9 ")
        print(" 4 ","|"," 5 ","|"," 6 ")
        print(" 1 ","|"," 2 ","|"," 3 ")
        
        player_index = int(input("1 to 9:\n"))
        
        
        
        while player_index not in (range(1,10)):
            print("Unvalid input please retry: \n")
        else:
        
            row1 = ""
            row2 = ""
            row3 = ""
   
        for i in range(len(listA)):
            row1 +=listA[i]
            row2 +=listB[i]
            row3 +=listC[i]
43/26: player_input()
45/1:
mylist = [1,2,3]
myset = set()
45/2: type(myset)
45/3:
class Sample():
    pass
45/4: my_sample = Sample()
45/5: type(my_sample)
45/6: my_dog = Dog()
45/7:
class Dog():
    
    def __init__(self,breed):
        
        self.breed = breed
45/8: my_sample = Sample()
45/9: type(my_sample)
45/10: my_dog = Dog()
45/11: my_dog = Dog(breed = "Lab")
45/12: type(my_dog)
45/13: type(my_dog.breed)
45/14: type(my_dog.breed)
45/15: my_dog.breed
45/16:
class Dog():
    
    def __init__(self,mybreed):
        
        self.breed = nybreed
43/27:
def player_input():
    valid_list = ['X','O']
    player_choice = input("make a choice between X or O")
   
    while player_choice not in valid_list:
        player_choice = input("make a choice between X or O")
    else:
    
        print("Choose the location of {}".format(player_choice))
        print("Like:")
        
        print(" 7 ","|"," 8 ","|"," 9 ")
        print(" 4 ","|"," 5 ","|"," 6 ")
        print(" 1 ","|"," 2 ","|"," 3 ")
        
        player_index = int(input("1 to 9:\n"))
        
        
        
        while player_index not in (range(1,10)):
            print("Unvalid input please retry: \n")
        else:
        
            row1 = ""
            row2 = ""
            row3 = ""
   
        for i in range(len(listA)):
            row1 +=listA[i]
            row2 +=listB[i]
            row3 +=listC[i]
43/28: player_input()
43/29:
def player_input():
    valid_list = ['X','O']
    player_choice = input("make a choice between X or O")
   
    while player_choice not in valid_list:
        player_choice = input("make a choice between X or O")
    else:
    
        print("Choose the location of {}".format(player_choice))
        print("Like:")
        
         listA =[" 7 ","|"," 8 ","|"," 9 "]
         listB =[" 4 ","|"," 5 ","|"," 6 "]
         listC =[" 1 ","|"," 2 ","|"," 3 "]
        
        player_index = int(input("1 to 9:\n"))
        
        
        
        while player_index not in (range(1,10)):
            print("Unvalid input please retry: \n")
        else:
        
            row1 = ""
            row2 = ""
            row3 = ""
   
        for i in range(len(listA)):
            row1 +=listA[i]
            row2 +=listB[i]
            row3 +=listC[i]
43/30: player_input()
43/31:
def player_input():
    valid_list = ['X','O']
    player_choice = input("make a choice between X or O")
   
    while player_choice not in valid_list:
        player_choice = input("make a choice between X or O")
    else:
    
        print("Choose the location of {}".format(player_choice))
        print("Like:")
        
         listA =[" 7 ","|"," 8 ","|"," 9 "]
         listB =[" 4 ","|"," 5 ","|"," 6 "]
         listC =[" 1 ","|"," 2 ","|"," 3 "]
        
        player_index = int(input("1 to 9:\n"))
        
        
        
    while player_index not in (range(1,10)):
        print("Unvalid input please retry: \n")
    else:
        
        row1 = ""
        row2 = ""
        row3 = ""
   
    for i in range(len(listA)):
            row1 +=listA[i]
            row2 +=listB[i]
            row3 +=listC[i]
43/32:
def player_input():
    valid_list = ['X','O']
    player_choice = input("make a choice between X or O")
   
    while player_choice not in valid_list:
        player_choice = input("make a choice between X or O")
    else:
    
        print("Choose the location of {}".format(player_choice))
        print("Like:")
        
        listA =[" 7 ","|"," 8 ","|"," 9 "]
        listB =[" 4 ","|"," 5 ","|"," 6 "]
        listC =[" 1 ","|"," 2 ","|"," 3 "]
        
        player_index = int(input("1 to 9:\n"))
        
        
        
    while player_index not in (range(1,10)):
        print("Unvalid input please retry: \n")
    else:
        
        row1 = ""
        row2 = ""
        row3 = ""
   
    for i in range(len(listA)):
            row1 +=listA[i]
            row2 +=listB[i]
            row3 +=listC[i]
43/33: player_input()
43/34:
from IPython.display import clear_output
clear_output()

def display_board(board):
    print(listA)
    print(listB)
    print(listC)
43/35:
test_board = ['X','O','X','O','X','O','X','O','X']
display_board(test_board)
49/1: type(my_sample)
49/2: type(myset)
49/3:
mylist = [1,2,3]
myset = set()
49/4: type(myset)
49/5:
class Dog():
    
    def __init__(self,mybreed):
        
        self.breed = mybreed
49/6: my_sample = Sample()
49/7: my_dog = Dog(breed = "Lab")
49/8: my_dog = Dog(mybreed = "Lab")
49/9: my_sample = Sample()
49/10: my_dog = Dog(mybreed = "Lab")
49/11: my_dog = Dog(mybreed = "Lab")
49/12: type(my_dog.breed)
49/13: type(my_dog.mybreed)
49/14: type(my_dog.breed)
49/15: my_dog.breed
49/16: my_dog = Dog(mybreed = 1)
49/17:
class Dog():
    
    def __init__(self,mybreed:str):
        
        asset type(mybreed) == str , "has to be string"
        self.breed = mybreed
49/18:
class Dog():
    
    def __init__(self,mybreed:str):
        
        assert type(mybreed) == str , "has to be string"
        self.breed = mybreed
49/19: my_dog = Dog(mybreed = 1)
49/20: my_dog = Dog(mybreed = "alsd")
49/21: my_dog.breed
49/22:
class Dog():
    
    def __init__(self,mybreed:str,name, spots):
        
        assert type(mybreed) == str , "has to be string"
        
        self.breed = mybreed
        self.name = name
        
        #Expect boolean True/False
        self.spots = spots
49/23: my_dog = Dog(breed = 'lab', name = 'Sammy',spots = True
49/24: my_dog = Dog(breed = 'lab', name = 'Sammy',spots = True)
49/25: my_dog = Dog(mybreed = 'lab', name = 'Sammy',spots = True)
49/26: my_dog.name
49/27: my_dog.spots
49/28: my_dog = Dog(mybreed = 'lab', name = 'Sammy',spots = "NO SPOTS")
49/29: my_dog.spots
49/30: type(my_dog.breed)
49/31: my_dog.breed
49/32:
class Dog

    # CLASS OBJECT ATTRIBUTE
    # SAME FOR ANY INSTANCE OF A CLASS
    species = "mammal"
    
    def __init__(self,mybreed:str,name):
        
        assert type(mybreed) == str , "has to be string"
        
        self.breed = mybreed
        self.name = name
        
        
        
    # Operations/Actions ---> Methods
    
    def bark(self):
        print("WOOF!")
49/33:
class Dog:

    # CLASS OBJECT ATTRIBUTE
    # SAME FOR ANY INSTANCE OF A CLASS
    species = "mammal"
    
    def __init__(self,mybreed:str,name):
        
        assert type(mybreed) == str , "has to be string"
        
        self.breed = mybreed
        self.name = name
        
        
        
    # Operations/Actions ---> Methods
    
    def bark(self):
        print("WOOF!")
49/34: my_dog = Dog("Lab","Frankie")
49/35: type(my_dog.breed)
49/36: my_dog.breed
49/37: my_dog.name
49/38: my_dog.bark
49/39: my_dog.bark()
49/40:
class Dog:

    # CLASS OBJECT ATTRIBUTE
    # SAME FOR ANY INSTANCE OF A CLASS
    species = "mammal"
    
    def __init__(self,mybreed:str,name):
        
        assert type(mybreed) == str , "has to be string"
        
        self.breed = mybreed
        self.name = name
        
        
        
    # Operations/Actions ---> Methods
    
    def bark(self):
        print(f"WOOF! My name is {self.name}.")
49/41: my_dog = Dog("Lab","Frankie")
49/42: my_dog.name
49/43: my_dog.bark()
49/44:
class Dog:

    # CLASS OBJECT ATTRIBUTE
    # SAME FOR ANY INSTANCE OF A CLASS
    species = "mammal"
    
    def __init__(self,mybreed:str,name):
        
        assert type(mybreed) == str , "has to be string"
        
        self.breed = mybreed
        self.name = name
        
        
        
    # Operations/Actions ---> Methods
    
    def bark(self,number):
        print(f"WOOF! My name is {self.name}. and the number is {number}")
49/45: my_dog = Dog("Lab","Frankie")
49/46: my_dog.name
49/47: my_dog.bark()
49/48: my_dog.bark(2)
49/49:
class Circle():
    # CLASS OBJECT ATTRIBUTE
    pi = 3.14
    
    def __init__(self,radius = 1):
        self.radius = radius
    
    def get_circumference(self):
        return self.radius * self.pi * 2
49/50: my_circle = Circle()
49/51: my_circle.get_circumference()
49/52: my_circle = Circle(30)
49/53: my_circle.get_circumference()
49/54:
class Circle():
    # CLASS OBJECT ATTRIBUTE
    pi = 3.14
    
    def __init__(self,radius = 1):
        self.radius = radius
        self.area = radius * radius * self.pi
    
    def get_circumference(self):
        return self.radius * self.pi * 2
49/55: my_circle = Circle(30)
49/56: my_circle.get_circumference()
49/57: my_circle.area
50/1:
class Animal():
    
    def __init__(self):
        print("Animal Created")
    
    def who_am_i():
        print("I am an animal")
        
    def eat(self):
50/2:
class Animal():
    
    def __init__(self):
        print("Animal Created")
    
    def who_am_i():
        print("I am an animal")
        
    def eat(self):
        print("I am eating.")
50/3: myanimal = Animal()
50/4: myanimal.who_am_i
50/5: myanimal.who_am_i()
50/6: myanimal.who_am_i()
50/7:
class Animal():
    
    def __init__(self):
        print("Animal Created")
    
    def who_am_i(self):
        print("I am an animal")
        
    def eat(self):
        print("I am eating.")
50/8: myanimal = Animal()
50/9: myanimal.who_am_i()
50/10:
class Dog(Animal):
    
    def __init__(self):
        Animal.__init__(self)
        print("Dog created")
50/11: mydog = Dog()
50/12: mydog.who_am_i()
50/13:
def who_am_i(self):
    print("I am a dog!")
50/14: mydog.who_am_i()
50/15:
class Animal():
    
    def __init__(self):
        print("Animal Created")
    
    def who_am_i(self):
        print("I am an animal")
        
    def eat(self):
        print("I am eating.")
50/16: myanimal = Animal()
50/17: myanimal.who_am_i()
50/18:
class Dog(Animal):
    
    def __init__(self):
        Animal.__init__(self)
        print("Dog created")
50/19: mydog = Dog()
50/20:
def who_am_i(self):
    print("I am a dog!")
50/21: mydog.who_am_i()
50/22: mydog.who_am_i()
50/23: mydog = Dog()
50/24:
def who_am_i(self):
    print("I am a dog!")
50/25: mydog.who_am_i()
50/26:
class Animal():
    
    def __init__(self):
        print("Animal Created")
    
    def who_am_i(self):
        print("I am an animal")
        
    def eat(self):
        print("I am eating.")
50/27: myanimal = Animal()
50/28: myanimal.who_am_i()
50/29:
class Dog(Animal):
    
    def __init__(self):
        Animal.__init__(self)
        print("Dog created")
50/30: mydog = Dog()
50/31:
def who_am_i(self):
    print("I am a dog!")
50/32: mydog.who_am_i()
50/33:
class Dog(Animal):
    
    def __init__(self):
        Animal.__init__(self)
        print("Dog created")
    def who_am_i(self):
    print("I am a dog!")
50/34:
class Dog(Animal):
    
    def __init__(self):
        Animal.__init__(self)
        print("Dog created")
    
    def who_am_i(self):
        print("I am a dog!")
50/35: mydog = Dog()
50/36: mydog.who_am_i()
50/37:
class Dog(Animal):
    
    def __init__(self):
        Animal.__init__(self)
        print("Dog created")
50/38: mydog = Dog()
50/39: mydog.who_am_i()
50/40:
class Dog(Animal):
    
    def __init__(self):
        Animal.__init__(self)
        print("Dog created")
    
    def who_am_i(self):
        print("I am a dog!")
50/41: mydog = Dog()
50/42: mydog.who_am_i()
50/43:
class Dog(Animal):
    
    def __init__(self):
        Animal.__init__(self)
        print("Dog created")
    
    def who_am_i(self):
        print("I am a dog!")
        
    def bark(self):
        print("WOOF")
    
     def eat(self):
        print("I am eating.")
50/45:
class Dog(Animal):
    
    def __init__(self):
        Animal.__init__(self)
        print("Dog created")
    
    def who_am_i(self):
        print("I am a dog!")
        
    def bark(self):
        print("WOOF")
    
    def eat(self):
        print("I am eating.")
50/46:
class Dog(Animal):
    
    def __init__(self):
        Animal.__init__(self)
        print("Dog created")
    
    def who_am_i(self):
        print("I am a dog!")
        
    def bark(self):
        print("WOOF")
    
    def eat(self):
        print("I am a dog and eating.")
50/47: mydog = Dog()
50/48: mydog.eat()
50/49: mydog.who_am_i()
50/50:
class Cat():
    
    def __init__(self,name):
        self.name = name
    
    def speak(self):
        return self.name + "says meow!"
50/51:
class Dog():
    
    def __init__(self,name):
        self.name = name
        
    def who_am_i(self):
        print("I am a dog!")
    
     def speak(self):
        return self.name + "says woof!"
50/53:
class Dog():
    
    def __init__(self,name):
        self.name = name
        
    def who_am_i(self):
        print("I am a dog!")
    
    def speak(self):
        return self.name + "says woof!"
50/54:
class Cat():
    
    def __init__(self,name):
        self.name = name
    
    def speak(self):
        return self.name + "says meow!"
50/55:
niko = Dog("niko")
felix = Cat("felix")
50/56:
print(niko.speak())
print(felix.speak)
50/57:
print(niko.speak())
print(felix.speak())
50/58:
class Dog():
    
    def __init__(self,name):
        self.name = name
        
    def who_am_i(self):
        print("I am a dog!")
    
    def speak(self):
        return self.name + " says woof!"
50/59:
class Cat():
    
    def __init__(self,name):
        self.name = name
    
    def speak(self):
        return self.name + " says meow!"
50/60:
niko = Dog("niko")
felix = Cat("felix")
50/61:
print(niko.speak())
print(felix.speak())
50/62:
for pet in [niko,felix]:
    print(type(pet))
    print(type(pet.speak()))
50/63:
def pet_speak(pet):
    print(pet.speak())
50/64: pet_speak(niko)
50/65: pet_speak(felix)
50/66:
class Animal():
    def __init__(self,name):
        self.name = name
    
    # Abstract Class
    def speak(self):
        raise NotImplementedError("Subclass must implement this abstract method")
50/67: myanimal = Animal("YC")
50/68: myanimal.speak()
50/69:
class Cat(Animal):
    
    def __init__(self,name):
        self.name = name
    
    def speak(self):
        return self.name + " says meow!"
50/70:
class Cat(Animal):
    
    def __init__(self,name):
        Animal.__init__(self,name)
    
    def speak(self):
        return self.name + " says meow!"
50/71: mycat =
50/72: mycat = Cat("ML")
50/73: mycat.speak()
51/1: mylist = [1,2,3]
51/2: len(mylist)
51/3:
class Sample():
    pass
51/4: mysample = Sample()
51/5: print(mysample)
51/6:
class Book():
    def __init__(self,title,author,pages):
        self.title = title
        self.author = author
        self.pages = pages
51/7: b =  Book('Python Rocks','Jose', 200)
51/8: print(b)
51/9:
class Book():
    def __init__(self,title,author,pages):
        self.title = title
        self.author = author
        self.pages = pages
        
    def __str__(self):
        return f"{self.title} by {self.author}"
51/10: b =  Book('Python Rocks','Jose', 200)
51/11: print(b)
51/12:
class Book():
    def __init__(self,title,author,pages):
        self.title = title
        self.author = author
        self.pages = pages
        
    def __repr__(self):
        return f"{self.title} by {self.author}"
51/13: b =  Book('Python Rocks','Jose', 200)
51/14: print(b)
51/15:
class Book():
    def __init__(self,title,author,pages):
        self.title = title
        self.author = author
        self.pages = pages
        
    def __str__(self):
        return f"{self.title} by {self.author}"
51/16: b =  Book('Python Rocks','Jose', 200)
51/17: print(b)
51/18: len(b)
51/19:
class Book():
    def __init__(self,title,author,pages):
        self.title = title
        self.author = author
        self.pages = pages
        
    def __str__(self):
        return f"{self.title} by {self.author}"
    
    def __len__(self):
        return self.pages
51/20: b =  Book('Python Rocks','Jose', 200)
51/21: print(b)
51/22: len(b)
51/23: del b
51/24: b
51/25:
class Book():
    def __init__(self,title,author,pages):
        self.title = title
        self.author = author
        self.pages = pages
        
    def __str__(self):
        return f"{self.title} by {self.author}"
    
    def __len__(self):
        return self.pages
    
    def __del__(self):
        print("A book object has been deleted")
51/26: b =  Book('Python Rocks','Jose', 200)
51/27: print(b)
51/28: len(b)
51/29: del b
51/30: b
52/1:
# EXAMPLE OUTPUT

coordinate1 = (3,2)
coordinate2 = (8,10)

li = Line(coordinate1,coordinate2)
52/2: coordinate1[1]
52/3: coordinate1[0]
52/4:
class Line:
    
    def __init__(self,coor1:tuple,coor2: tuple):
        self.coor1 = coor1
        self.coor2 = coor2
    
    def distance(self):
        return  abs((self.coor1[0] - self.coor2[0])**2 + (self.coor1[1] - self.coor2[1])**2)**(1/2)
        
    
    def slope(self):
        return (self.coor1[1] - self.coor2[1]) / (self.coor1[0] - self.coor2[0])
52/5:
# EXAMPLE OUTPUT

coordinate1 = (3,2)
coordinate2 = (8,10)

li = Line(coordinate1,coordinate2)
52/6: coordinate1[0]
52/7: li.distance()
52/8: li.slope()
52/9:
class Cylinder:
    
    pi = 3.14
    
    def __init__(self,height=1,radius=1):
        self.height = height
        self.radius = radius
        
    def volume(self):
        return Cylinder.pi * self.radius * self.radius * self.height
    
    def surface_area(self):
        return 2*Cylinder.pi* self.radius * self.radius * self.height + 2 * pi * self.radius * self.height
52/10:
# EXAMPLE OUTPUT
c = Cylinder(2,3)
52/11: c.volume()
52/12: c.surface_area()
52/13:
class Cylinder:
    
    pi = 3.14
    
    def __init__(self,height=1,radius=1):
        self.height = height
        self.radius = radius
        
    def volume(self):
        return Cylinder.pi * self.radius * self.radius * self.height
    
    def surface_area(self):
        return 2 *Cylinder.pi * self.radius * self.radius * self.height + 2 * pi * self.radius * self.height
52/14:
# EXAMPLE OUTPUT
c = Cylinder(2,3)
52/15: c.volume()
52/16: c.surface_area()
52/17:
class Cylinder:
    
    pi = 3.14
    
    def __init__(self,height=1,radius=1):
        self.height = height
        self.radius = radius
        
    def volume(self):
        return Cylinder.pi * self.radius * self.radius * self.height
    
    def surface_area(self):
        return 2 *Cylinder.pi * self.radius * self.radius * self.height + 2 * Cylinder.pi * self.radius * self.height
52/18:
# EXAMPLE OUTPUT
c = Cylinder(2,3)
52/19: c.volume()
52/20: c.surface_area()
52/21:
class Cylinder:
    
    pi = 3.14
    
    def __init__(self,height=1,radius=1):
        self.height = height
        self.radius = radius
        
    def volume(self):
        return Cylinder.pi * self.radius * self.radius * self.height
    
    def surface_area(self):
        return 2 * Cylinder.pi * self.radius * self.radius + (2 * Cylinder.pi * self.radius * self.height)
52/22:
# EXAMPLE OUTPUT
c = Cylinder(2,3)
52/23: c.volume()
52/24: c.surface_area()
53/1:
class Account:
    def __init__(self,owner,balance):
        self.owner = owner
        self.balance = balance
53/2:
# 1. Instantiate the class
acct1 = Account('Jose',100)
53/3:
# 2. Print the object
print(acct1)
53/4:
class Account:
    def __init__(self,owner,balance):
        self.owner = owner
        self.balance = balance
        
    def __repr__(self):
        class_name = type(self).__name__
        return f"Class is : {class_name} owner name is{self.owner}.\nBalance is {self.balance}"
53/5:
# 1. Instantiate the class
acct1 = Account('Jose',100)
53/6:
# 2. Print the object
print(acct1)
53/7:
class Account:
    def __init__(self,owner,balance):
        self.owner = owner
        self.balance = balance
        
    def __repr__(self):
        class_name = type(self).__name__
        return f"Class is : {class_name}\nOwner name is{self.owner}.\nBalance is {self.balance}"
53/8:
# 1. Instantiate the class
acct1 = Account('Jose',100)
53/9:
# 2. Print the object
print(acct1)
53/10:
class Account:
    def __init__(self,owner,balance):
        self.owner = owner
        self.balance = balance
        
    def __repr__(self):
        class_name = type(self).__name__
        return f"Class is : {class_name}\nOwner name is {self.owner}.\nBalance is {self.balance}"
53/11:
# 1. Instantiate the class
acct1 = Account('Jose',100)
53/12:
# 2. Print the object
print(acct1)
53/13:
# 3. Show the account owner attribute
acct1.owner
53/14:
# 4. Show the account balance attribute
acct1.balance
53/15:
class Account:
    def __init__(self,owner,balance):
        self.owner = owner
        self.balance = balance
        
    def __repr__(self):
        class_name = type(self).__name__
        return f"Class is : {class_name}\nOwner name is {self.owner}.\nBalance is {self.balance}"
    
    def deposit(self,value):
            print("Your balance is {}".format(self.balance))
            balance += value
            print("Your new balance after deposit is {}".format(self.balance))
53/16:
# 1. Instantiate the class
acct1 = Account('Jose',100)
53/17:
# 2. Print the object
print(acct1)
53/18:
# 3. Show the account owner attribute
acct1.owner
53/19:
# 4. Show the account balance attribute
acct1.balance
53/20:
# 3. Show the account owner attribute
acct1.owner
53/21:
# 4. Show the account balance attribute
acct1.balance
53/22:
# 5. Make a series of deposits and withdrawals
acct1.deposit(50)
53/23:
class Account:
    def __init__(self,owner,balance):
        self.owner = owner
        self.balance = balance
        
    def __repr__(self):
        class_name = type(self).__name__
        return f"Class is : {class_name}\nOwner name is {self.owner}.\nBalance is {self.balance}"
    
    def deposit(self,value):
            print("Your balance is {}".format(self.balance))
            self.balance += value
            print("Your new balance after deposit is {}".format(self.balance))
53/24:
# 1. Instantiate the class
acct1 = Account('Jose',100)
53/25:
# 2. Print the object
print(acct1)
53/26:
# 3. Show the account owner attribute
acct1.owner
53/27:
# 4. Show the account balance attribute
acct1.balance
53/28:
# 5. Make a series of deposits and withdrawals
acct1.deposit(50)
53/29:
class Account:
    def __init__(self,owner,balance):
        self.owner = owner
        self.balance = balance
        
    def __repr__(self):
        class_name = type(self).__name__
        return f"Class is : {class_name}\nOwner name is {self.owner}.\nBalance is {self.balance}"
    
    def deposit(self,value):
            print("Your balance is {}".format(self.balance))
            self.balance += value
            print("Your new balance after deposit is {}".format(self.balance))
            
    def withdraw(self,value):
        while (self.balance >= value):
            self.balance -= value
        else:
            raise Exception("Your withdraw must not higherthan your balance.")
53/30:
# 1. Instantiate the class
acct1 = Account('Jose',100)
53/31:
# 2. Print the object
print(acct1)
53/32:
# 3. Show the account owner attribute
acct1.owner
53/33:
# 4. Show the account balance attribute
acct1.balance
53/34:
class Account:
    def __init__(self,owner,balance):
        self.owner = owner
        self.balance = balance
        
    def __repr__(self):
        class_name = type(self).__name__
        return f"Class is : {class_name}\nOwner name is {self.owner}.\nBalance is {self.balance}"
    
    def deposit(self,value):
            print("Your balance is {}".format(self.balance))
            self.balance += value
            print("Your new balance after deposit is {}".format(self.balance))
            
    def withdraw(self,value):
        while (self.balance >= value):
            self.balance -= value
            print("Your new balance is")
        else:
            raise Exception("Your withdraw must not higherthan your balance.")
53/35:
# 1. Instantiate the class
acct1 = Account('Jose',100)
53/36:
# 2. Print the object
print(acct1)
53/37:
# 3. Show the account owner attribute
acct1.owner
53/38:
# 4. Show the account balance attribute
acct1.balance
53/39:
# 5. Make a series of deposits and withdrawals
acct1.deposit(50)
53/40: acct1.withdraw(75)
53/41:
class Account:
    def __init__(self,owner,balance):
        self.owner = owner
        self.balance = balance
        
    def __repr__(self):
        class_name = type(self).__name__
        return f"Class is : {class_name}\nOwner name is {self.owner}.\nBalance is {self.balance}"
    
    def deposit(self,value):
            print("Your balance is {}".format(self.balance))
            self.balance += value
            print("Your new balance after deposit is {}".format(self.balance))
            
    def withdraw(self,value):
        while (self.balance >= value):
            self.balance -= value
            print(f"Your new balance is {self.balance}")
        else:
            raise Exception("Your withdraw must not higherthan your balance.")
53/42:
# 1. Instantiate the class
acct1 = Account('Jose',100)
53/43:
# 2. Print the object
print(acct1)
53/44:
# 3. Show the account owner attribute
acct1.owner
53/45:
# 4. Show the account balance attribute
acct1.balance
53/46:
# 5. Make a series of deposits and withdrawals
acct1.deposit(50)
53/47: acct1.withdraw(75)
53/48:
class Account:
    def __init__(self,owner,balance):
        self.owner = owner
        self.balance = balance
        
    def __repr__(self):
        class_name = type(self).__name__
        return f"Class is : {class_name}\nOwner name is {self.owner}.\nBalance is {self.balance}"
    
    def deposit(self,value):
            print("Your balance is {}".format(self.balance))
            self.balance += value
            print("Your new balance after deposit is {}".format(self.balance))
            
    def withdraw(self,value):
        while (self.balance >= value):
            self.balance -= value
            print(f"Your new balance is {self.balance}")
            break
        else:
            raise Exception("Your withdraw must not higherthan your balance.")
53/49:
class Account:
    def __init__(self,owner,balance):
        self.owner = owner
        self.balance = balance
        
    def __repr__(self):
        class_name = type(self).__name__
        return f"Class is : {class_name}\nOwner name is {self.owner}.\nBalance is {self.balance}"
    
    def deposit(self,value):
            print("Your balance is {}".format(self.balance))
            self.balance += value
            print("Your new balance after deposit is {}".format(self.balance))
            
    def withdraw(self,value):
        while (self.balance >= value):
            self.balance -= value
            print(f"Your new balance is {self.balance}")
            break
        else:
            raise Exception("Your withdraw must not higherthan your balance.")
53/50:
# 1. Instantiate the class
acct1 = Account('Jose',100)
53/51:
# 2. Print the object
print(acct1)
53/52:
# 3. Show the account owner attribute
acct1.owner
53/53:
# 4. Show the account balance attribute
acct1.balance
53/54:
# 5. Make a series of deposits and withdrawals
acct1.deposit(50)
53/55: acct1.withdraw(75)
53/56:
# 6. Make a withdrawal that exceeds the available balance
acct1.withdraw(500)
53/57:
class Account:
    def __init__(self,owner,balance):
        self.owner = owner
        self.balance = balance
        
    def __repr__(self):
        class_name = type(self).__name__
        return f"Class is : {class_name}\nOwner name is {self.owner}.\nBalance is {self.balance}"
    
    def deposit(self,value):
            print("Your balance is {}".format(self.balance))
            self.balance += value
            print("Your new balance after deposit is {}".format(self.balance))
            
    def withdraw(self,value):
        while (self.balance >= value):
            self.balance -= value
            print(f"Your new balance is {self.balance}")
            break
        else:
            print(f"Your balance is {self.balance}")
            raise Exception("Your withdraw must not higherthan your balance.")
53/58:
# 1. Instantiate the class
acct1 = Account('Jose',100)
53/59:
# 2. Print the object
print(acct1)
53/60:
# 3. Show the account owner attribute
acct1.owner
53/61:
# 4. Show the account balance attribute
acct1.balance
53/62:
# 5. Make a series of deposits and withdrawals
acct1.deposit(50)
53/63: acct1.withdraw(75)
53/64:
# 6. Make a withdrawal that exceeds the available balance
acct1.withdraw(500)
53/65:
class Account:
    def __init__(self,owner,balance):
        self.owner = owner
        self.balance = balance
        
    def __repr__(self):
        class_name = type(self).__name__
        return f"Class is : {class_name}\nOwner name is {self.owner}.\nBalance is {self.balance}"
    
    def deposit(self,value):
            print("Your balance is {}".format(self.balance))
            self.balance += value
            print("Your new balance after deposit is {}".format(self.balance))
            
    def withdraw(self,value):
        while (self.balance >= value):
            self.balance -= value
            print(f"Your new balance is {self.balance}")
            break
        else:
            print(f"Your balance is {self.balance}")
            raise Exception("Your withdraw must not higher than your balance.")
57/1:
def add(n1,n2):
    print(n1 + n2)
57/2: add(10,20)
57/3: number1 = 10
57/4: number2 = input("Please provide a number: ")
57/5: add(number1,number2)
57/6: number2 = int(input("Please provide a number: "))
57/7: add(number1,number2)
57/8:
try:
    #Want to attempt this code
    #May have an error
    result = 10 + 10
except:
    print("Hey it looks like you are not adding correctly")
57/9:
try:
    #Want to attempt this code
    #May have an error
    result = 10 + 10
except:
    print("Hey it looks like you are not adding correctly")
else:
    print("Add went well")
    print(result)
57/10:
try:
    f = open('testfile','w')
    f.write("Write a test line")
except TypeError:
    print("There was a type error")
except OSError:
    print("Hey you have an OS Error")
finally:
    print("I always run")
57/11:
try:
    f = open('testfile','w')
    f.write("Write a test line")
except TypeError:
    print("There was a type error")
except OSError:
    print("Hey you have an OS Error")
finally:
    print("I always run")
57/12:
try:
    f = open('testfile','r')
    f.write("Write a test line")
except TypeError:
    print("There was a type error")
except OSError:
    print("Hey you have an OS Error")
finally:
    print("I always run")
57/13:
try:
    f = open('testfile','r')
    f.write("Write a test line")
except TypeError:
    print("There was a type error")
except:
    print("All other exceptions!")
finally:
    print("I always run")
57/14:
try:
    f = open('testfile','w')
    f.write("Write a test line")
except TypeError:
    print("There was a type error")
except:
    print("All other exceptions!")
finally:
    print("I always run")
57/15:
def ask_for_int():
    try:
        result = int(input("Please provide a number: "))
    except:
        print("Whoops! That is not a number")
    finally:
        print("End of try/except/finally")
57/16: ask_for_int()
57/17: ask_for_int()
57/18:
def ask_for_int():
 
    while True: 
        try:
            result = int(input("Please provide a number: "))
        except:
            print("Whoops! That is not a number")
            continue
        else:
            print("Yes thank you")
            break
        finally:
            print("End of try/except/finally")
57/19: ask_for_int()
58/1:
for i in ['a','b','c']:
    print(i**2)
58/2:
try:
    for i in ['a','b','c']:
        print(i**2)
except TypeError:
    print("You can not do that")
58/3:
try:
    for i in ['a','b','c']:
        print(i**2)
except TypeError:
    print("You can not do that")
else:
    print("Power went well")
finally:
    print("End of the script")
58/4:
try:
    for i in [1,2,3]:
        print(i**2)
except TypeError:
    print("You can not do that")
else:
    print("Power went well")
finally:
    print("End of the script")
58/5:
try:
    x = 5
    y = 0
    z = x/y
except ZeroDivisionError:
    print("You can not divide any number to zero")
finally:
    print("All Done.")
58/6:
def ask():
     while True:
        try:
            number = int(input("Input an integer"))
        except:
            print("An error occured! Please try again!")
        else:
            print(f"Thank you, your number squared is {number**2})
            break
        finally:
            pass
58/7:
def ask():
     while True:
        try:
            number = int(input("Input an integer"))
        except:
            print("An error occured! Please try again!")
        else:
            print(f"Thank you, your number squared is {number**2}")
            break
        finally:
            pass
58/8:
def ask():
     while True:
        try:
            number = int(input("Input an integer"))
        except:
            print("An error occured! Please try again!")
        else:
            print(f"Thank you, your number squared is {number**2}")
            break
        finally:
            pass
58/9: ask()
58/10:
def ask():
     while True:
        try:
            number = int(input("Input an integer: "))
        except:
            print("An error occured! Please try again!")
        else:
            print(f"Thank you, your number squared is:  {number**2}")
            break
        finally:
            pass
58/11: ask()
64/1:
def func():
    return 1
64/2: func()
64/3: dunc
64/4: func
64/5:
def hello():
    return "Hello!"
64/6: hello()
64/7: hello
64/8: greet = hello
64/9: greet()
64/10: del hello
64/11: hello()
64/12: greet()
64/13:
def hello(name='Jose'):
    print(f"Hello{name}")
64/14: hello()
64/15:
def hello(name='Jose'):
    print("The hello() function has been executed!")
    
    def greet():
        return '\t This is the greet function inside hello!'
64/16: hello()
64/17:
def hello(name='Jose'):
    print("The hello() function has been executed!")
    
    def greet():
        return '\t This is the greet function inside hello!'
    
    print(greet())
64/18: hello()
64/19:
def hello(name='Jose'):
    print("The hello() function has been executed!")
    
    def greet():
        return '\t This is the greet function inside hello!'
    
    def welcome():
    
    print(greet())
64/20:
def hello(name='Jose'):
    print("The hello() function has been executed!")
    
    def greet():
        return '\t This is the greet function inside hello!'
    
    def welcome():
        return '\t This is welcome() inside hello'
        
    
    print(greet())
    print(welcome())
64/21: hello()
64/22: welcome()
65/1:
#Python program to illustrate functions
# can be treated ass objects

def shout(text):
    return text.upper()

print(shout('Hello'))

yell = shout

print(yell('Hello'))
65/2:
#Python program to illustrate functions
# can be treated ass objects

def shout(text):
    return text.upper()

print(shout('Hello'))

yell = shout

print(yell('Hello'))
65/3: # Python program to illustrate functions
65/4:
# Python program to illustrate functions
# can be passed as arguments to other functions

def shout(text):
    return text.upper()

def whisper(text):
    return text.lower()

def greet(func):
    #storing the function in a variable
    greeting = func("""Hi, I am created by a function
    passed as an argument.""")
    print(greeting)
65/5:
greet(shout)
greet(whisper)
65/6:
# Python program to illustrate functions
# can be passed as arguments to other functions

def shout(text):
    return text.upper()

def whisper(text):
    return text.lower()

def greet(func):
    #storing the function in a variable
    greeting = func("""Hi, I am created by a function passed as an argument.""")
    print(greeting)
65/7:
greet(shout)
greet(whisper)
65/8:
# Python program to illustrate functions
# Functions can return another function

def create_adder(x):
    def adder(y):
        return x+y
    
    return adder

add_15 = create_adder(15)

print(add_15(10))
65/9:
#Python program to illustrate functions
# can be treated ass objects

def shout(text):
    return text.upper()

print(shout('Hello'))

yell = shout("Hello")

print(yell)
65/10:
#Python program to illustrate functions
# can be treated ass objects

def shout(text):
    return text.upper()

print(shout('Hello'))

yell = shout("Hello")

print(yell('HelloSDAADS'))
66/1:
s = 'Global Variable'

def check_for_locals():
    print(locals())
66/2: print(globals())
66/3: print(globals().keys())
66/4: globals()['s']
65/11:
def new_decorator(original_func):
    def wrap_func():
        print("Some extra code, before the originial function")
        original_func()
        print("Some extra code, after the original function!")
65/12:
def func_needs_decorator():
    print("I want to be decorated!!')
65/13:
def func_needs_decorator():
    print('I want to be decorated!!')
65/14: func_needs_decorator
65/15:
def new_decorator(original_func):
    def wrap_func():
        print("Some extra code, before the originial function")
        original_func()
        print("Some extra code, after the original function!")
    print(wrap_func)
65/16:
def func_needs_decorator():
    print('I want to be decorated!!')
65/17: new_decorator(hello)
65/18: hello()
65/19: new_decorator()
65/20:
def a():
    return "a"
65/21: new_decorator(a)
65/22: new_decorator(func_needs_decorator)
65/23: decorated = new_decorator(func_needs_decorator)
65/24: decorated
65/25: print(decorated)
65/26:
def func_needs_decorator():
    print('I want to be decorated!!')
65/27: func_needs_decorator
65/28: decorated = new_decorator(func_needs_decorator)
65/29: decorated
65/30: decorated()
65/31:
def new_decorator(original_func):
    def wrap_func():
        print("Some extra code, before the originial function")
        original_func()
        print("Some extra code, after the original function!")
    print(wrap_func)
65/32:
def func_needs_decorator():
    print('I want to be decorated!!')
65/33: func_needs_decorator
65/34: decorated = new_decorator(func_needs_decorator)
65/35: decorated()
65/36: func_needs_decorator()
65/37:
def new_decorator(original_func):
    def wrap_func():
        print("Some extra code, before the originial function")
        original_func()
        print("Some extra code, after the original function!")
    
    return wrap_func
65/38:
def func_needs_decorator():
    print('I want to be decorated!!')
65/39: func_needs_decorator()
65/40: decorated = new_decorator(func_needs_decorator)
65/41: decoer
65/42: decorated
65/43: decorated()
65/44: @new_decorator
65/45: @new_decorator
65/46:
@new_decorator
def func_needs_decorator():
    print('I want to be decorated!!')
65/47: func_needs_decorator()
65/48: func_needs_decorator()
65/49: decorated = new_decorator(func_needs_decorator)
65/50: decorated()
65/51:
def func_needs_decorator():
    print('I want to be decorated!!')
65/52: func_needs_decorator()
65/53:
@new_decorator
def func_needs_decorator():
    print('I want to be decorated!!')
65/54: func_needs_decorator()
68/1: print(range(10))
68/2: print(list(range(10)))
68/3:
def create_cubes(n):
    result = []
    for x in range(n):
        result.append(x**3)
    return result
68/4: create_cubes(10)
68/5:
def create_cubes2(n):
    for x in range(n):
68/6:
def create_cubes2(n):
    for x in range(n):
        yield x**3
68/7: create_cubes2(10)
68/8: print(create_cubes2)
68/9: list(create_cubes2)
68/10: list(create_cubes2(10))
68/11:
def fibonacci(n):
    start = 0
    for x in range(n):
        yield start + x
68/12: fibonacci(4)
68/13: list(fibonacci(4))
68/14:
def fibonacci(n):
    start = 0
    for x in range(n):
        temp = 0
        yield start +=x
68/15:
def fibonacci(n):
    start = 0
    for x in range(n):
        temp = x
        yield temp + x
68/16: fibonacci(4)
68/17: list(fibonacci(4))
68/18:
def fibonacci(n):
    a = 1
    b = 1
    for x in range(n):
        temp = x
        yield a + b + temp
68/19: fibonacci(4)
68/20: list(fibonacci(4))
68/21:
def fun_generator():
    yield "Hello world!!"
    yield "Geeksforgeeks"

    obj = fun_generator()
    
    print(type(obj))
68/22:
def fun_generator():
    yield "Hello world!!"
    yield "Geeksforgeeks"

obj = fun_generator()
68/23: print(type(obj))
68/24: print(next(obj))
68/25: print(next(obj))
68/26:
def fun_generator():
    yield "Hello world!!"
    yield "Geeksforgeeks"

obj = fun_generator
68/27: print(type(obj))
68/28: print(next(obj))
68/29:
def fun_generator():
    yield "Hello world!!"
    yield "Geeksforgeeks"

obj = fun_generator()
68/30: print(type(obj))
68/31: print(next(obj))
68/32: print(next(obj))
68/33: def inf_sequence():
68/34:
def inf_sequence():
    num = 0
    
    while True:
        yield num
        num +=1
68/35:
for i in inf_sequence():
    print(i, end = "10")
68/36:
def fibonacci(n):
    a = 1
    b = 1
    for x in range(n):
        yield a,b
        a = a + b
        b = b + a
68/37: list(fibonacci(10))
68/38:
def fibonacci(n):
    a = 1
    b = 1
    for x in range(n):
        yield a
        a = a + b
        b = b + a
68/39: list(fibonacci(10))
68/40:
def fibonacci(n):
    a = 1
    b = 1
    for x in range(n):
        yield a
        a = a + b
        yield b
        b = b + a
68/41: list(fibonacci(10))
68/42:
def fibonacci(n):
    a = 1
    b = 1
    for x in range(n):
        yield a , yield b
        a = a + b
        
        b = b + a
68/43: list(fibonacci(3))
68/44:
def fibonacci(n):
    a = 1
    b = 1
    
    if n % 2 == 0:
        n /=2
    else:
         n =(n - 1) / 2
    for x in range(n):
        yield a
        a = a + b
        yield b
        b = b + a
68/45: list(fibonacci(3))
68/46:
def fibonacci(n):
    a = 1
    b = 1
    
    if n % 2 == 0:
         n = n / 2
    else:
         n =(n - 1) / 2
    for x in range(n):
        yield a
        a = a + b
        yield b
        b = b + a
68/47: list(fibonacci(3))
68/48:
def fibonacci(n : int):
    a = 1
    b = 1
    
    if n % 2 == 0:
         n = n / 2
    else:
         n =(n - 1) / 2
    for x in range(n):
        yield a
        a = a + b
        yield b
        b = b + a
68/49: list(fibonacci(3))
68/50:
def fibonacci(n : int):
    a = 1
    b = 1
    temp = n
    
    if temp % 2 == 0:
        temp = temp / 2
    else:
         temp = (temp - 1) / 2
            
    for x in range(n):
        yield a
        a = a + b
        yield b
        b = b + a
68/51: list(fibonacci(3))
68/52:
def fibonacci(n : int):
    a = 1
    b = 1
    temp = n
    
    if temp % 2 == 0:
        temp = temp / 2
    else:
         temp = (temp - 1) / 2
            
    for x in range(temp):
        yield a
        a = a + b
        yield b
        b = b + a
68/53: list(fibonacci(3))
68/54:
def fibonacci(n : int):
    a = 1
    b = 1
    
    if n % 2 == 0:
         n = n / 2
    else:
         n = (n + 1) / 2
            
    for x in range(n):
        yield a
        a = a + b
        yield b
        b = b + a
68/55: list(fibonacci(3))
68/56:
def fibonacci(n : int):
    a = 1
    b = 1
    
    if n % 2 == 0:
         n = n / 2
    else:
         n = (n) / 2
            
    for x in range(n):
        yield a
        a = a + b
        yield b
        b = b + a
68/57: list(fibonacci(3))
68/58:
def fibonacci(n : int):
    a = 1
    b = 1
    
    if n % 2 == 0:
         n = n / 2
    else:
         n = (n - 1) / 2
            
    for x in range(int(n)):
        yield a
        a = a + b
        yield b
        b = b + a
68/59: list(fibonacci(3))
68/60:
def fibonacci(n : int):
    a = 1
    b = 1
    
    if n % 2 == 0:
         n = n / 2
    else:
         n = (n + 1) / 2
            
    for x in range(int(n)):
        yield a
        a = a + b
        yield b
        b = b + a
68/61: list(fibonacci(3))
68/62:
def fibonacci(n : int):
    a = 1
    b = 1
    
    if n % 2 == 0:
         n = n / 2
    else:
         n = (n) / 2
            
    for x in range(int(n)):
        yield a
        a = a + b
        yield b
        b = b + a
68/63: list(fibonacci(3))
68/64:
def fibonacci(n : int):
    a = 1
    b = 1
    
    if n % 2 == 0:
         n = n / 2
    else:
         n = (n / 2) + 1
            
    for x in range(int(n)):
        yield a
        a = a + b
        yield b
        b = b + a
68/65: list(fibonacci(3))
68/66:
def fibonacci(n : int):
    a = 1
    b = 1
    
            
    for x in range(int(n)):
    
        yield a
        a = b
        b = b + a
68/67: list(fibonacci(3))
68/68: list(fibonacci(20))
68/69: list(fibonacci(20))
68/70:
def fibonacci(n):
    a = 1
    b = 1
    
            
    for x in range(n):
    
        yield a
        a = b
        b = b + a
68/71: list(fibonacci(20))
68/72: list(fibonacci(5))
68/73:
def fibonacci(n : int):
    a = 1
    b = 1
    
            
    for x in range(int(n)):
       '''
        yield a
        a = a + b
        yield b
        b = b + a
        '''
        yield a
        a,b = a + b,a+b
68/74: list(fibonacci(5))
68/75:
def fibonacci(n : int):
    a = 1
    b = 1
    
            
    for x in range(int(n)):
        '''
        yield a
        a = a + b
        yield b
        b = b + a
        '''
        yield a
        a,b = a + b,a+b
68/76: list(fibonacci(5))
68/77: for number in fibonacci(10):
68/78:
for number in fibonacci(10):
    print(number)
68/79:
def fibonacci(n):
    a = 1
    b = 1
    
            
    for x in range(int(n)):
        '''
        yield a
        a = a + b
        yield b
        b = b + a
        '''
        yield a
        a,b = b,a+b
68/80:
for number in fibonacci(10):
    print(number)
68/81:
def fibonacci(n):
    a = 1
    b = 1
    
            
    for x in range(int(n)):
        '''
        yield a
        a = a + b
        yield b
        b = b + a
        '''
        yield a
        a = b
        b = a + b
68/82:
for number in fibonacci(10):
    print(number)
68/83:
def fibonacci(n):
    a = 1
    b = 1
    
            
    for x in range(int(n)):
        '''
        yield a
        a = a + b
        yield b
        b = b + a
        '''
        yield a
        b = a + b
        a = b
68/84:
for number in fibonacci(10):
    print(number)
68/85:
def fibonacci(n):
    a = 1
    b = 1
    
            
    for x in range(int(n)):
        '''
        yield a
        a = a + b
        yield b
        b = b + a
        '''
        yield a
        b = a + b
        a = b
68/86:
for number in fibonacci(10):
    print(number)
68/87:
def fibonacci(n):
    a = 1
    b = 1
    
            
    for x in range(int(n)):
        '''
        yield a
        a = a + b
        yield b
        b = b + a
        '''
        yield a
        a,b = b, a + b
68/88:
for number in fibonacci(10):
    print(number)
70/1:
a = 3
b = 5
a = b
b = a + b
print(a, b)  # Bu print ifadesi 5 8 Ã§Ä±ktÄ±sÄ±nÄ± verecektir.
70/2:
def simple_gen():
    for x in range(3):
        yield x
70/3:
for number in simple_gen():
    print(number)
70/4: g = simple_gen()
70/5: print(g)
70/6: g
70/7: print(next(g))
70/8: print(next(g))
70/9: print(next(g))
70/10: print(next(g))
70/11: s = 'hello'
70/12:
for letter in s:
    letter.s
70/13:
for letter in s:
    next(letter)
70/14:
for letter in s:
    print(letter)
70/15: s_iter = iter(s)
70/16: next(s_iter)
70/17: next(s_iter)
70/18: next(s_iter)
72/1:
def gensquares(N):
    for x in range(N):
        yield N**2
72/2:
for x in gensquares(10):
    print(x)
72/3:
def gensquares(N):
    for x in range(N):
        yield x**2
72/4:
for x in gensquares(10):
    print(x)
72/5:
import random

random.randint(1,10)
72/6:
def rand_num(low,high,n):
number1 = int(input("Enter a number: "))
number2 = int(input("\nEnter a number: "))

    if number1 < number2:
        for x in range(n):
            yield random.randint(number1,number2+1)
    
    elif number1 > number2:
        for x in range(n):
            yield random.randint(number2,number1+1)
    else:
        except:
            print("Numbers must not be equal")
72/7:
def rand_num(low,high,n):
    number1 = int(input("Enter a number: "))
    number2 = int(input("\nEnter a number: "))

    if number1 < number2:
        for x in range(n):
            yield random.randint(number1,number2+1)
    
    elif number1 > number2:
        for x in range(n):
            yield random.randint(number2,number1+1)
    else:
        except:
            print("Numbers must not be equal")
72/8:
def rand_num(low,high,n):
    number1 = int(input("Enter a number: "))
    number2 = int(input("\nEnter a number: "))

    if number1 < number2:
        for x in range(n):
            yield random.randint(number1,number2+1)
    
    elif number1 > number2:
        for x in range(n):
            yield random.randint(number2,number1+1)
    else:
        print("Numbers must not be equal")
72/9:
for num in rand_num(1,10,12):
    print(num)
72/10:
def rand_num(low,high,n):
    number1 = int(input("Enter a number: "))
    number2 = int(input("\nEnter a number: "))

    if number1 < number2:
        for x in range(n):
            yield random.randint(number1,number2)
    
    elif number1 > number2:
        for x in range(n):
            yield random.randint(number2,number1)
    else:
        print("Numbers must not be equal")
72/11:
for num in rand_num(1,10,12):
    print(num)
72/12:
def rand_num(low,high,n):
    number1 = int(input("Enter a number: "))
    number2 = int(input("\nEnter a number: "))

    if number1 < number2:
        for x in range(n):
            yield random.randint(number1,number2)
    
    elif number1 > number2:
        for x in range(n):
            yield random.randint(number2,number1)
    else:
        print("Numbers must not be equal")
72/13:
for num in rand_num(1,10,12):
    print(num)
72/14:
s = 'hello'

s_iter = iter(s)

next(s_iter)

#code here
72/15:
s = 'hello'

s_iter = iter(s)

next(s_iter)

#code here
72/16:
s = 'hello'

s_iter = iter(s)

next(s_iter)

#code here
72/17: next(s_iter)
72/18:
my_list = [1,2,3,4,5]

gencomp = (item for item in my_list if item > 3)

for item in gencomp:
    print(item)
72/19:
def gen_func(x):
    for i in range(x):
        yield i
72/20: print(gen_func(10))
72/21:
for i in gen_func(10):
    print(i)
72/22:
gen = gen_func(10)
next(gen)
72/23: next(gen)
73/1: from collections import Counter
73/2: mylist = [1,1,1,1,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3]
73/3: Counter(mylist)
73/4: mylist = ['a','a','a',10,10,10]
73/5: Counter(mylist)
73/6: Counter('aaaaaaaaaaaabbbbfdfdskÅlf')
73/7: sentence = "How many times does each word show up in this sentence with  a word"
73/8: type(sentence.split())
73/9: Counter(sentence.split())
73/10: Counter(sentence.lower().split())
73/11: letters = 'aaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbccccccccccccddddddd'
73/12: c = Counter(letters)
73/13: c
73/14: c.most_common()
73/15: c.most_common(2)
73/16: from collections import defaultdict
73/17: d = {'a':10 }
73/18: d['a']
73/19: d['WRONG']
73/20: d = defaultdict(lambda: 0)
73/21: d['correct']
73/22: d['correct'] = 100
73/23: d['correct']
73/24: d
73/25: print(d)
73/26: d['WRONG KEY!']
73/27: mytuple = (10,20,30)
73/28: mytuple.count()
73/29: mytuple.count(10)
73/30: mytuple[0]
73/31: from collections import namedtuple
73/32: Dog = namedtuple('Dog',['age','breed','name'])
73/33: Dog
73/34: sammy = Dog(age=5,breed = 'Husky',name='Sam')
73/35: type(sammy)
73/36: sammy
73/37: sammy.age
73/38: sammy.breed
73/39: sammy[0]
74/1: pwd
74/2:
f = open('practice.txt','w+')
f.write('This is a test string')
f.close()
74/3: import os
74/4: os.getcwd()
74/5: os.listdir()
74/6: os.listdir(os.getcwd())
74/7: os.listdir("..")
74/8: os.listdir("/etc")
74/9: import shutil
74/10: shutil.move("practice.txt","~")
74/11: os.getcwd()
74/12: shutil.move("practice.txt","home/yasar")
74/13:
f = open('practice.txt','w+')
f.write('This is a test string')
f.close()
74/14: import os
74/15: os.getcwd()
74/16: os.listdir()
74/17: os.listdir("/etc")
74/18: import shutil
74/19: shutil.move("practice.txt","home/yasar")
74/20: os.getcwd()
74/21: shutil.move("practice.txt","/home/yasar")
74/22: os.listdir('~/')
74/23: os.listdir('~')
74/24: os.listdir('home/yasar')
74/25: os.listdir('/home/yasar/Aptal_Odev/')
74/26: import send2trash
74/27: shutil.move("/home/yasar/practice.txt",os.getcwd())
74/28: os.listdir('.')
74/29: type(os.listdir("."))
74/30: send2trash.send2trash("./practice.txt")
74/31: os.listdir(".")
74/32: os.walk
74/33: type(os.getcwd())
74/34: os.walk(".")
74/35:
for i in os.walk("."):
    print i
74/36:
for i in os.walk("."):
    print(i)
74/37: (os.getcwd()
74/38: (os.getcwd())
74/39: os.walk("./Complete-Python-3-Bootcamp/")
74/40: a =  os.walk("./Complete-Python-3-Bootcamp/")
74/41:
for i in a:
    print(i)
74/42:
for i,j,k in a:
    print(i,j,k)
74/43:
for i,j,k in a:
    print(i,j,k)
74/44:
for i,j,k in a:
    print(i,j)
74/45:
for folder,sub_folder,files in os.walk("./Complete-Python-3-Bootcamp/12-Advanced Python Modules/"):
    print(f"The folders are {folder})
74/46:
for folder,sub_folder,files in os.walk("./Complete-Python-3-Bootcamp/12-Advanced Python Modules/"):
    print(f"The folders are {folder}")
74/47:
for folder,sub_folder,files in os.walk("./Complete-Python-3-Bootcamp/12-Advanced Python Modules/"):
    print(f"The folders are {folder}")
    for sub_f in folder:
        print("The subfolders are {}".format(sub_f))
74/48:
for folder,sub_folder,files in os.walk(path):
    print(f"The folders are {folder}")
    for sub_f in  os.walk(path+f"/{folder})
        print("The subfolders are {}".format(sub_f))
74/49:
for folder,sub_folder,files in os.walk(path):
    print(f"The folders are {folder}")
    for sub_f in  os.walk(path+f"/{folder}")
        print("The subfolders are {}".format(sub_f))
74/50:
for folder,sub_folder,files in os.walk(path):
    print(f"The folders are {folder}")
    for sub_f in  os.walk(path+f"/{folder}"):
        print("The subfolders are {}".format(sub_f))
74/51: path = "./Complete-Python-3-Bootcamp/12-Advanced Python Modules/"
74/52:
for folder,sub_folder,files in os.walk(path):
    print(f"The folders are {folder}")
    for sub_f in  os.walk(path+f"/{folder}"):
        print("The subfolders are {}".format(sub_f))
74/53:
for folder,sub_folder,files in os.walk(path):
    print(f"The folders are {folder}")
    print("\n")
    print("\n")
    for sub_f in  os.walk(path+f"/{folder}"):
        print("The subfolders are {}".format(sub_f))
74/54:
for folder,sub_folder,files in os.walk(path):
    print(f"The folders are {folder}")
    print("\n")
    print("\n")
    for sub_f in  os.walk(path+f"{folder}"):
        print("The subfolders are {}".format(sub_f))
74/55:
for folder,sub_folder,files in os.walk(path):
    print(f"The folders are {folder}")
    print("\n")
    print("\n")
    for sub_f in  os.walk(folder):
        print("The subfolders are {}".format(sub_f))
74/56:
for a b,c in os.walk(path):
    print(a,b,c)
74/57:
for a,b,c in os.walk(path):
    print(a,b,c)
74/58: for folder, sub_folder, files in os.walk(path):
74/59:
for folder, sub_folder, files in os.walk(path):
    print(f"The folders are {folder})
    print("\n")
    print("The subfolders are {}".format(os.walk(folder)))
74/60:
for folder, sub_folder, files in os.walk(path):
    print(f"The folders are {folder}")
    print("\n")
    print("The subfolders are {}".format(os.walk(folder)))
74/61:
for a,b,c in range(10):
    print(a,b,c)
74/62:
for a in range(10):
    print(a)
74/63:
for a,b in range(10):
    print(a)
74/64:
for a,b in (1,2):
    print(a,b)
74/65:
for a,b in (1,2):
    print(a)
    print(b)
74/66:
for a in (1,2):
    print(a)
74/67:
for a,b in ([1,2],[3,4])
    print(a,b)
74/68:
for a,b in ([1,2],[3,4]):
    print(a,b)
74/69:
for a,b in ([1,2],[3,4],[5,6],[7,8])
    print(a,b)
74/70:
for a,b in ([1,2],[3,4],[5,6],[7,8]):
    print(a,b)
74/71:
for a,b,c in os.walk("."):
    print(a,b,c)
74/72:
for a,b,c in os.walk("/home/yasar/Lectures/"):
    print(a,b,c)
74/73:
for a,b,c in os.walk("/home/yasar/Lectures/"):
    print(a)
    print(b)
    print(c)
74/74:
for a,b,c in os.walk("/home/yasar/Lectures/"):
    print(a)
    print("-------------------------------------")
    print(b)
    print("-------------------------------------")
    print(c)
74/75: for folder, subfolder,files in os.walk(path):
74/76:
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
    print("Currently looking at {}".format(folder)):
    print("\n")
    print(f"The subfolders of {folder}:")
    for sub_f in subfolder:
        print(sub_f)
74/77:
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
    print("Currently looking at {}".format(folder))
    print("\n")
    print(f"The subfolders of {folder}:")
    for sub_f in subfolder:
        print(sub_f)
74/78:
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
    print("Currently looking at {}".format(folder))
    print("\n")
    print(f"The files of {folder}:")
    for f_iles in files:
        print(f_iles)
74/79:
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
    print("Currently looking at {}".format(folder))
    print("\n")
    print(f"The files of {folder}:")
    print("|\n|__")
        print(f_iles)
74/80:
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
    print("Currently looking at {}".format(folder))
    print("\n")
    print(f"The files of {folder}:")
    print("|\n|__")
    for f_iles in files:
        print(f_iles)
74/81:
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
    print("Currently looking at {}".format(folder))
    print("\n")
    print(f"The files of {folder}:")
    print("|\n|-->")
    for f_iles in files:
        print(f_iles)
74/82:
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
    print("Currently looking at {}".format(folder))
    print("\n")
    print(f"The files of {folder}:")
    print("|\n|-->")
    for sub in subfolder:
        print(sub)
74/83:
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
    print("Currently looking at {}".format(folder))
    print("\n")
    print(f"The files of {folder}:")
    print("|\n|-->")
    for f in files:
        print(f)
74/84:
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
    print("Currently looking at {}".format(folder))
    print("\n")
    print(f"The files of {folder}:")
    print("|\n|-->")
74/85:
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
   # print("Currently looking at {}".format(folder))
    #print("\n")
    print(f"The files of {folder}:")
    #print("|\n|-->")
74/86:
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
   # print("Currently looking at {}".format(folder))
    #print("\n")
    print(f"The files of {folder} , {subfolder}:")
    #print("|\n|-->")
74/87:
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
   # print("Currently looking at {}".format(folder))
    #print("\n")
    print(f"The files of {folder} , {subfolder}:")
    #print("|\n|-->")
74/88:
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
   # print("Currently looking at {}".format(folder))
    #print("\n")
    print(f"The files of {folder} , {files}:")
    #print("|\n|-->")
74/89:
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
   # print("Currently looking at {}".format(folder))
    #print("\n")
    print(f"The files of {folder} , {subfolder}:")
    #print("|\n|-->")
74/90:
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
    print("Currently looking at {}".format(folder))
    print("\n")
    print(f"The files of {folder} , {subfolder}:")
    #print("|\n|-->")
74/91:
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
    print("Currently looking at {}".format(folder)):
    for sub in subfolder:
        print("The subfolders are {}".format(subfolder))
    for file in files:
        print(file)
74/92:
for folder, subfolder,files in os.walk():
    print("Currently looking at {}".format(folder))
    print("\n")
    #print("|\n|-->")
74/93:
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
    print("Currently looking at {}".format(folder)):
 )
74/95:
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
    print("Currently looking at {}".format(folder)):
74/96:
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
    print("Currently looking at {}".format(folder))
74/97:
for folder, subfolder,files in os.walk():
    if folder in subfolder:
        continue:
    print(folder)
74/98:
for folder, subfolder,files in os.walk():
    if folder in subfolder:
        continue
    print(folder)
74/99:
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
    if folder in subfolder:
        continue
    print(folder)
74/100:
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
    if folder in subfolder:
        print(folder)
74/101: type(os.walk("/home/yasar/Lectures/"))
74/102:
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
   print(type(folder),type(subfolder),type(files))
74/103: len(os.walk("/home/yasar/Lectures/"))
74/104: os.walk(".")[1]
74/105:
for i in os.walk(".")[1]:
    print(i)
74/106:
subfolderlist = []
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
        print(subfolder)
74/107:
subfolderlist = []
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
       print("The current directory is{}".format(folder))
        for f in files:
            print(f)
            break
74/108:
subfolderlist = []
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
        print("The current directory is{}".format(folder))
        for f in files:
            print(f)
            break
74/109:
subfolderlist = []
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
        print("The current directory is{}".format(folder))
        for files in folder:
            print(f)
            break
74/110:
subfolderlist = []
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
        print("The current directory is{}".format(folder))
        for files in folder:
            print(files)
            break
74/111:
subfolderlist = []
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
        print("The current directory is{}".format(folder))
        for fi in folder:
            print(files)
            break
74/112:
subfolderlist = []
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
        print("The current directory is{}".format(folder))
        for fi in folder:
            print(files)
            exit()
76/1:
subfolderlist = []
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
        print("The current directory is{}".format(folder))
        for fi in folder:
            print(files)
            exit()
76/2: import os
76/3:
subfolderlist = []
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
        print("The current directory is{}".format(folder))
        for fi in folder:
            print(files)
            exit()
77/1:
subfolderlist = []
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
        print("The current directory is{}".format(folder))
        for fi in folder:
            print(files)
            exit(0)
77/2:
import os
subfolderlist = []
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
        print("The current directory is{}".format(folder))
        for fi in folder:
            print(files)
            exit(0)
78/1:
import os
subfolderlist = []
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
        print("The current directory is{}".format(folder))
        for fi in folder:
            print(fi)
            exit(0)
79/1:
import os
subfolderlist = []
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
        print("The current directory is{}".format(folder))
79/2:
import os
subfolderlist = []
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
    yield folder
79/3:
import os
subfolderlist = []
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
        yield folder
79/4:
import os
subfolderlist = []
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
            yield folder
79/5:
import os
subfolderlist = []
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
    print(f"Your current directory is {folder}")
        for s_ in subfolder:
            print(f"\tThe subfolders are {s_})
    
        for f in files:
            print(f"The files are {f}")
79/6:
import os
subfolderlist = []
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
    print(f"Your current directory is {folder}")
    for s_ in subfolder:
        print(f"\tThe subfolders are {s_})
    
    for f in files:
        print(f"The files are {f}")
79/7:
import os
subfolderlist = []
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
    print(f"Your current directory is {folder}")
    for s_ in subfolder:
        print(f"\tThe subfolders are {s_}")
    
    for f in files:
        print(f"The files are {f}")
79/8:
import os
subfolderlist = []
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
    print(f"Your current directory is {folder}")
    for s_ in subfolder:
        print(f"\tThe subfolders are {s_}")
    
    for f in files:
        print(f"\tThe files are {f}")
79/9:
import os
subfolderlist = []
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
    print(f"Your current directory is {folder}")
    for s_ in subfolder:
        print(f"\tThe subfolders are {s_}")
        print("\n")
    
    for f in files:
        print(f"\tThe files are {f}")
        print("\n")
79/10:
import os
subfolderlist = []
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
    print(f"Your current directory is {folder}")
    for s_ in subfolder:
        print(f"\tThe subfolders are {s_}")
        print("\n")
    
    for f in files:
        print(f"\tThe files are {f}")
    print("\n")
79/11:
import os
subfolderlist = []
for folder, subfolder,files in os.walk("/home/yasar/Lectures/"):
    print(f"Your current directory is {folder}")
    for s_ in subfolder:
        print(f"\tThe subfolders are {s_}")
    print("\n")
    
    for f in files:
        print(f"\tThe files are {f}")
    print("\n")
80/1: import datetime
80/2: mytime = datetime.time()
80/3: print(mytime)
80/4: mytime = datetime.time(2,20)
80/5: print(mytime)
80/6: mytime.minute
80/7: mytime.minute.as_integer_ratio
80/8: mytime.minute.as_integer_ratio()
80/9: datetime.time(2,20).minute()
80/10: datetime.time(2,20).minute
80/11: mytime = datetime.time(2,20)
80/12: mytime.minute
80/13: mytime.hour
80/14: print(mytime)
80/15: mytime = datetime.time(2)
80/16: mytime.minute
80/17: mytime.hour
80/18: print(mytime)
80/19: mytime.microsecond
80/20: mytime = datetime.time(13,20,1,20)
80/21: mytime.minute
80/22: mytime.hour
80/23: print(mytime)
80/24: mytime.microsecond
80/25: type(mytime)
80/26: today = datetime.date.today
80/27: print(today)
80/28: print(today)
80/29: print(1,2)
80/30: a =  {"y":1 , "t":2}
80/31: print(a["y"])
80/32: print(a[1])
80/33: print(a["t"])
80/34: a =  {"y":1 , "t":2, 0:2}
80/35: print(a[0])
80/36: print(today)
80/37: today = datetime.date.today()
80/38: print(today)
80/39: today.year
80/40: today.day
81/1:
class NameOfClass():
    def __init__(self,param1,param2):
        self.param1 = param1
        self.param2 = param2
        
    def some_method(self):
        #perform some action
        print(self1.param)

    yasar = NameOfClass("A","B")
81/2:
class NameOfClass():
    def __init__(self,param1,param2):
        self.param1 = param1
        self.param2 = param2
        
    def some_method(self):
        #perform some action
        print(self1.param)

    yasar = NameOfClass("A","B")
81/3:
class NameOfClass():
    def __init__(self,param1,param2):
        self.param1 = param1
        self.param2 = param2
        
    def some_method(self):
        #perform some action
        print(self1.param)

yasar = NameOfClass("A","B")
81/4: print(yasar)
81/5: print(yasar.param1)
80/41: today.ctime()
80/42: from datetime import datetime
80/43: mydatetime = datetime(2021,10,3,14,20,1)
80/44: print(mydatetime)
80/45: mydatetime.year = 2020
80/46: mydatetime.replace(year = 2020)
80/47: print(mydatetime)
80/48: mydatetime = mydatetime.replace(year = 2020)
80/49: print(mydatetime)
80/50: datetime.__file__
80/51: print(datetime.__file__)
80/52: print(os.__file__)
80/53: today.year
80/54: today.year()
80/55: today.year = 1200
80/56: datetime.fromisoformat
80/57: datetime.fromisoformat("2023")
80/58: datetime.fromisoformat("1234567890")
80/59: datetime.fromisoformat("2023-12-12")
80/60: print(datetime.fromisoformat("2023-12-12"))
80/61: print(datetime.ctime()
80/62: datetime.__format__
80/63: datetime.__file___
80/64: datetime.__file__
80/65: from datetime import date
80/66:
date1 = date(2021,11,3)
date2 = date(2020,11,3)
80/67: result = date1 - date2
80/68: type(result)
80/69: result.days
80/70:
date1 = date(2021,11,2)
date2 = date(2020,11,3)
80/71: result.days
80/72:
date1 = date(2021,11,2)
date2 = date(2020,11,3)
80/73: result = date1 - date2
80/74: type(result)
80/75: result.days
80/76: from datetime import datetime
80/77: datetime1 = datetime(2021,11,3)
80/78: datetime2 = datetime(2020,11,3,12,0)
80/79: datetime1 - datetime2
80/80: datetime1 = datetime(2021,11,3,22,0)
80/81: datetime2 = datetime(2020,11,3,12,0)
80/82: datetime1 - datetime2
80/83: mydiff = datetime1 - datetime2
80/84: mydiff.seconds
80/85: mydiff.total_seconds()
82/1: import math
82/2: help(math)
82/3: value = 4.35
82/4: math.floor(value)
82/5: math.ceil(value)
82/6: round(value)
82/7: round(4.5)
82/8: round(5.5)
82/9: round(6.5)
82/10: math.pi
82/11: import turtle
82/12: turtle.draw()
82/13: turtle.draw
82/14: turtle.up(20)
82/15: turtle.up()
82/16: turtle.up(20)
82/17: math.e
82/18: math.inf
82/19: math.man
82/20: math.nan
82/21: import math
82/22: math.log(math.e)
82/23: math.log(100,10)
82/24: math.sin(math.pi/2)
82/25: math.sin(10)
82/26: math.sin(-10)
82/27: math.degrees(pi/2)
82/28: math.degrees(math.pi/2)
82/29: math.radians(180)
82/30: math.radians(90)
82/31: import random
82/32: random.randint(0,100)
82/33: random.randint(0,100)
82/34: random.randint(0,100)
82/35: random.randint(0,100)
82/36: random.seed()
82/37: random.randint(0,100)
82/38: print(random.randint(0,100))
82/39: print(random.randint(0,100))
82/40: random.seed(101)
82/41: print(random.randint(0,100))
82/42: print(random.randint(0,100))
82/43: random.randint(0,100)
82/44:
print(random.randint(0,100))
print(random.randint(0,100))
print(random.randint(0,100))
print(random.randint(0,100))
82/45: random.seed(101)
82/46:
print(random.randint(0,100))
print(random.randint(0,100))
print(random.randint(0,100))
print(random.randint(0,100))
82/47:
print(random.randint(0,100))
print(random.randint(0,100))
print(random.randint(0,100))
print(random.randint(0,100))
print(random.randint(0,100))
print(random.randint(0,100))
82/48: import random
82/49: random.randint(0,100)
82/50: random.seed(101)
82/51:
print(random.randint(0,100))
print(random.randint(0,100))
print(random.randint(0,100))
print(random.randint(0,100))
print(random.randint(0,100))
print(random.randint(0,100))
82/52: random.seed(100)
82/53:
print(random.randint(0,100))
print(random.randint(0,100))
print(random.randint(0,100))
print(random.randint(0,100))
print(random.randint(0,100))
print(random.randint(0,100))
82/54: random.seed(101)
82/55:
print(random.randint(0,100))
print(random.randint(0,100))
print(random.randint(0,100))
print(random.randint(0,100))
print(random.randint(0,100))
print(random.randint(0,100))
82/56: random.choice(range(20))
82/57: # Sample with replacement
82/58: random.choices(range(20),10)
82/59: random.choices(range(20),k = 10)
82/60: random.choices(range(20),k = 10)
82/61: random.choices(range(20),cum_weights=10,20)
82/62: random.choices(range(20),10,cum_weights=10,20)
82/63:
#Sample without replacemen
random.sample(population=)
82/64:
def add (num1 =0 , num2)
    return num1 + num2
82/65:
def add (num1=0, num2)
    return num1 + num2
82/66:
def add (num1=0, num2 = 0)
    return num1 + num2
82/67:
def add (num1=0, num2):
    return num1 + num2
82/68:
def add (num1=0, num2=0):
    return num1 + num2
82/69: add()
82/70: add(10,20)
82/71: add(num1=10,num2=12)
82/72:
#Sample without replacemen
random.sample(population=range(20),k = 10)
82/73: random.shuffle(range(20))
82/74: random.shuffle(list(range(20))
82/75: random.shuffle(list(range(20)))
82/76: a = random.shuffle(list(range(20)))
82/77: print(a)
82/78: mylist = [1,2,4]
82/79: mylist = [range(20)
82/80: mylist =range(20)
82/81: random.shuffle(mylist)
82/82: random.shuffle(mylist)
82/83: mylist =[1,2,3,4,0,5,3,10231]
82/84: random.shuffle(mylist)
82/85: mylist
82/86: random.uniform(a = 0, b = 100)
82/87: random.gauss(mu = 0 , sigma=1)
83/1:
# Phone Number
# (555)-555-5555

# Regex Pattern
# r"(\d\d\d)-\d\d\d-\d\d\d\d
83/2: text = "The agent's phone number is 408-555-1234. Call soon"
83/3: 'phone' in text
83/4: import re
83/5: pattern = 'phone'
83/6: re.search(pattern)
83/7: re.search(pattern,text)
83/8: pattern = 'The'
83/9: re.search(pattern,text)
83/10: pattern = "NOT IN TEXT"
83/11: re.search(pattern,text)
83/12: re.search(pattern,text).span()
83/13: re.search(pattern,text).span()
83/14: re.search(pattern,text).span
83/15: re.search(pattern,text)
83/16: import re
83/17: pattern = 'The'
83/18: re.search(pattern,text)
83/19: pattern = "NOT IN TEXT"
83/20: re.search(pattern,text)
83/21: pattern = 'phone'
83/22: re.search(pattern,text)
83/23: match = re.search(pattern,text)
83/24: match.span()
83/25: match(start)
83/26: match.start
83/27: match.start()
83/28: match.end()
83/29: text = 'my phone once, my phone twice'
83/30: pattern = "phone"
83/31: match = re.search(pattern,text)
83/32: text = 'my phone once, my phone twice'
83/33: match
83/34: matches = re.findall('phone')
83/35: matches = re.findall('phone',text)
83/36: text = 'my phone once, my phone twice'
83/37: match
83/38: matches = re.findall('phone',text)
83/39: matches
83/40: len(matches)
83/41:
for match in re.finditer('phone',text):
    print(match)
83/42:
for match in re.finditer('phone',text):
    print(match)
    print(match.groupdict())
83/43:
for match in re.finditer('phone',text):
    print(match)
    print(match.group())
83/44:
for match in re.finditer('phone',text):
    print(match.span())
83/45:
for match in re.finditer('phone',text):
    print(match)
    print(match.group())
    print(match.span())
83/46: text = 'My phone number is 408-555-1234'
83/47: phone = re.search('408-555-1234',text)
83/48: text = 'My phone number is 408-555-1234'
83/49: phone = re.search('408-555-1234',text)
83/50: phone
83/51: phone = re.search(r'\d\d\d-\d\d\d-\d\d\d\d',text)
83/52: phone
83/53: text = 'My phone number is 408-555-7980'
83/54: phone = re.search(r'\d\d\d-\d\d\d-\d\d\d\d',text)
83/55: phone
83/56: phone.group()
83/57: phone = re.search(r'\d{3}-\d{3}-\d{4}',text)
83/58: phone
83/59: phone_pattern = re.compile(r'(\d{3})-(\d{3})-(\d{4}))
83/60: phone_pattern = re.compile(r'(\d{3})-(\d{3})-(\d{4})')
83/61: phone_pattern
83/62: phone_pattern = re.compile(r'(\d{3})-(\d{3})-(\d{4})',text)
83/63: phone_pattern = re.compile(r'(\d{3})-(\d{3})-(\d{4})')
83/64: results = re.search(phone_pattern,text)
83/65: results
83/66: results.group()
83/67: results.group()
83/68: results.span()
83/69: results.group(1)
83/70: results.group(2)
83/71: results.group(3)
83/72: results.group(4)
83/73: re.search(r'cat','The cat is here')
83/74: re.search(r'cat|dog','The dog is here')
83/75: re.search(r'cat|dog','The dog is here, cat is between there')
83/76: search = re.search(r'cat|dog','The dog is here, cat is between there')
83/77: search.group()
83/78: search.group(2)
83/79: search = re.findall(r'cat|dog','The dog is here, cat is between there')
83/80: search
83/81: search.span()
83/82: re.search(r'cat|dog','The dog is here, cat is between there')
83/83: re.findall(r'at','The cat in the hat sat there')
83/84: re.findall(r'.at','The cat in the hat sat there')
83/85: re.findall(r'.at','The cat in the hater sat there')
83/86: re.findall(r'.at.','The cat in the hater sat there')
83/87: re.findall(r'.at.r','The cat in the hater sat there')
83/88: re.findall(r'.at,','The cat in the hater sat there')
83/89: re.findall(r'.at','The cat in the hater sat there')
83/90: re.findall(r'.at','The cat in the chater sat there')
83/91: re.findall(r'..at','The cat in the chater sat there')
83/92:
#Starts with digit
re.findall(r'^\d','The 2 is a number')
83/93:
#Starts with digit
re.findall(r'^\d','The a2 is a number')
83/94:
#Starts with digit
re.findall(r'^\d','2 is a number')
83/95:
#Ends with digit
re.findall(r'\d$','2 is a number')
83/96:
#Ends with digit
re.findall(r'\d$','The number is 22')
83/97: phrase = 'there are 3 numbers 34 inside 5 this sentence.'
83/98: pattern = r'[^\d]'
83/99: re.findall(pattern,phrase)
83/100: pattern = r'[^\]+'
83/101: pattern = r'[^\d]+'
83/102: phrase = 'there are 3 numbers 34 inside 5 this sentence.'
83/103: pattern = r'[^\d]+'
83/104: re.findall(pattern,phrase)
83/105: phrase = 'there are 3 numbers 34 inside 5 this sentence.'
83/106: pattern = r'[^\d]'
83/107: re.findall(pattern,phrase)
83/108: phrase = 'there are 3 numbers 34 inside 5 this sentence.'
83/109: pattern = r'[^\d]'
83/110: re.findall(pattern,phrase)
83/111: re.search(pattern,phrase)
83/112: pattern = r'[^\d+'
83/113: re.search(pattern,phrase)
83/114: pattern = r'[^\d]+'
83/115: re.search(pattern,phrase)
83/116: phrase = 'there ar7e 3 numbers 34 inside 5 this sentence.'
83/117:
#One or more
pattern = r'[^\d]+'
83/118: re.search(pattern,phrase)
83/119:
#One or more
pattern = r'[^\d]'
83/120: re.search(pattern,phrase)
83/121: test_phrase = "This is a string! But it has punctuation.How can we remove it?"
83/122:
test_pattern = r'[^\w]+'
sum = re.findall(test_pattern,test_phrase)
83/123: sum
83/124:
test_pattern = r'[\w]+'
sum = re.findall(test_pattern,test_phrase)
83/125: sum
83/126: type(sum)
83/127:
a = ""
for word in sum:
    a +=word
83/128: print(a)
83/129:
a = ""
for word in sum:
    a +=word + " "
83/130: print(a)
83/131: test_pattern = r'[^!.?]+'
83/132: sum = re.findall(test_pattern,test_phrase)
83/133: sum
83/134: " ".join(sum)
83/135: text = 'Only find the hypen-words in this sentence. But you do not know how long-ish they are'
83/136: test_pattern = r'[-]+'
83/137: test = re.findall(test_pattern,text)
83/138: test
83/139: test_pattern = r'[-*]+'
83/140: test = re.findall(test_pattern,text)
83/141: test
83/142: test_pattern = r'[-*]'
83/143: test = re.findall(test_pattern,text)
83/144: test
83/145: test_pattern = r'[^ ]|[-]'
83/146: test = re.findall(test_pattern,text)
83/147: test
83/148: test_pattern = r'[^ ]+|[-]'
83/149: test = re.findall(test_pattern,text)
83/150: test
83/151: test_pattern = r'[^ ]+&[-]'
83/152: test = re.findall(test_pattern,text)
83/153: test
83/154: test_pattern = r'[ ]+&[-]'
83/155: test = re.findall(test_pattern,text)
83/156: test
83/157: test_pattern = r'[]+&[-]'
83/158: test = re.findall(test_pattern,text)
83/159: test
83/160: test_pattern = r'[^a]+&[-]'
83/161: test = re.findall(test_pattern,text)
83/162: test
83/163: test_pattern = r'[\w]+&[-]'
83/164: test = re.findall(test_pattern,text)
83/165: test
83/166: test_pattern = r'[\w]+|[-]'
83/167: test = re.findall(test_pattern,text)
83/168: test
83/169: test_pattern = r'[\w]+-[\w]'
83/170: test = re.findall(test_pattern,text)
83/171: test
83/172: test_pattern = r'[\w]+-[\w]+'
83/173: test = re.findall(test_pattern,text)
83/174: test
83/175: test_pattern = r'[^ ]+-[\^ ]+'
83/176: test = re.findall(test_pattern,text)
83/177: test
83/178: test_pattern = r'[^ ]+-[^ ]+'
83/179: test = re.findall(test_pattern,text)
83/180: test
83/181: text = 'Hello, would you like some catfish?'
83/182: texttwo = 'Hello, would you like to take a catnap?'
83/183: textthree = 'Hello, have you seen this caterpillar?'
83/184: re.search(r'cat(fish|nap|claw)',text)
83/185: re.search(r'cat(fish|nap|claw)',texttwo)
83/186: re.search(r'cat(fish|nap|erpillar)',texttwo)
83/187: re.search(r'cat(fish|nap|erpillar)',textthree)
85/1:
def func_one(n):
    return [str(num) for in range(n)]
85/2:
def func_one(n):
    return [str(num) for num in range(n)]
85/3: func_one(10)
85/4:
def func_two(n):
    return list(map(str,range(n)))
85/5: func_two(10)
85/6: import time
85/7:
#Current time before

#Run code

#Current time after
85/8:
#Current time before
start_time = time.time()
#Run code
result = func_one(1000000)
#Current time after running code
end_time = time.time()
#Elapsed time
elapsed_time = end_time - start_time

print(elapsed_time)
85/9:
#Current time before
start_time = time.time()
#Run code
result = func_two(1000000)
#Current time after running code
end_time = time.time()
#Elapsed time
elapsed_time = end_time - start_time

print(elapsed_time)
85/10:
#Current time before
start_time = time.time()
#Run code
result = func_one(1000000)
#Current time after running code
end_time = time.time()
#Elapsed time
elapsed_time = end_time - start_time

print(elapsed_time)
85/11:
#Current time before
start_time = time.time()
#Run code
result = func_two(1000000)
#Current time after running code
end_time = time.time()
#Elapsed time
elapsed_time = end_time - start_time

print(elapsed_time)
85/12:
#Current time before
start_time = time.time()
#Run code
result = func_two(1000000)
#Current time after running code
end_time = time.time()
#Elapsed time
elapsed_time = end_time - start_time

print(elapsed_time)
85/13:
#Current time before
start_time = time.time()
#Run code
result = func_one(1000000)
#Current time after running code
end_time = time.time()
#Elapsed time
elapsed_time = end_time - start_time

print(elapsed_time)
85/14: import timeit
85/15:
stmt = '''
func_one(100)
'''
85/16:
setup = """
def func_one(n):
    return [str(num) for num in range(n)]
"""
85/17: timeit.timeit(stmt=stmt,setup=setup,number=100000)
85/18:
stmt = '''
func_two(100)
'''
85/19:
setup = '''
def func_two(n):
    return list(map(str,range(n)))
'''
85/20:
setup2 = '''
def func_two(n):
    return list(map(str,range(n)))
'''
85/21:
stmt2 = '''
func_two(100)
'''
85/22:
setup2 = '''
def func_two(n):
    return list(map(str,range(n)))
'''
85/23: timeit.timeit(stmt=stmt2,setup=setup2,number=100000)
85/24:
stmt = '''
func_one(100)
'''
85/25:
setup = """
def func_one(n):
    return [str(num) for num in range(n)]
"""
85/26: timeit.timeit(stmt=stmt,setup=setup,number=1000000)
85/27:
stmt2 = '''
func_two(100)
'''
85/28:
setup2 = '''
def func_two(n):
    return list(map(str,range(n)))
'''
85/29: timeit.timeit(stmt=stmt2,setup=setup2,number=1000000)
85/30:
%%timeit
func_one(100)
85/31:
%timeit
func_two(100)
85/32:
%%timeit
func_two(100)
86/1: f = open("forzip1.txt","w+")
86/2: f.writable()
86/3: f.write('ONE FÄ°LE')
86/4: f.close()
86/5:
f = open("forzip2.txt","w+")
f.write('TWO FÄ°LE')
f.close()
86/6: import zipfile
86/7: comp_file = zipfile.ZipFile('comp_file.zip','w')
86/8: comp_file.write('forzip1.txt',compress_type=zipfile.ZIP_DEFLATED)
86/9: comp_file.write('forzip2.txt',compress_type=zipfile.ZIP_DEFLATED)
86/10: comp_file.close()
86/11: zip_obj = zipfile.ZipFile('comp_file.zip','r')
86/12: zip_obj.extractall('extracted_content')
86/13: import shutil
86/14: pwd
86/15: output_filename = 'example'
86/16: dir_to_zip ='/home/yasar/Python-Learning/extracted_content'
86/17: pwd
86/18: dir_to_zip ='/home/yasar/Python-Learning/
86/19: dir_to_zip ='/home/yasar/Python-Learning/'
86/20: import shutil
86/21: dir_to_zip ='/home/yasar/Python-Learning/'
86/22: pwd
86/23: output_filename = 'example'
86/24: shutil.make_archive(output_filename,'zip',dir_to_zip)
86/25: shutil.unpack_archive('example.zip','final_unzip','zip')
86/26: shutil.unpack_archive('example.zip','final_unzip','zip')
86/27: dir_to_zip ='/home/yasar/Python-Learning/extracted_content/'
86/28: pwd
86/29: output_filename = 'example'
86/30: shutil.make_archive(output_filename,'zip',dir_to_zip)
86/31: shutil.unpack_archive('example.zip','final_unzip','zip')
86/32: import shutil
86/33: dir_to_zip ='/home/yasar/Python-Learning/extracted_content/'
86/34: pwd
86/35: output_filename = 'example'
86/36: shutil.make_archive(output_filename,'zip',dir_to_zip)
86/37: shutil.unpack_archive('example.zip','final_unzip','zip')
86/38: output_filename = 'example'
86/39: shutil.make_archive(output_filename,'zip',dir_to_zip)
86/40: shutil.unpack_archive('example.zip','/home/yasar/Python-Learning/final_unzip','zip')
86/41: pwd
89/1: pwd
89/2: import shutil
89/3:
import shutil
import os
89/4: shutil.unpack_archive('unzip_me_for_instructions.zip','/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file','zip')
89/5: f = open('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/Instructions.txt','r')
89/6: f.readable()
89/7: f.readlines()
89/8:
for files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
        print(files)
89/9:
for files,subfolder,file in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    print(type(file))
89/10:
for folder,subfolder,file in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for f in file:
        if ".txt" in f:
            print(f)
89/11:
import shutil
import os
import re
89/12: type(f.readlines())
89/13: f = open('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/Instructions.txt','r')
89/14: f.readable()
89/15: f.readlines()
89/16: type(f.readlines())
89/17:
for folder,subfolder,file in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for f in file:
        if ".txt" in f:
            r = open(f,"r")
            reading = r.readlines()
            if re.findall(r"\d\d\d-\d\d\d-\d\d\d\d","".join(reading)) != []:
                print(f)
89/18:
for folder,subfolder,file in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for f in file:
        if ".txt" in f:
            r = open(f,"r")
            reading = r.readlines()
           # if re.findall(r"\d\d\d-\d\d\d-\d\d\d\d","".join(reading)) != []:
        #    print(f)
89/19:
for folder,subfolder,file in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for f in file:
        if ".txt" in f:
            r = open(f,"r")
            r.readlines()
           # if re.findall(r"\d\d\d-\d\d\d-\d\d\d\d","".join(reading)) != []:
        #    print(f)
89/20:
for folder,subfolder,file in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for f in file:
        if ".txt" in f:
            print(f)
           # r = open(f,"r")
           # r.readlines()
           # if re.findall(r"\d\d\d-\d\d\d-\d\d\d\d","".join(reading)) != []:
        #    print(f)
89/21:
for folder,subfolder,file in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for f in file:
        if ".txt" in f:
            print(f)
            r = open(f"{f}","r")
           # r.readlines()
           # if re.findall(r"\d\d\d-\d\d\d-\d\d\d\d","".join(reading)) != []:
        #    print(f)
89/22:
for folder,subfolder,file in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for f in file:
        if ".txt" in f:
            print(f)
            r = open(f"{f}","r+")
           # r.readlines()
           # if re.findall(r"\d\d\d-\d\d\d-\d\d\d\d","".join(reading)) != []:
        #    print(f)
89/23:
for folder,subfolder,file in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for f in file:
        if ".txt" in f:
           
            r = open(f,"r+")
           # r.readlines()
           # if re.findall(r"\d\d\d-\d\d\d-\d\d\d\d","".join(reading)) != []:
        #    print(f)
89/24:
for folder,subfolder,file in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for f in file:
        if ".txt" in f:
           
            re = open(f,"r+")
           # r.readlines()
           # if re.findall(r"\d\d\d-\d\d\d-\d\d\d\d","".join(reading)) != []:
        #    print(f)
89/25:
for folder,subfolder,file in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for f in file:
        if ".txt" in f:
            print(type(f))
           # r.readlines()
           # if re.findall(r"\d\d\d-\d\d\d-\d\d\d\d","".join(reading)) != []:
        #    print(f)
89/26:
for folder,subfolder,file in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for f in file:
        if ".txt" in f:
            fi = open(f,'r')
           # r.readlines()
           # if re.findall(r"\d\d\d-\d\d\d-\d\d\d\d","".join(reading)) != []:
        #    print(f)
89/27:
for folder,subfolder,file in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for f in file:
        if ".txt" in f:
            fi = open(subfolder+f,'r')
           # r.readlines()
           # if re.findall(r"\d\d\d-\d\d\d-\d\d\d\d","".join(reading)) != []:
        #    print(f)
89/28:
for folder,subfolder,file in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for sub in subfolder:
        for f in subfolder:
            print(file)
89/29:
for folder,subfolder,file in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for sub in subfolder:
        print(sub)
89/30:
for folder,subfolder,file in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for sub in subfolder:
        print(file)
89/31:
for folder,subfolder,file in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for subs in folder:
        for fl in subs:
            print(fl)
89/32:
for folder,subfolder,file in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for subs in folder:
        print(subs)
89/33:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
   
    for f in files:
        print(f)
89/34:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
   
    for f in files:
        file_o = open(f,"r")
        file_o.readlines()
89/35:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
   
    for f in files:
        print(type(f))
89/36:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
   
    for f in files:
        file_open =
89/37:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
   
    for sub in subfolder:
           print(sub)
89/38:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
   
    for sub in subfolder:
           type(subfolder)
89/39:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
   
    for sub in subfolder:
           print(type(subfolder))
89/40:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
   
    for sub,f in subfolder,files:
            print(sub,f)
89/41:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
   
            if files in subfolder:
                print(files,subfolder)
89/42:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
   
            if files in subfolder:
                print(files,subfolder)
89/43:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
   
            if files in subfolder:
                print(files,subfolder)
89/44:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
   
        for sub in subfolder:
            if files in f (for k in files):
                print(f)
89/45:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
   
        for sub in subfolder:
            if files in (f for k in files):
                print(f)
89/46:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
   
        for sub in subfolder:
            if files in (f for k in files):
                 print(type(k))
89/47:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
   
        for sub in subfolder:
            if files in (f for k in files):
                 print(type(k))
89/48:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
   
        for sub in subfolder:
           for f in files:
                print(type(f))
89/49:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    str = ""
    for i in range(len(files)):
        str= folder + files
        print(str)
89/50:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    str = ""
    for i in rfiles:
        str= folder + i
        print(str)
89/51:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    str = ""
    for i in files:
        str= folder + i
        print(str)
89/52:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    str = ""
    for s in subfolder:
        for f in files:
            str = s+f
            print(str)
89/53:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    str = ""
    for s in subfolder:
        print(s)
89/54:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    str = ""
    for s in subfolder:
        print(folder+s)
89/55:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    str = ""
    for s in subfolder:
        for f in files:
            print(folder+s+f)
89/56:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    str = ""
    for f in files:
        for s in subfolder:
            print(folder+s+f)
89/57:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    str = ""
    for f in files:
        for s in subfolder:
        print(folder+s+f)
89/58:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    str = ""
    for f in files:
        for s in subfolder:
            str=folder+f+s
            print(str)
89/59:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    str = ""
    for f in files:
        for s in subfolder:
            str=folder+s+f
            print(str)
89/60:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    str = ""
    for s_ in subfolder:
        print(s_)

    for f in files:
        print(f)
89/61:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    str = ""
    for s_ in subfolder:
        print(s_)

    for f in files:
        print(s_,f)
89/62:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    print(folder)
89/63:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    print(subfolder)
89/64:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    if subfolder != []:
        print(subfolder)
89/65:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    if subfolder != []:
        print(subfolder)
    
    print(files)
89/66:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    print(folder)
89/67:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for f in files:
        if f in os.listdir(folder):
            print(folder+f)
89/68:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for f in files:
        if f in os.listdir(folder):
            print(folder"/"+f)
89/69:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for f in files:
        if f in os.listdir(folder):
            print(folder+"/"+f)
89/70:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
        if s in os.listdir(folder):
            print(folder+s)
89/71:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
        if s in os.listdir(folder):
            for f in files:
                if f in os.listdir(s+f):
                    print(f)
89/72:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
        if s in os.listdir(folder):
            print(folder+s)
89/73:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
        if s in os.listdir(folder):
            for f in files:
                if f in os.listdir((folder+s))
                    print(f)
89/74:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
        if s in os.listdir(folder):
            for f in files:
                if f in os.listdir((folder+s)):
                    print(f)
89/75:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
        if s in os.listdir(folder):
            print(os.listdir(folder+s))
89/76:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
        if s in os.listdir(folder):
            for f in files:
                if f in (os.listdir(folder+s)):
                    print(f)
89/77:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
        if s in os.listdir(folder):
            for f in files:
                print(f)
89/78:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
        if s in os.listdir(folder):
            print(folder+s)
89/79:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
        if s in os.listdir(folder):
            print(os.listdir(folder+s))
89/80:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
        if s in os.listdir(folder):
            for f in files:
                if str(f) in os.listdir(folder+s):
                    print(f)
89/81:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
        if s in os.listdir(folder):
            for f in files:
                f = str(f)
                if f in os.listdir(folder+s):
                    print(f)
89/82:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
        if s in os.listdir(folder):
            for f in files:
            print(type(f))
89/83:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
        if s in os.listdir(folder):
            for f in files:
                print(type(f))
89/84:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
        if s in os.listdir(folder):
            for f in files:
                print(f)
89/85:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
        print(os.listdir(s))
89/86:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
        print(os.listdir(f+s))
89/87:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
        print(os.listdir(f+"/"+s))
89/88:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
        print(os.listdir(folder+"/"+s))
89/89:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
        yield (os.listdir(folder+"/"+s))
89/90:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         len((os.listdir(folder+"/"+s)))
89/91:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         print(len((os.listdir(folder+"/"+s))))
89/92:
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    subfolder_list = []
    for s in subfolder:
         subfolder_list.append(folder + "/" + s)
    
    for slist in subfolder_list:
        if files in os.listdir(slist):
            print(slist+files)
89/93:
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + "/" + s)
    
    for f in files:
        file_list.append(f)
        

for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            print(sub_l+"/"+fi)
89/94:
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + "/" + s)
    
    for f in files:
        file_list.append(f)
        

for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            print(sub_l+fi)
89/95:
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + "/" + s)
    
    for f in files:
        file_list.append(f)
        

for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            print(sub_l+fi)
89/96:
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + "/" + s)
    
    for f in files:
        file_list.append(f)
        

for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            print(sub_l+"."+fi)
89/97:
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + "/" + s)
    
    for f in files:
        file_list.append(f)
        

for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            print(fi)
89/98:
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + s)
    
    for f in files:
        file_list.append(f)
        

for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            print(subl+fi)
89/99:
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + s)
    
    for f in files:
        file_list.append(f)
        

for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            print(sub_l+fi)
89/100:
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + s)
    
    for f in files:
        file_list.append(f)
        

for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            print(sub_l+"/"+fi)
89/101:
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + s)
    
    for f in files:
        file_list.append(f)
        
phone_pattern = re.compile(r'(\d{3})-(\d{3})-(\d{4})')
for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            f = open((sub_l+"/"+fi),"r")
89/102:
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + s)
    
    for f in files:
        file_list.append(f)
        
phone_pattern = re.compile(r'(\d{3})-(\d{3})-(\d{4})')
for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            f = open((sub_l+"/ss"+fi),"r")
89/103:
import re
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + s)
    
    for f in files:
        file_list.append(f)
        
phone_pattern = re.compile(r'(\d{3})-(\d{3})-(\d{4})')
for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            f = open((sub_l+"/"+fi),"r")
            if re
89/104:
import re
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + s)
    
    for f in files:
        file_list.append(f)
        
phone_pattern = re.compile(r'(\d{3})-(\d{3})-(\d{4})')
for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            f = open((sub_l+"/"+fi),"r")
            if re.search(phone_pattern,f.readlines()) != "":
                print(fi)
89/105:
import re
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + s)
    
    for f in files:
        file_list.append(f)
        
phone_pattern = re.compile(r'(\d{3})-(\d{3})-(\d{4})')
for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            f = open((sub_l+"/"+fi),"r")
            if re.search(phone_pattern,f.readlines()) != "":
                print(2)
89/106:
import re
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + s)
    
    for f in files:
        file_list.append(f)
        
phone_pattern = re.compile(r'(\d{3})-(\d{3})-(\d{4})')
for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            f = open((sub_l+"/"+fi),"r")
            if re.search(phone_pattern,f.readlines()) != None:
                print(2)
89/107:
import re
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + s)
    
    for f in files:
        file_list.append(f)
        
phone_pattern = re.compile(r'(\d{3})-(\d{3})-(\d{4})')
for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            f = open((sub_l+"/"+fi),"r")
            results = re.search(phone_pattern,f.readlines())
            if results.group != "":
                print(fi)
89/108:
import re
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + s)
    
    for f in files:
        file_list.append(f)
        
phone_pattern = re.compile(r'(\d{3})-(\d{3})-(\d{4})')
for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            f = open((sub_l+"/"+fi),"r")
            results = re.search(phone_pattern,"".join(f.readlines())
            if results.group != "":
                print(fi)
89/109:
import re
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + s)
    
    for f in files:
        file_list.append(f)
        
phone_pattern = re.compile(r'(\d{3})-(\d{3})-(\d{4})')
for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            f = open((sub_l+"/"+fi),"r")
            results = re.search(phone_pattern,"".join(f.readlines()))
            if results.group != "":
                print(fi)
89/110:
import re
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + s)
    
    for f in files:
        file_list.append(f)
        
phone_pattern = re.compile(r'(\d{3})-(\d{3})-(\d{4})')
for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            f = open((sub_l+"/"+fi),"r")
            results = re.search(phone_pattern,"".join(f.readlines()))
            if results.group != NOne:
                print(fi)
89/111:
import re
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + s)
    
    for f in files:
        file_list.append(f)
        
phone_pattern = re.compile(r'(\d{3})-(\d{3})-(\d{4})')
for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            f = open((sub_l+"/"+fi),"r")
            results = re.search(phone_pattern,"".join(f.readlines()))
            if results.group != None:
                print(fi)
89/112:
import re
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + s)
    
    for f in files:
        file_list.append(f)
        
phone_pattern = re.compile(r'(\d{3})-(\d{3})-(\d{4})')
for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            f = open((sub_l+"/"+fi),"r")
            results = re.search(phone_pattern,"".join(f.readlines()))
            if type(results.group) != None:
                print(fi)
89/113:
import re
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + s)
    
    for f in files:
        file_list.append(f)
        
phone_pattern = re.compile(r'(\d{3})-(\d{3})-(\d{4})')
for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            f = open((sub_l+"/"+fi),"r")
            results = re.search(phone_pattern,"".join(f.readlines()))
            if type(results.group()) != "":
                print(fi)
89/114:
import re
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + s)
    
    for f in files:
        file_list.append(f)
        
phone_pattern = re.compile(r'(\d{3})-(\d{3})-(\d{4})')
for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            f = open((sub_l+"/"+fi),"r")
            f.readlines()
89/115:
import re
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + s)
    
    for f in files:
        file_list.append(f)
        
phone_pattern = re.compile(r'(\d{3})-(\d{3})-(\d{4})')
for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            f = open((sub_l+"/"+fi),"r")
            print(f.readlines())
89/116:
import re
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + s)
    
    for f in files:
        file_list.append(f)
        
phone_pattern = re.compile(r'(\d{3})-(\d{3})-(\d{4})')
for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            f = open((sub_l+"/"+fi),"r")
            len((f.readlines()))
89/117:
import re
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + s)
    
    for f in files:
        file_list.append(f)
        
phone_pattern = re.compile(r'(\d{3})-(\d{3})-(\d{4})')
for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            f = open((sub_l+"/"+fi),"r")
            print(len((f.readlines())))
89/118:
import re
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + s)
    
    for f in files:
        file_list.append(f)
        
phone_pattern = re.compile(r'(\d{3})-(\d{3})-(\d{4})')
for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            f = open((sub_l+"/"+fi),"r")
            str = "".join(f.readlines())
            print(str)
89/119:
import re
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + s)
    
    for f in files:
        file_list.append(f)
        
phone_pattern = re.compile(r'(\d{3})-(\d{3})-(\d{4})')
for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            f = open((sub_l+"/"+fi),"r")
            str = "".join(f.readlines())
            print(str)
            print("-------------------------------------")
89/120:
import re
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + s)
    
    for f in files:
        file_list.append(f)
        
phone_pattern = re.compile(r'(\d{3})-(\d{3})-(\d{4})')
for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            f = open((sub_l+"/"+fi),"r")
            str = "".join(f.readlines())
            results = re.search(phone_pattern,str)
            if results.group() != "":
                print(fi)
89/121:
import re
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + s)
    
    for f in files:
        file_list.append(f)
        
phone_pattern = re.compile(r'(\d{3})-(\d{3})-(\d{4})')
for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            f = open((sub_l+"/"+fi),"r")
            str = "".join(f.readlines())
            results = re.search(phone_pattern,str)
            print(results.group)
89/122:
import re
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + s)
    
    for f in files:
        file_list.append(f)
        
phone_pattern = re.compile(r'(\d{3})-(\d{3})-(\d{4})')
for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            f = open((sub_l+"/"+fi),"r")
            stri = "".join(f.readlines())
            results = re.search(phone_pattern,stri)
            print(results.group)
89/123:
import re
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + s)
    
    for f in files:
        file_list.append(f)
        
phone_pattern = re.compile(r'(\d{3})-(\d{3})-(\d{4})')
for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            f = open((sub_l+"/"+fi),"r")
            stri = "".join(f.readlines())
            results = re.search(phone_pattern,stri)
            print(results.group())
89/124:
import re
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + s)
    
    for f in files:
        file_list.append(f)
        
phone_pattern = re.compile(r'(\d{3})-(\d{3})-(\d{4})')
for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            f = open((sub_l+"/"+fi),"r")
            stri = "".join(f.readlines())
            results = re.search(phone_pattern,stri)
            results.group()
89/125:
import re
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + s)
    
    for f in files:
        file_list.append(f)
        
phone_pattern = re.compile(r'(\d{3})-(\d{3})-(\d{4})')
for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            f = open((sub_l+"/"+fi),"r")
            stri = "".join(f.readlines())
            results = re.search(phone_pattern,stri)
            print(results)
89/126:
import re
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + s)
    
    for f in files:
        file_list.append(f)
        
phone_pattern = re.compile(r'(\d{3})-(\d{3})-(\d{4})')
for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            f = open((sub_l+"/"+fi),"r")
            stri = "".join(f.readlines())
            results = re.search(phone_pattern,stri)
            if results != None:
                print(fi)
89/127:
import re
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + s)
    
    for f in files:
        file_list.append(f)
        
phone_pattern = re.compile(r'(\d{3})-(\d{3})-(\d{4})')
for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            f = open((sub_l+"/"+fi),"r")
            stri = "".join(f.readlines())
            results = re.search(phone_pattern,stri)
            if results != None:
                print(sub_l+"/"fi)
89/128:
import re
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + s)
    
    for f in files:
        file_list.append(f)
        
phone_pattern = re.compile(r'(\d{3})-(\d{3})-(\d{4})')
for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            f = open((sub_l+"/"+fi),"r")
            stri = "".join(f.readlines())
            results = re.search(phone_pattern,stri)
            if results != None:
                print(sub_l+"/"+fi)
89/129:
import re
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + s)
    
    for f in files:
        file_list.append(f)
        
phone_pattern = re.compile(r'(\d{3})-(\d{3})-(\d{4})')
for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            f = open((sub_l+"/"+fi),"r")
            stri = "".join(f.readlines())
            results = re.search(phone_pattern,stri)
            if results != None:
                print(results.group())
                print(sub_l+"/"+fi)
89/130:
import re
subfolder_list = []
file_list = []
for folder,subfolder,files in  os.walk('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/'):
    for s in subfolder:
         subfolder_list.append(folder + s)
    
    for f in files:
        file_list.append(f)
        
phone_pattern = re.compile(r'(\d{3})-(\d{3})-(\d{4})')
for sub_l in subfolder_list:
    for fi in file_list:
        if fi in os.listdir(sub_l):
            f = open((sub_l+"/"+fi),"r")
            stri = "".join(f.readlines())
            results = re.search(phone_pattern,stri)
            if results != None:
                print(results.group())
                print(fi)
89/131: f.read()
89/132: f.readlines()
89/133: f = open('/home/yasar/Python-Learning/Complete-Python-3-Bootcamp/12-Advanced Python Modules/08-Advanced-Python-Module-Exercise/unzipped_file/extracted_content/Instructions.txt','r')
89/134: f.readable()
89/135: f.readlines
89/136: f.readlines()
90/1: import requests
90/2: result = requests.get("https://example.com")
90/3: type(result)
90/4: result.text
90/5: import bs4
90/6: soup = bs4.BeautifulSoup(result.text,"lxml")
90/7: soup
90/8: soup.select('title')
90/9: soup.select('p')
90/10: soup.select('title')
90/11: soup.select('title')[0].getText()
90/12: site_paragraphs = soup.select("p")
90/13: type(site_paragraphs[0])
90/14: type(site_paragraphs)
90/15: type(site_paragraphs())
92/1: import bs4
91/1: type(site_paragraphs())
91/2: import requests
91/3: result = requests.get("https://example.com")
91/4: type(result)
91/5: result.text
91/6: import bs4
91/7: soup = bs4.BeautifulSoup(result.text,"lxml")
91/8: soup
91/9: soup.select('title')[0].getText()
91/10: site_paragraphs = soup.select("p")
91/11: type(site_paragraphs())
91/12: site_paragraphs = soup.select("p")
96/1: req =
96/2: import requests
96/3: result = requests.get("https://example.com")
96/4: type(result)
96/5: result.text
96/6: import bs4
96/7: soup = bs4.BeautifulSoup(result.text,"lxml")
96/8: soup
96/9: soup.select('title')[0].getText()
96/10: site_paragraphs = soup.select("p")
96/11: req =
96/12: req = requests.get("https://en.wikipedia.org/wiki/Grace_Hopper")
96/13: print(req.text)
96/14: soup = (req,"lxml)
96/15: soup = (req,"lxml")
96/16: print(soup.prettify())
96/17: soup = (req,"html.parser")
96/18: print(soup)
96/19: print(soup.text)
96/20: print(req.text)
96/21: print(req.text)
96/22: soup = bs4.BeautifulSoup(res.text,"lxml")
96/23: soup = bs4.BeautifulSoup(req.text,"lxml")
96/24: soup
96/25: soup.select(".mw-headline")
96/26: for ss in soup.select(".mv-headline"):
96/27:
for ss in soup.select(".mv-headline"):
    print(ss.text)
96/28:
for ss in soup.select(".mv-headline"):
    print(ss)
96/29:
for ss in soup.select(".mv-headline"):
    print(ss)
96/30: soup.select(".mw-headline")
96/31: type(soup.select(".mv-headline")
96/32: type(soup.select(".mv-headline"))
96/33: soup = bs4.BeautifulSoup(req.text,"lxml")
96/34:
for element in soup.select(".mw-headline"):
    print(element.text)
96/35: print(soup.select(".mw-headline")
96/36: print(soup.select(".mw-headline")[0])
96/37: res = requests.get("https://tr.wikipedia.org/wiki/Deep_Blue")
96/38: print(res)
96/39: soup = bs4.BeautifulSoup(res.text,"html.parser")
96/40: soup
96/41: soup.select("img")
96/42: soup.select("img > src")
96/43: soup.select("src")
96/44: soup.select("img")
96/45: soup.select(".mw-file-element")
96/46: computer = soup.select(".mw-file-element")[0]
96/47: computer
96/48: type(computer)
96/49: computer["src"]
96/50: computer["class"]
96/51: computer["src"]
96/52: image_link = requests.get('//upload.wikimedia.org/wikipedia/commons/thumb/b/be/Deep_Blue.jpg/180px-Deep_Blue.jpg')
96/53: image_link = requests.get('upload.wikimedia.org/wikipedia/commons/thumb/b/be/Deep_Blue.jpg/180px-Deep_Blue.jpg')
96/54: image_link = requests.get('upload.wikimedia.org/wikipedia/commons/thumb/b/be/Deep_Blue.jpg/180px-Deep_Blue.jpg')
96/55: image_link = requests.get('https://upload.wikimedia.org/wikipedia/commons/thumb/b/be/Deep_Blue.jpg/180px-Deep_Blue.jpg')
96/56: image_link.content
96/57: f = open('my_computer_image.jpg','wb')
96/58: f.write(image_link.content)
96/59: f.close()
96/60: f = open('~/my_computer_image.jpg','wb')
96/61: f = open('/home/yasar/my_computer_image.jpg','wb')
96/62: f.write(image_link.content)
96/63: f.close()
96/64: res = requests.get("https://books.toscrape.com/")
96/65: bts_soup = bs4.BeautifulSoup(res.text, "html.parser")
96/66: pages = range(50)
96/67:

bts_soup.select("p > i")
96/68:

type(bts_soup.select("p > i"))
96/69:

(bts_soup.select("p > i"))
96/70:

 star = (bts_soup.select("p > i") && (bts_soup.select(".icon-star")
96/71:

 star = (bts_soup.select("p > i") and(bts_soup.select(".icon-star")
96/72:

 star = (bts_soup.select("p > i") and(bts_soup.select(".icon-star"))
96/73:

 star = (bts_soup.select("p > i")) and(bts_soup.select(".icon-star"))
96/74: print(star)
96/75:

 star = (bts_soup.select("p class = 'star-rating Three' > i")) and (bts_soup.select(".icon-star"))
96/76:

 star = (bts_soup.select("p")) and (bts_soup.select("star-rating Three"))
96/77: # GOAL: Get title of every book with a 2 star
96/78: page_syntax = "https://books.toscrape.com/catalogue/page-*.html"
96/79:

for i in range(2,51):
    r = requests.get(page_syntax.replace("*",str(i)))
    site_soup = bs4.BeautifulSoup(r.text, "html.parser")
    print(site_soup.select("a"))
96/80: r = requests.get(page_syntax.replace("*",str(1)))
96/81: soup = bs4.BeautifulSoup((r.text,'lxml'))
96/82: r = requests.get(page_syntax)
96/83: soup = bs4.BeautifulSoup((r.text,'lxml'))
96/84: r = requests.get(page_syntax)
96/85: soup = bs4.BeautifulSoup((r.text,'lxml'))
96/86:
import bs4
soup = bs4.BeautifulSoup((r.text,'lxml'))
96/87: soup = bs4.BeautifulSoup(r.text,'lxml')
96/88: soup
96/89: soup = bs4.BeautifulSoup(r.text,'html.parser')
96/90: soup
96/91: print(soup)
96/92: r = requests.get(page_syntax)
96/93: soup = bs4.BeautifulSoup(r.text,'html.parser')
96/94: print(soup)
96/95: page_syntax = "https://books.toscrape.com/catalogue/page-1.html"
96/96:

for i in range(2,51):
    r = requests.get(page_syntax.replace("*",str(i)))
    site_soup = bs4.BeautifulSoup(r.text, "html.parser")
    print(site_soup.select("a"))
96/97: r = requests.get(page_syntax)
96/98: soup = bs4.BeautifulSoup(r.text,'html.parser')
96/99: print(soup)
96/100: len(soup)
96/101: len(soup)
96/102: soup = bs4.BeautifulSoup(r.text,'html.parser')
96/103: len(soup)
96/104: r = requests.get(page_syntax)
96/105: soup = bs4.BeautifulSoup(r.text,'html.parser')
96/106: len(soup)
96/107: print(soup)
96/108: soup = bs4.BeautifulSoup(r.text,'html.parser')
96/109: page_syntax = "https://books.toscrape.com/catalogue/page-1.html"
96/110: r = requests.get(page_syntax)
96/111: soup = bs4.BeautifulSoup(r.text,'html.parser')
96/112: soup
96/113: len(soup)
96/114: soup = bs4.BeautifulSoup(r.text,'lxml')
96/115: len(soup)
96/116: len(soup)
96/117: soup.select(".product_pod")
96/118: len(soup.select(".product_pod"))
96/119:
stars = ["One","Two","Three","Four","Five"]
for page in range(1,51):
    r = requests.get(page_syntax.replace("*",str(i)))
    soup = bs4.BeautifulSoup(r.text,'lxml')
    for book in range(len(soup.select(".product-pod"))):
        print(book)
96/120:
stars = ["One","Two","Three","Four","Five"]
for page in range(1,51):
    r = requests.get(page_syntax.replace("*",str(i)))
    soup = bs4.BeautifulSoup(r.text,'lxml')
    for book in range(len(soup.select(".product-pod"))):
        print(book.select(".star-rating.Three")
96/121:
stars = ["One","Two","Three","Four","Five"]
for page in range(1,51):
    r = requests.get(page_syntax.replace("*",str(i)))
    soup = bs4.BeautifulSoup(r.text,'lxml')
    for book in range(len(soup.select(".product-pod"))):
        print(book.select(".star-rating.Three"))
96/122:
stars = ["One","Two","Three","Four","Five"]
for page in range(1,51):
    r = requests.get(page_syntax.replace("*",str(i)))
    soup = bs4.BeautifulSoup(r.text,'lxml')
    for book in range(len(soup.select(".product-pod"))):
        print(1)
96/123: page_syntax = "https://books.toscrape.com/catalogue/page-*.html"
96/124:
stars = ["One","Two","Three","Four","Five"]
for page in range(1,51):
    r = requests.get(page_syntax.replace("*",str(i)))
    soup = bs4.BeautifulSoup(r.text,'lxml')
    for book in range(len(soup.select(".product-pod"))):
        print(1)
96/125:
'''
stars = ["One","Two","Three","Four","Five"]
for page in range(1,51):
    r = requests.get(page_syntax.replace("*",str(i)))
    soup = bs4.BeautifulSoup(r.text,'lxml')
    for book in range(len(soup.select(".product-pod"))):
        print(1)
'''
96/126: page_syntax = "https://books.toscrape.com/catalogue/page-*.html"
96/127: res = requests.get(page_syntax.replace("*","1")
96/128: res = requests.get(page_syntax.replace("*","1"))
96/129: soup = bs4.BeautifulSoup(res.text,'lxml')
96/130: products = soup.select(".product_pod")
96/131: print(products)
96/132: len(products)
96/133: product[0]
96/134: products[0]
96/135: example = products[0]
96/136: example.select(".star-rating Three")
96/137: example.select(".star-rating.Three")
96/138: example.select(".star-rating.Two")
96/139: [] == example.select(".star-rating.Two")
96/140: [] == example.select(".star-rating.Three")
96/141: example.select(".star-rating.Three")
96/142:
for book in products:
    if book.select(".star-rating Three")
96/143:
for book in products:
    if book.select(".star-rating Three") == []:
        continue
    print(book)
96/144:
for book in products:
    if book.select(".star-rating.Three") == []:
        continue
    print(book)
96/145:
for book in products:
    if book.select(".star-rating.Three").select('a') == []:
96/146:
for book in products:
    if book.select(".star-rating.Three").select('a') == []:
        continue
    print(book)
96/147: example.select()
96/148: example.select('a')
96/149: example
96/150: example.select('a')[1]
96/151: type(example.select('a')[1])
96/152: type(example.select('a')[1]['title'])
96/153: (example.select('a')[1]['title'])
96/154:
#for i in range(20):
#    r = requests.get("https://books.toscrape.com/catalogue/page-1.html")
soup = bs4.BeautifulSoup(r.text, "lxml")
len(soup)
96/155:


r = requests.get("https://books.toscrape.com/catalogue/page-1.html")
soup = bs4.BeautifulSoup(r.text, "lxml")
stars = soup.select(".product_pod")
print(len(stars))
96/156:


r = requests.get("https://books.toscrape.com/catalogue/page-1.html")
soup = bs4.BeautifulSoup(r.text, "lxml")
stars = soup.select(".product_pod")
print(stars['title']
96/157:


r = requests.get("https://books.toscrape.com/catalogue/page-1.html")
soup = bs4.BeautifulSoup(r.text, "lxml")
stars = soup.select(".product_pod")
print(stars['title'])
96/158:


r = requests.get("https://books.toscrape.com/catalogue/page-1.html")
soup = bs4.BeautifulSoup(r.text, "lxml")
stars = soup.select(".product_pod")

type(stars)
96/159:


r = requests.get("https://books.toscrape.com/catalogue/page-1.html")
soup = bs4.BeautifulSoup(r.text, "lxml")
stars = soup.select(".product_pod")

print(stars)
96/160:


r = requests.get("https://books.toscrape.com/catalogue/page-1.html")
soup = bs4.BeautifulSoup(r.text, "lxml")
stars = soup.select("a")

for book in stars:
    print(book['href'])
96/161:


r = requests.get("https://books.toscrape.com/catalogue/page-1.html")
soup = bs4.BeautifulSoup(r.text, "lxml")
stars = soup.select("a")

for book in stars:
    print(book['title'])
96/162:


r = requests.get("https://books.toscrape.com/catalogue/page-1.html")
soup = bs4.BeautifulSoup(r.text, "lxml")
stars = soup.select("a")

for book in stars:
    print(book['title'])
96/163:


r = requests.get("https://books.toscrape.com/catalogue/page-1.html")
soup = bs4.BeautifulSoup(r.text, "lxml")
stars = soup.select("a")

for book in stars:
    print(book)
96/164:


r = requests.get("https://books.toscrape.com/catalogue/page-1.html")
soup = bs4.BeautifulSoup(r.text, "lxml")
stars = soup.select(".product_pod")

for book in stars:
    print(book.select('a')
96/165:


r = requests.get("https://books.toscrape.com/catalogue/page-1.html")
soup = bs4.BeautifulSoup(r.text, "lxml")
stars = soup.select(".product_pod")

for book in stars:
    print(book.select('a'))
96/166:


r = requests.get("https://books.toscrape.com/catalogue/page-1.html")
soup = bs4.BeautifulSoup(r.text, "lxml")
stars = soup.select(".product_pod")

for book in stars:
    print(book.select('a')['title'])
96/167:


r = requests.get("https://books.toscrape.com/catalogue/page-1.html")
soup = bs4.BeautifulSoup(r.text, "lxml")
stars = soup.select(".product_pod")

for book in stars:
    print(book['title']
96/168:


r = requests.get("https://books.toscrape.com/catalogue/page-1.html")
soup = bs4.BeautifulSoup(r.text, "lxml")
stars = soup.select(".product_pod")

for book in stars:
    print(book['title'])
96/169:


r = requests.get("https://books.toscrape.com/catalogue/page-1.html")
soup = bs4.BeautifulSoup(r.text, "lxml")
stars = soup.select(".product_pod")

for book in stars:
   print(book)
96/170:


r = requests.get("https://books.toscrape.com/catalogue/page-1.html")
soup = bs4.BeautifulSoup(r.text, "lxml")
stars = soup.select(".product_pod")

for book in stars:
    a_tag = stars.select('a')
    print(a_tag[1]['class'])
96/171:


r = requests.get("https://books.toscrape.com/catalogue/page-1.html")
soup = bs4.BeautifulSoup(r.text, "lxml")
stars = soup.select(".product_pod")

for book in stars:
    a_tag = stars.select('a')
    print(a_tag[1]['href'])
96/172:


r = requests.get("https://books.toscrape.com/catalogue/page-1.html")
soup = bs4.BeautifulSoup(r.text, "lxml")
stars = soup.select(".product_pod")

for book in stars:
    a_tag = stars.select('a')
    print(a_tag[1]['src'])
96/173:


r = requests.get("https://books.toscrape.com/catalogue/page-1.html")
soup = bs4.BeautifulSoup(r.text, "lxml")
stars = soup.select(".product_pod")

for book in stars:
    a_tag = stars.select('a')
    print(a_tag[1]['title'])
96/174:


r = requests.get("https://books.toscrape.com/catalogue/page-1.html")
soup = bs4.BeautifulSoup(r.text, "lxml")
stars = soup.select(".product_pod")

for book in stars:
    a_tag = stars.select('a')
    print(a_tag)
96/175:


r = requests.get("https://books.toscrape.com/catalogue/page-1.html")
soup = bs4.BeautifulSoup(r.text, "lxml")
stars = soup.select(".product_pod")

for book in stars:
    print(book)
    print("*********************************************************************")
96/176:


r = requests.get("https://books.toscrape.com/catalogue/page-1.html")
soup = bs4.BeautifulSoup(r.text, "lxml")
stars = soup.select(".product_pod")

for book in stars:
    print(book.select('a')
    print("*********************************************************************")
96/177:


r = requests.get("https://books.toscrape.com/catalogue/page-1.html")
soup = bs4.BeautifulSoup(r.text, "lxml")
stars = soup.select(".product_pod")

for book in stars:
    print(book.select('a'))
    print("*********************************************************************")
96/178:


r = requests.get("https://books.toscrape.com/catalogue/page-1.html")
soup = bs4.BeautifulSoup(r.text, "lxml")
stars = soup.select(".product_pod")

for book in stars:
    print(book.select('a')[1])
96/179:


r = requests.get("https://books.toscrape.com/catalogue/page-1.html")
soup = bs4.BeautifulSoup(r.text, "lxml")
stars = soup.select(".product_pod")

for book in stars:
    print(book.select('a')[1]['title'])
96/180:


r = requests.get("https://books.toscrape.com/catalogue/page-1.html")
soup = bs4.BeautifulSoup(r.text, "lxml")
stars = soup.select(".product_pod")

for book in stars:
    print(book.select('article'))
96/181:


r = requests.get("https://books.toscrape.com/catalogue/page-1.html")
soup = bs4.BeautifulSoup(r.text, "lxml")
stars = soup.select(".product_pod")

for book in stars:
    print(book.select('div'))
96/182:


r = requests.get("https://books.toscrape.com/catalogue/page-1.html")
soup = bs4.BeautifulSoup(r.text, "lxml")
stars = soup.select(".product_pod")

for book in stars:
    print(book.select('div > a'))
96/183:


r = requests.get("https://books.toscrape.com/catalogue/page-1.html")
soup = bs4.BeautifulSoup(r.text, "lxml")
stars = soup.select(".product_pod")

for book in stars:
    print(book.select('div > a')[1])
96/184:


r = requests.get("https://books.toscrape.com/catalogue/page-1.html")
soup = bs4.BeautifulSoup(r.text, "lxml")
stars = soup.select(".product_pod")

for book in stars:
    print(book.select('div > a')[0])
96/185:


r = requests.get("https://books.toscrape.com/catalogue/page-1.html")
soup = bs4.BeautifulSoup(r.text, "lxml")
stars = soup.select(".product_pod")

for book in stars:
    print(book.select('div > a')[0]['alt'])
96/186:


r = requests.get("https://books.toscrape.com/catalogue/page-1.html")
soup = bs4.BeautifulSoup(r.text, "lxml")
stars = soup.select(".product_pod")

for book in stars:
    print(book.select('a')[0]['alt'])
96/187:


r = requests.get("https://books.toscrape.com/catalogue/page-1.html")
soup = bs4.BeautifulSoup(r.text, "lxml")
stars = soup.select(".product_pod")

for book in stars:
    print(book.select('a')[1]['src'])
96/188:


r = requests.get("https://books.toscrape.com/catalogue/page-1.html")
soup = bs4.BeautifulSoup(r.text, "lxml")
stars = soup.select(".product_pod")

for book in stars:
    print(book.select('a')[1]['title'])
96/189:


r = requests.get("https://books.toscrape.com/catalogue/page-1.html")
soup = bs4.BeautifulSoup(r.text, "lxml")
stars = soup.select(".product_pod")

for book in stars:
    print(book.select('a')[1]['href'])
96/190:
for page_number in range(1,51):
    r = requests.get("https://books.toscrape.com/catalogue/page-{}.html".format(page_number))
    soup = bs4.BeautifulSoup(r.text, "lxml")
    stars = soup.select(".product_pod")
    
    for book in stars:
        print(book.select('a')[1]['href'])
96/191:
for page_number in range(1,51):
    r = requests.get("https://books.toscrape.com/catalogue/page-{}.html".format(page_number))
    soup = bs4.BeautifulSoup(r.text, "lxml")
    stars = soup.select(".product_pod")
    
    for book in stars:
        print(book.select('a')[1]['title'])
96/192:
i = 0
for page_number in range(1,51):
    r = requests.get("https://books.toscrape.com/catalogue/page-{}.html".format(page_number))
    soup = bs4.BeautifulSoup(r.text, "lxml")
    stars = soup.select(".product_pod")
 
    for book in stars:
        print(book.select('a')[1]['title'])
        i+=1
print(i)
96/193:
i = 0
for page_number in range(1,51):
    r = requests.get("https://books.toscrape.com/catalogue/page-{}.html".format(page_number))
    soup = bs4.BeautifulSoup(r.text, "lxml")
    stars = soup.select(".product_pod")
 
    for book in stars:
        print(book.select('.star-rating.Three')[1]['title'])
        i+=1
print(i)
96/194:
i = 0
for page_number in range(1,51):
    r = requests.get("https://books.toscrape.com/catalogue/page-{}.html".format(page_number))
    soup = bs4.BeautifulSoup(r.text, "lxml")
    stars = soup.select(".product_pod")
 
    for book in stars:
        print(book.select('img'))
        i+=1
print(i)
96/195:
i = 0
for page_number in range(1,51):
    r = requests.get("https://books.toscrape.com/catalogue/page-{}.html".format(page_number))
    soup = bs4.BeautifulSoup(r.text, "lxml")
    stars = soup.select(".product_pod")
 
    for book in stars:
        print(book.select('img')[0]['alt'])
        i+=1
print(i)
96/196:
i = 0
for page_number in range(1,51):
    r = requests.get("https://books.toscrape.com/catalogue/page-{}.html".format(page_number))
    soup = bs4.BeautifulSoup(r.text, "lxml")
    stars = soup.select(".product_pod")
 
    for book in stars:
        print(book.select('a > img')[0]['alt'])
        i+=1
print(i)
96/197:
i = 0
for page_number in range(1,51):
    r = requests.get("https://books.toscrape.com/catalogue/page-{}.html".format(page_number))
    soup = bs4.BeautifulSoup(r.text, "lxml")
    stars = soup.select(".product_pod")
 
    for book in stars:
        print(book.select('a > img')[0]['class'])
        i+=1
print(i)
96/198:
i = 0
for page_number in range(1,51):
    r = requests.get("https://books.toscrape.com/catalogue/page-{}.html".format(page_number))
    soup = bs4.BeautifulSoup(r.text, "lxml")
    stars = soup.select(".product_pod")
 
    for book in stars:
        print(book.select('a')[1]['title'])
        i+=1
print(i)
96/199:
for page_number in range(1,51):
    r = requests.get("https://books.toscrape.com/catalogue/page-{}.html".format(page_number))
    soup = bs4.BeautifulSoup(r.text, "lxml")
    stars = soup.select(".product_pod")
 
    for book in stars:
        if book.select(".star-rating.Three") != []:
            print(book.select("a")[1]['title'])
96/200:
number_of_non_star = 0
number_of_one_star = 0
number_of_two_stars = 0
number_of_three_stars = 0
number_of_four_stars = 0
number_of_five_stars = 0
for page_number in range(1,51):
    r = requests.get("https://books.toscrape.com/catalogue/page-{}.html".format(page_number))
    soup = bs4.BeautifulSoup(r.text, "lxml")
    stars = soup.select(".product_pod")
 
    for book in stars:
        if book.select(".star-rating.Three") != []:
            #print(book.select("a")[1]['title'])
            number_of_three_stars +=1
        elif book.select(".star-rating.Four") != []:   
            number_of_four_stars+=1
        elif book.select(".star-rating.Five") != []:
            number_of_five_stars+=1
        elif book.select(".star-rating.One") != []:
            number_of_one_star+=1
        elif book.select(".star-rating.Two") != []:
            number_of_two_stars+=1
        else:
            number_of_non_star+=1
print(number_of_non_star+number_of_two_stars+number_of_three_stars+number_of_four_stars+number_of_five_stars)
97/1: import requests
97/2: result = requests.get("https://example.com")
97/3: type(result)
97/4: result.text
97/5: import bs4
97/6: soup = bs4.BeautifulSoup(result.text,"lxml")
97/7: soup
97/8: soup.select('title')[0].getText()
97/9: site_paragraphs = soup.select("p")
97/10: req = requests.get("https://en.wikipedia.org/wiki/Grace_Hopper")
97/11: soup = bs4.BeautifulSoup(req.text,"lxml")
97/12: soup.select(".mw-headline")
97/13:
for element in soup.select(".mw-headline"):
    print(element.text)
97/14: print(soup.select(".mw-headline")[0])
97/15: res = requests.get("https://tr.wikipedia.org/wiki/Deep_Blue")
97/16: print(res)
97/17: soup = bs4.BeautifulSoup(res.text,"html.parser")
97/18: soup
97/19: soup.select(".mw-file-element")
97/20: computer = soup.select(".mw-file-element")[0]
97/21: computer
97/22: type(computer)
97/23: computer["src"]
97/24: image_link = requests.get('https://upload.wikimedia.org/wikipedia/commons/thumb/b/be/Deep_Blue.jpg/180px-Deep_Blue.jpg')
97/25: image_link.content
97/26: f = open('/home/yasar/my_computer_image.jpg','wb')
97/27: f.write(image_link.content)
97/28: f.close()
97/29: res = requests.get("https://books.toscrape.com/")
97/30: pages = range(50)
97/31: star_rate = ["One","Two","Three","Four","Five"]
97/32:

 star = (bts_soup.select("p")) and (bts_soup.select("star-rating Two"))
97/33:
number_of_three_stars = 0
for page_number in range(1,51):
    r = requests.get("https://books.toscrape.com/catalogue/page-{}.html".format(page_number))
    soup = bs4.BeautifulSoup(r.text, "lxml")
    stars = soup.select(".product_pod")
 
    for book in stars:
        if book.select(".star-rating.Three") != []:
            print(book.select("a")[1]['title'])
97/34:
number_of_three_stars = 0
for page_number in range(1,51):
    r = requests.get("https://books.toscrape.com/catalogue/page-{}.html".format(page_number))
    soup = bs4.BeautifulSoup(r.text, "lxml")
    stars = soup.select(".product_pod")
 
    for book in stars:
        if book.select(".star-rating.Two") != []:
            print(book.select("a")[1]['title'])
98/1: import bs4
98/2: import requests
98/3: req = requests.get('http://quotes.toscrape.com/')
98/4: soup = bs4.BeautifulSoup(req.text,'lxml')
98/5: soup
98/6: div = soup.select('.author')
98/7: div
98/8: div.text
98/9: div
98/10: type(div)
98/11: div
98/12: div[0]
98/13: div[0][0]
98/14: type(div[0])
98/15: div.div
98/16: div.a
98/17: div = soup.select('.col-md-8')
98/18: print(div)
98/19: div.select(".author")
98/20: type(div)
98/21: type(div[0])
98/22: div[0]
98/23: div = soup.select('.col-md-8 a')
98/24: div[0]
98/25: div = soup.select('.col-md-8 .author')
98/26: div
98/27: div = soup.select('.col-md-8 .author').text
98/28: div = soup.select('.col-md-8 .author')
98/29: div[0]
98/30: div[0]['itemprop']
98/31: div[0]
98/32: div
98/33: div[21::-7]
98/34: div[0][21::-7]
98/35: div
98/36:
for author in div:
    auth = str(div).replace('<small class="author" itemprop="author">',"").replace("</small>","")
    print(auth)
98/37:
auth_set = set()
for author in div:
    auth = str(div).replace('<small class="author" itemprop="author">',"").replace("</small>","")
    auth_set.add(auth)
print(auth_set)
98/38: quotes = soup.select(".quote .text")
98/39: quotes
98/40: quotes.text
98/41: quotes
98/42: len(quotes)
98/43: soup = bs4.BeautifulSoup(req.a,'lxml')
98/44: soup.p
98/45: soup.p.text
98/46: quotes = soup.select(".quote .text").text
98/47: quotes = soup.text
98/48: quotes = soup.get_text()
98/49: quotes = soup.get_text()
98/50: quotes = soup.get_text()
98/51: quotes
98/52: quotes = soup.small
98/53: quotes = soup.small
98/54: quotes
98/55: quotes = soup.small.text
98/56: quotes
98/57: quotes = soup.small.text
98/58: quotes
98/59: quotes
98/60: quotes
98/61:
for i in soup.find_all('small'):
    print(i)
98/62:
for i in soup.find_all('small'):
    print(i.text)
98/63:
auth_set = set()
for i in soup.find_all('small'):
    set.add(i.text)
98/64:
auth_set = set()
for i in soup.find_all('small'):
    auth_set.add(i.text)
98/65: auth_set
98/66:
auth = soup.select(".quote")
auth
98/67:
auth = soup.select(".quote .text")

authors = auth.find_all(
98/68: auth = soup.select(".quote .text")
98/69:
auth = soup.select(".quote .text")
auth
98/70:
auth = soup.select(".quote .text")
auth.text
98/71: auth = soup.find_all(class_=".quote")
98/72:
auth = soup.find_all(class_=".quote")
auth
98/73:
auth = soup.select(".quote .text")
auth
98/74:
auth = soup.select(".quote .text")
auth.span.text
98/75:
auth = soup.select(".quote .text")
auth.span
98/76:
auth = soup.select(".quote .text")
auth.s
98/77:
auth = soup.select(".quote .text")
auth.a
98/78:
auth = soup.select(".quote .text")
type(auth)
98/79:
auth = soup.select(".quote .text")
auth[0]
98/80:
auth = span
auth[0]
98/81:
auth = soup.span
auth[0]
98/82: auth = soup.span
98/83:
auth = soup.span
auth
98/84:
auth = soup.span.text
auth
98/85:
auth = soup.find_all('span')
auth
98/86:
auth = soup.find_all('span')
auth.text
98/87:
for quotes in soup.find_all('span'):
    print(quotes.text)
98/88: type(soup.find_all("span"))
98/89:
for quotes in soup.find_all('span', class_=text):
    quotes.text
98/90:
for quotes in soup.find_all('span', class_="text"):
    quotes.text
98/91:
for quotes in soup.find_all('span', class_="text"):
   print(quotes.text)
98/92: print(soup.select(".tag-item"))
98/93: print(soup.find_all("span",class_="tag-item"))
98/94: print(soup.find_all("a",class_="tag"))
98/95: print(soup.find("a",class_="tag"))
98/96: soup.find("a",class_="tag").text
98/97: soup.find("h2",class_="tag").text
98/98: soup.select("h2")
98/99: soup.select("div span")
98/100:
a = soup.select("div span .tag-item")
a.select(".tah
98/101: a = soup.select("div span .tag-item")
98/102:
a = soup.select("div span .tag-item")
a
98/103: a = soup.select("div span a")
98/104:
a = soup.select("div span a")
a
98/105: a = soup.select("div span a .tag")
98/106:
a = soup.select("div span a .tag")
a
98/107: a = soup.select("div span a")
98/108:
a = soup.select("div span a")
a.text
98/109:
a = soup.select("div span a")
a
98/110:
a = soup.select("div span a")
a.a
98/111:
a = soup.select("div span a")
a
98/112:
a = soup.select("div span a")
b = a.select(".tag")
98/113:
a = soup.find_all("div span a")
b = a.select(".tag")
98/114: a = soup.find_all("div span a")
98/115:
page_base = "http://quotes.toscrape.com/page/{}/"

length = 20

for i in range(1,length+1):
    r = requests.get(page_base.format(i)
    soup = bs4.BeautifulSoup(r.text,"lxml")
    print(soup.select("div col-md-8")
98/116:
page_base = "http://quotes.toscrape.com/page/{}/"

length = 20

for i in range(1,length+1):
    r = requests.get(page_base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    print(soup.select("div col-md-8")
98/117:
page_base = "http://quotes.toscrape.com/page/{}/"

length = 20

for i in range(1,length+1):
    r = requests.get(page_base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    print(soup.select("div col-md-8"))
98/118:
page_base = "http://quotes.toscrape.com/page/{}/"

length = 20

for i in range(1,length+1):
    r = requests.get(page_base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    print(soup.select("div .col-md-8"))
98/119:
page_base = "http://quotes.toscrape.com/page/{}/"

length = 20

for i in range(1,length+1):
    r = requests.get(page_base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    print(f"THIS IS PAGE {i}!!!!!!!!!!!!')
    print(soup.select("div .col-md-8"))
98/120:
page_base = "http://quotes.toscrape.com/page/{}/"

length = 20

for i in range(1,length+1):
    r = requests.get(page_base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    print(f"THIS IS PAGE {i}!!!!!!!!!!!!")
    print(soup.select("div .col-md-8"))
98/121:
page_base = "http://quotes.toscrape.com/page/{}/"

length = 20

for i in range(1,length+1):
    r = requests.get(page_base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    if soup.select("div .col-md-8") !=[]
        print(f"THIS IS PAGE {i}!!!!!!!!!!!!")
        print(soup.select("div .col-md-8"))
    else:
        print(f" FROM  PAGE {i} IS EMPTY!!!!!!!!!!!!!!!")
        break
98/122:
page_base = "http://quotes.toscrape.com/page/{}/"

length = 20

for i in range(1,length+1):
    r = requests.get(page_base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    if soup.select("div .col-md-8") !=[]:
        print(f"THIS IS PAGE {i}!!!!!!!!!!!!")
        print(soup.select("div .col-md-8"))
    else:
        print(f" FROM  PAGE {i} IS EMPTY!!!!!!!!!!!!!!!")
        break
98/123:
page_base = "http://quotes.toscrape.com/page/{}/"
i = 1
r = requests.get(page_base.format(i))
soup = bs4.BeautifulSoup(r.text,"lxml")
while (soup.select("div .col-md-8") != [])
    print(f"THIS IS PAGE {i}!!!!!!!!!!!!")
    i+=1        
    #print(soup.select("div .col-md-8"))
else:
    print("From page {} is invalid".format(i)}
98/124:
page_base = "http://quotes.toscrape.com/page/{}/"
i = 1
r = requests.get(page_base.format(i))
soup = bs4.BeautifulSoup(r.text,"lxml")
while (soup.select("div .col-md-8") != [])
    print(f"THIS IS PAGE {i}!!!!!!!!!!!!")
    i+=1        
    #print(soup.select("div .col-md-8"))
else:
    print("From page {} is invalid".format(i))
98/125:
page_base = "http://quotes.toscrape.com/page/{}/"
i = 1
r = requests.get(page_base.format(i))
soup = bs4.BeautifulSoup(r.text,"lxml")
while (soup.select("div .col-md-8") != []):
    print(f"THIS IS PAGE {i}!!!!!!!!!!!!")
    i+=1        
    #print(soup.select("div .col-md-8"))
else:
    print("From page {} is invalid".format(i))
98/126:
page_base = "http://quotes.toscrape.com/page/{}/"
i = 1
r = requests.get(page_base.format(i))
soup = bs4.BeautifulSoup(r.text,"lxml")
while (soup.select("div .col-md-8") != []):
    print(f"THIS IS PAGE {i}!!!!!!!!!!!!")
    i+=1
    r = requests.get(page_base.format(i))
    #print(soup.select("div .col-md-8"))
else:
    print("From page {} is invalid".format(i))
98/127:
page_base = "http://quotes.toscrape.com/page/{}/"
i = 11
r = requests.get(page_base.format(i))
soup = bs4.BeautifulSoup(r.text,"lxml")
print(soup.select("div .col-md-8")
98/128:
page_base = "http://quotes.toscrape.com/page/{}/"
i = 11
r = requests.get(page_base.format(i))
soup = bs4.BeautifulSoup(r.text,"lxml")
print(soup.select("div .col-md-8"))
98/129:
page_base = "http://quotes.toscrape.com/page/{}/"
i = 11
r = requests.get(page_base.format(i))
soup = bs4.BeautifulSoup(r.text,"lxml")
print(soup.find_all("div",class_="col-md-8"))
98/130:
page_base = "http://quotes.toscrape.com/page/{}/"
i = 11
r = requests.get(page_base.format(i))
soup = bs4.BeautifulSoup(r.text,"lxml")
len(soup.find_all("div",class_="col-md-8"))
98/131:
page_base = "http://quotes.toscrape.com/page/{}/"
i = 1
r = requests.get(page_base.format(i))
soup = bs4.BeautifulSoup(r.text,"lxml")
len(soup.find_all("div",class_="col-md-8"))
98/132:
page_base = "http://quotes.toscrape.com/page/{}/"
i = 1
r = requests.get(page_base.format(i))
soup = bs4.BeautifulSoup(r.text,"lxml")
print(soup.find_all("div",class_="col-md-8"))
98/133:
page_base = "http://quotes.toscrape.com/page/{}/"
i = 1
r = requests.get(page_base.format(i))
soup = bs4.BeautifulSoup(r.text,"lxml")
print(soup.find_all("div",class_="col-md-8")[1])
98/134:
page_base = "http://quotes.toscrape.com/page/{}/"
i = 1
r = requests.get(page_base.format(i))
soup = bs4.BeautifulSoup(r.text,"lxml")
print(soup.find_all("div",class_="col-md-8")[0])
98/135:
page_base = "http://quotes.toscrape.com/page/{}/"
i = 11
r = requests.get(page_base.format(i))
soup = bs4.BeautifulSoup(r.text,"lxml")
print(soup.find_all("div",class_="col-md-8")[0])
98/136:
page_base = "http://quotes.toscrape.com/page/{}/"
i = 11
r = requests.get(page_base.format(i))
soup = bs4.BeautifulSoup(r.text,"lxml")
print("No quotes" in str(soup.find_all("div",class_="col-md-8")[1]))
98/137:
page_base = "http://quotes.toscrape.com/page/{}/"
i = 1
r = requests.get(page_base.format(i))
soup = bs4.BeautifulSoup(r.text,"lxml")
print("No quotes" in str(soup.find_all("div",class_="col-md-8")[1]))
98/138:
page_base = "http://quotes.toscrape.com/page/{}/"
i = 1
r = requests.get(page_base.format(i))
soup = bs4.BeautifulSoup(r.text,"lxml")
while "No quotes" not in str(soup.find_all("div",class_="col-md-8")[1])):
    print(f"THIS IS PAGE {i}!!!!!!!!!!!!")
    i+=1
    r = requests.get(page_base.format(i))
    #print(soup.select("div .col-md-8"))
98/139:
page_base = "http://quotes.toscrape.com/page/{}/"
i = 1
r = requests.get(page_base.format(i))
soup = bs4.BeautifulSoup(r.text,"lxml")
while "No quotes" not in str(soup.find_all("div",class_="col-md-8")[1]):
    print(f"THIS IS PAGE {i}!!!!!!!!!!!!")
    i+=1
    r = requests.get(page_base.format(i))
    #print(soup.select("div .col-md-8"))
98/140:
page_base = "http://quotes.toscrape.com/page/{}/"
i = 1
r = requests.get(page_base.format(i))
while "No quotes" in str(soup.find_all("div",class_="col-md-8")[1]):
    print(f"THIS IS PAGE {i}!!!!!!!!!!!!")
    i+=1
    r = requests.get(page_base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    #print(soup.select("div .col-md-8"))
98/141:
page_base = "http://quotes.toscrape.com/page/{}/"
i = 1
r = requests.get(page_base.format(i))
while "No quotes" not in str(soup.find_all("div",class_="col-md-8")[1]):
    print(f"THIS IS PAGE {i}!!!!!!!!!!!!")
    i+=1
    r = requests.get(page_base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    #print(soup.select("div .col-md-8"))
98/142:
page_base = "http://quotes.toscrape.com/page/{}/"
i = 1
r = requests.get(page_base.format(i))
while "No quotes" not in str(soup.find_all("div",class_="col-md-8")[1]):
    print(f"THIS IS PAGE {i}!!!!!!!!!!!!")
    i+=1
    r = requests.get(page_base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    #print(soup.select("div .col-md-8"))
print(f"AFTER PAGE {i+1} there is no valid page")
98/143:
page_base = "http://quotes.toscrape.com/page/{}/"
i = 1
r = requests.get(page_base.format(i))
while "No quotes" not in str(soup.find_all("div",class_="col-md-8")[1]):
    print(f"THIS IS PAGE {i}!!!!!!!!!!!!")
    i+=1
    r = requests.get(page_base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    #print(soup.select("div .col-md-8"))
print(f"AFTER PAGE {i+1} there is no valid page")
98/144:
page_base = "http://quotes.toscrape.com/page/{}/"
i = 1
r = requests.get(page_base.format(i))
soup = bs4.BeautifulSoup(r.text,"lxml")
while "No quotes" not in str(soup.find_all("div",class_="col-md-8")[1]):
    print(f"THIS IS PAGE {i}!!!!!!!!!!!!")
    i+=1
    r = requests.get(page_base.format(i))
   
    #print(soup.select("div .col-md-8"))
print(f"AFTER PAGE {i+1} there is no valid page")
98/145:
page_base = "http://quotes.toscrape.com/page/{}/"
i = 1
r = requests.get(page_base.format(i))
soup = bs4.BeautifulSoup(r.text,"lxml")
while "No quotes" in str(soup.find_all("div",class_="col-md-8")[1])):
    print(f"THIS IS PAGE {i}!!!!!!!!!!!!")
    i+=1
    r = requests.get(page_base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    #print(soup.select("div .col-md-8"))
else:
98/146:
page_base = "http://quotes.toscrape.com/page/{}/"
i = 1
r = requests.get(page_base.format(i))
soup = bs4.BeautifulSoup(r.text,"lxml")
while "No quotes" in str(soup.find_all("div",class_="col-md-8")[1]):
    print(f"THIS IS PAGE {i}!!!!!!!!!!!!")
    i+=1
    r = requests.get(page_base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    #print(soup.select("div .col-md-8"))
print("this page {} is not valid.".format(i+1))
98/147:
page_base = "http://quotes.toscrape.com/page/{}/"
i = 1
r = requests.get(page_base.format(i))
soup = bs4.BeautifulSoup(r.text,"lxml")
while "No quotes" in str(soup.find_all("div",class_="col-md-8")[1]):
    print(f"THIS IS PAGE {i}!!!!!!!!!!!!")
    i+=1
    r = requests.get(page_base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    #print(soup.select("div .col-md-8"))
98/148:
page_base = "http://quotes.toscrape.com/page/{}/"
i = 1
r = requests.get(page_base.format(i))
soup = bs4.BeautifulSoup(r.text,"lxml")
while "No quotes" in str(soup.find_all("div",class_="col-md-8")[1])):
    print(f"THIS IS PAGE {i}!!!!!!!!!!!!")
    i+=1
    r = requests.get(page_base.format(i))
    #print(soup.select("div .col-md-8"))
else:
    print("From page {} is invalid".format(i))
98/149:
page_base = "http://quotes.toscrape.com/page/{}/"
i = 1
r = requests.get(page_base.format(i))
soup = bs4.BeautifulSoup(r.text,"lxml")
while "No quotes" in str(soup.find_all("div",class_="col-md-8")[1])):
    print(f"THIS IS PAGE {i}!!!!!!!!!!!!")
    i+=1
    r = requests.get(page_base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    #print(soup.select("div .col-md-8"))
98/150:
page_base = "http://quotes.toscrape.com/page/{}/"
i = 1
r = requests.get(page_base.format(i))
soup = bs4.BeautifulSoup(r.text,"lxml")
while "No quotes" in str(soup.find_all("div",class_="col-md-8")[1]):
    print(f"THIS IS PAGE {i}!!!!!!!!!!!!")
    i+=1
    r = requests.get(page_base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    #print(soup.select("div .col-md-8"))
98/151:
page_base = "http://quotes.toscrape.com/page/{}/"
i = 1
r = requests.get(page_base.format(i))
soup = bs4.BeautifulSoup(r.text,"lxml")
while "No quotes" in str(soup.find_all("div",class_="col-md-8")[1]):
    print(f"THIS IS PAGE {i}!!!!!!!!!!!!")
    i+=1
    r = requests.get(page_base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    #print(soup.select("div .col-md-8"))
98/152:
page_base = "http://quotes.toscrape.com/page/{}/"
i = 1
r = requests.get(page_base.format(i))
soup = bs4.BeautifulSoup(r.text,"lxml")
while "No quotes" not in str(soup.find_all("div",class_="col-md-8")[1]):
    print(f"THIS IS PAGE {i}!!!!!!!!!!!!")
    i+=1
    r = requests.get(page_base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    #print(soup.select("div .col-md-8"))
98/153:
page_base = "http://quotes.toscrape.com/page/{}/"
i = 1
r = requests.get(page_base.format(i))
soup = bs4.BeautifulSoup(r.text,"lxml")
while "No quotes" not in str(soup.find_all("div",class_="col-md-8")[1]):
    print(f"THIS IS PAGE {i}!!!!!!!!!!!!")
    i+=1
    r = requests.get(page_base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    #print(soup.select("div .col-md-8"))
print("AFTER THE PAGE {i+1}, THERE IS NOT ANY VALID PAGES!!!!!!")
98/154:
page_base = "http://quotes.toscrape.com/page/{}/"
i = 1
r = requests.get(page_base.format(i))
soup = bs4.BeautifulSoup(r.text,"lxml")
while "No quotes" not in str(soup.find_all("div",class_="col-md-8")[1]):
    print(f"THIS IS PAGE {i}!!!!!!!!!!!!")
    i+=1
    r = requests.get(page_base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    #print(soup.select("div .col-md-8"))
print(f"AFTER THE PAGE {i+1}, THERE IS NOT ANY VALID PAGES!!!!!!")
98/155:
page_base = "http://quotes.toscrape.com/page/{}/"
i = 1
r = requests.get(page_base.format(i))
soup = bs4.BeautifulSoup(r.text,"lxml")
while "No quotes" not in str(soup.find_all("div",class_="col-md-8")[1]):
    print(f"THIS IS PAGE {i}!!!!!!!!!!!!")
    i+=1
    r = requests.get(page_base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    #print(soup.select("div .col-md-8"))
print(f"AFTER THE PAGE {i}, THERE IS NOT ANY VALID PAGES!!!!!!")
100/1:
base = "https://quotes.toscrape.com/page/{}/"
i = 1
req = requests.get(base.format(i))
soup = bs4.BeautifulSoup(req.text,'lxml')
auth_set_1 = set()
auth_set_all = set()

for i in soup.find_all('small'):
    auth_set_1.add(i.text)

while "No quotes" not in str(soup.find_all("div",class_="col-md-8")[1]):
    print(f"THIS IS PAGE {i}!!!!!!!!!!!!")
    i+=1
    r = requests.get(base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    for auth in soup.find_all('small'):
        auth_set_all.add(auth.text)
100/2:
import bs4
import requests
base = "https://quotes.toscrape.com/page/{}/"
i = 1
req = requests.get(base.format(i))
soup = bs4.BeautifulSoup(req.text,'lxml')
auth_set_1 = set()
auth_set_all = set()

for i in soup.find_all('small'):
    auth_set_1.add(i.text)

while "No quotes" not in str(soup.find_all("div",class_="col-md-8")[1]):
    print(f"THIS IS PAGE {i}!!!!!!!!!!!!")
    i+=1
    r = requests.get(base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    for auth in soup.find_all('small'):
        auth_set_all.add(auth.text)
100/3:
import bs4
import requests
base = "https://quotes.toscrape.com/page/{}/"
i = 1
req = requests.get(base.format(i))
soup = bs4.BeautifulSoup(req.text,'lxml')
auth_set_1 = set()
auth_set_all = set()

for i in soup.find_all('small'):
    auth_set_1.add(i.text)

while "No quotes" not in str(soup.find_all("div",class_="col-md-8")[1]):
    i+=1
    r = requests.get(base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    for auth in soup.find_all('small'):
        auth_set_all.add(auth.text)
100/4:
import bs4
import requests
base = "https://quotes.toscrape.com/page/{}/"
i = 1
req = requests.get(base.format(i))
soup = bs4.BeautifulSoup(req.text,'lxml')
auth_set_1 = set()
auth_set_all = set()

for auths in soup.find_all('small'):
    auth_set_1.add(auths.text)

while "No quotes" not in str(soup.find_all("div",class_="col-md-8")[1]):
    i+=1
    r = requests.get(base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    for auth in soup.find_all('small'):
        auth_set_all.add(auth.text)
100/5:
import bs4
import requests
base = "https://quotes.toscrape.com/page/{}/"
i = 1
req = requests.get(base.format(i))
soup = bs4.BeautifulSoup(req.text,'lxml')
auth_set_1 = set()
auth_set_all = set()

for auths in soup.find_all('small'):
    auth_set_1.add(auths.text)

while "No quotes" not in str(soup.find_all("div",class_="col-md-8")[1]):
    i+=1
    r = requests.get(base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    for auth in soup.find_all('small'):
        auth_set_all.add(auth.text)
print(auth_set_all)
100/6:
import bs4
import requests
base = "https://quotes.toscrape.com/page/{}/"
i = 1
req = requests.get(base.format(i))
soup = bs4.BeautifulSoup(req.text,'lxml')
auth_set_1 = set()
auth_set_all = set()

for auths in soup.find_all('small'):
    auth_set_1.add(auths.text)

while "No quotes" not in str(soup.find_all("div",class_="col-md-8")[1]):
    i+=1
    r = requests.get(base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    for auth in soup.find_all('small'):
        auth_set_all.add(auth.text)
print(auth_set_all[0])
100/7:
import bs4
import requests
base = "https://quotes.toscrape.com/page/{}/"
i = 1
req = requests.get(base.format(i))
soup = bs4.BeautifulSoup(req.text,'lxml')
auth_set_1 = set()
auth_set_all = set()

print(auth_set_1)
for auths in soup.find_all('small'):
    auth_set_1.add(auths.text)

while "No quotes" not in str(soup.find_all("div",class_="col-md-8")[1]):
    i+=1
    r = requests.get(base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    for auth in soup.find_all('small'):
        auth_set_all.add(auth.text)
100/8:
import bs4
import requests
base = "https://quotes.toscrape.com/page/{}/"
i = 1
req = requests.get(base.format(i))
soup = bs4.BeautifulSoup(req.text,'lxml')
auth_set_1 = set()
auth_set_all = set()

for auths in soup.find_all('small'):
    auth_set_1.add(auths.text)

print(auth_set_1)

while "No quotes" not in str(soup.find_all("div",class_="col-md-8")[1]):
    i+=1
    r = requests.get(base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    for auth in soup.find_all('small'):
        auth_set_all.add(auth.text)
100/9:
import bs4
import requests
base = "https://quotes.toscrape.com/page/{}/"
i = 1
req = requests.get(base.format(i))
soup = bs4.BeautifulSoup(req.text,'lxml')
auth_set_1 = set()
auth_set_all = set()

for auths in soup.find_all('small'):
    auth_set_1.add(auths.text)

print(auth_set_1)

while "No quotes" not in str(soup.find_all("div",class_="col-md-8")[1]):
    i+=1
    r = requests.get(base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    for auth in soup.find_all('small'):
        auth_set_all.add(auth.text)

print(auth_set_all - auth_set_1)
100/10:
import bs4
import requests
base = "https://quotes.toscrape.com/page/{}/"
i = 1
req = requests.get(base.format(i))
soup = bs4.BeautifulSoup(req.text,'lxml')
auth_set_1 = set()
auth_set_all = set()

for auths in soup.find_all('small'):
    auth_set_1.add(auths.text)

print(auth_set_1)

while "No quotes" not in str(soup.find_all("div",class_="col-md-8")[1]):
    i+=1
    r = requests.get(base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    for auth in soup.find_all('small'):
        auth_set_all.add(auth.text)

print(auth_set_all.difference(auth_set_1))
100/11:
import bs4
import requests
base = "https://quotes.toscrape.com/page/{}/"
i = 1
req = requests.get(base.format(i))
soup = bs4.BeautifulSoup(req.text,'lxml')
auth_set_1 = set()
auth_set_all = set()

for auths in soup.find_all('small'):
    auth_set_1.add(auths.text)

print(auth_set_1)

while "No quotes" not in str(soup.find_all("div",class_="col-md-8")[1]):
    i+=1
    r = requests.get(base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    for auth in soup.find_all('small'):
        auth_set_all.add(auth.text)

print(auth_set_all.difference(auth_set_all))
100/12:
import bs4
import requests
base = "https://quotes.toscrape.com/page/{}/"
i = 1
req = requests.get(base.format(i))
soup = bs4.BeautifulSoup(req.text,'lxml')
auth_set_1 = set()
auth_set_all = set()

for auths in soup.find_all('small'):
    auth_set_1.add(auths.text)

while "No quotes" not in str(soup.find_all("div",class_="col-md-8")[1]):
    i+=1
    r = requests.get(base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    for auth in soup.find_all('small'):
        auth_set_all.add(auth.text)

print(auth_set_all.difference(auth_set_1))
100/13:
import bs4
import requests
base = "https://quotes.toscrape.com/page/{}/"
i = 1
req = requests.get(base.format(i))
soup = bs4.BeautifulSoup(req.text,'lxml')
auth_set_1 = set()
auth_set_all = set()

for auths in soup.find_all('small'):
    auth_set_1.add(auths.text)

while "No quotes" not in str(soup.find_all("div",class_="col-md-8")[1]):
    i+=1
    r = requests.get(base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    for auth in soup.find_all('small'):
        auth_set_all.add(auth.text)

auth_set_all.add(auth_set_1)
print(auth_set_all)
100/14:
import bs4
import requests
base = "https://quotes.toscrape.com/page/{}/"
i = 1
req = requests.get(base.format(i))
soup = bs4.BeautifulSoup(req.text,'lxml')
auth_set_1 = set()
auth_set_all = set()

for auths in soup.find_all('small'):
    auth_set_1.add(auths.text)

while "No quotes" not in str(soup.find_all("div",class_="col-md-8")[1]):
    i+=1
    r = requests.get(base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    for auth in soup.find_all('small'):
        auth_set_all.add(auth.text)

auth_set_all.union(auth_set_1)
print(auth_set_all)
100/15:
import bs4
import requests
base = "https://quotes.toscrape.com/page/{}/"
i = 1
req = requests.get(base.format(i))
soup = bs4.BeautifulSoup(req.text,'lxml')
auth_set_1 = set()
auth_set_all = set()

for auths in soup.find_all('small'):
    auth_set_1.add(auths.text)

while "No quotes" not in str(soup.find_all("div",class_="col-md-8")[1]):
    i+=1
    r = requests.get(base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    for auth in soup.find_all('small'):
        auth_set_all.add(auth.text)

auth_set_all.union(auth_set_1)
auth_set_all.sort()
100/16:
import bs4
import requests
base = "https://quotes.toscrape.com/page/{}/"
i = 1
req = requests.get(base.format(i))
soup = bs4.BeautifulSoup(req.text,'lxml')
auth_set_1 = set()
auth_set_all = set()

for auths in soup.find_all('small'):
    auth_set_1.add(auths.text)

while "No quotes" not in str(soup.find_all("div",class_="col-md-8")[1]):
    i+=1
    r = requests.get(base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    for auth in soup.find_all('small'):
        auth_set_all.add(auth.text)

auth_set_all.union(auth_set_1)
print(sorted(auth_set_all))
100/17:
import bs4
import requests
base = "https://quotes.toscrape.com/page/{}/"
i = 1
req = requests.get(base.format(i))
soup = bs4.BeautifulSoup(req.text,'lxml')
auth_set_1 = set()
auth_set_all = set()

for auths in soup.find_all('small'):
    auth_set_1.add(auths.text)

while "No quotes" not in str(soup.find_all("div",class_="col-md-8")[1]):
    i+=1
    r = requests.get(base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    for auth in soup.find_all('small'):
        auth_set_all.add(auth.text)

auth_set_all.union(auth_set_1)
for element in sorted(auth_set_all):
    print(element)
    print("\n")
100/18:
import bs4
import requests
base = "https://quotes.toscrape.com/page/{}/"
i = 1
req = requests.get(base.format(i))
soup = bs4.BeautifulSoup(req.text,'lxml')
auth_set_1 = set()
auth_set_all = set()

for auths in soup.find_all('small'):
    auth_set_1.add(auths.text)

while "No quotes" not in str(soup.find_all("div",class_="col-md-8")[1]):
    i+=1
    r = requests.get(base.format(i))
    soup = bs4.BeautifulSoup(r.text,"lxml")
    for auth in soup.find_all('small'):
        auth_set_all.add(auth.text)

auth_set_all.union(auth_set_1)
for element in sorted(auth_set_all):
    print(element)
101/1: !pip install pillow
101/2: !pip install  --upgrade pillow
101/3: from PIL import Image
101/4: mac =  Image.open('example.jpg')
101/5: mac =  Image.open('example.jpg')
101/6: mac.show()
101/7: type(mac)
101/8: mac
101/9: mac.size
101/10: type(mac.filename)
101/11: mac.filename
101/12: mac.format_description
101/13: mac.crop((0,0,100,100))
101/14: mac.crop((20,20,100,100))
101/15: mac.crop((20,70,100,100))
101/16: mac.crop((0,0,100,100))
101/17: mac.crop((20,80,100,100))
101/18: mac.crop((20,80,123,111))
101/19: mac.size.x
101/20: mac.size
101/21: mac.size[0]
101/22: mac.size[1]
101/23: mac.size[2]
101/24: mac.paste(im=cropped, box = (0,0))
101/25: cropped = mac.crop((2;0,80,123,111)) # mac.crop(start_X,start_Y,target_X,target_Y)
101/26: cropped = mac.crop((20,80,123,111)) # mac.crop(start_X,start_Y,target_X,target_Y)
101/27: mac.paste(im=cropped, box = (0,0))
101/28: mac.paste(im=cropped, box = (0,0))
101/29: mac
101/30: mac.paste(im=cropped, box = (125,0))
101/31: mac
101/32: mac.resize((3000,500))
101/33: mac.resize((500,500))
101/34: mac.resize((1000,1000))
101/35: mac.resize((100,100))
101/36: mac.rotate(90)
101/37: red = Image.open("red.jpg")
101/38: red
101/39: blue = Image.open("blue.jpg")
101/40: blue
101/41: blue.putalpha(0)
101/42: blue.putalpha(1)
101/43: blue.putalpha(10)
101/44: blue
101/45: blue.putalpha(0)
101/46: blue
101/47: blue.putalpha(100)
101/48: blue
101/49: blue.putalpha(10000000000)
101/50: blue.putalpha(255)
101/51: blue
101/52: blue.putalpha(104)
101/53: blue
101/54: red.putalpha(128)
101/55: red
101/56: red.putalpha(158)
101/57: red
101/58: red.putalpha(200)
101/59: red
101/60: red.putalpha(180)
101/61: red
101/62: mac.putalpha(128)
101/63: mac
101/64: mac.putalpha(254)
101/65: mac
101/66: blue.paste(red, (0,0))
101/67: blue
101/68: red.crop((0,0,100,100))
101/69: red
101/70: red
101/71: red = red.crop((0,0,100,100))
101/72: red
101/73: blue.paste(red,(0,0))
101/74: blue
101/75: blue.putalpha(255)
101/76: blue
101/77: blue
101/78: blue = Image.open("blue.jpg")
101/79: blue
101/80: blue.putalpha(255)
101/81: blue
101/82: red.putalpha(180)
101/83: red
101/84: mac.putalpha(128)
101/85: red = red.crop((0,0,100,100))
101/86: red
101/87: blue.paste(red,(0,0))
101/88: blue
101/89: blue.paste(red,(0,0),mask = red)
101/90: blue
101/91: blue
101/92: red
101/93: blue.paste(red,(0,0),mask = red)
101/94: blue
101/95: blue
101/96: blue.paste(im=red,box=(0,0),mask = red)
101/97: blue
101/98: blue.save("./quarter_red.png")
101/99: qua = open("quarter_red.png")
101/100: qua
101/101: qua.show()
101/102: qua
101/103: qua =Image.open("quarter_red.png")
101/104: qua
102/1: from PIL import Image
102/2: mtrx = Image.open("word_matrix.png")
102/3: mtrx
102/4: mask = Image.open("mask.png")
102/5: mask
102/6: mask.size
102/7: mtrx = Image.open("word_matrix.png")
102/8: mtrx
102/9: mtrx.size
102/10: mask.resize((1015,559))
102/11: mask
102/12: mtrx.paste(im=resized, box=(0,0),mask = resized)
102/13: resized = mask.resize((1015,559))
102/14: mtrx.paste(im=resized, box=(0,0),mask = resized)
102/15: mtrx
102/16: mtrx.putalpha(0)
102/17: mtrx
102/18: mtrx.putalpha(50)
102/19: mtrx
102/20: mtrx.paste(im=resized, box=(0,0))
102/21: mtrx
102/22: mtrx.size
102/23: resized = mask.resize(1015,753)
102/24: resized = mask.resize((1015,753))
102/25: resized
102/26: resized.putalpha(100)
102/27: resized
102/28: new = mtrx.paste(resized,box=(0,0),mask=resized)
102/29: new
102/30: mtrx
102/31: from PIL import Image
102/32: mtrx = Image.open("word_matrix.png")
102/33: mtrx = Image.open("word_matrix.png")
102/34: mtrx
102/35: mask = Image.open("mask.png")
102/36: mask
102/37: mask.size
102/38: mtrx.size
102/39: resized.putalpha(100)
102/40: mtrx
102/41: new = mtrx.paste(resized,box=(0,0),mask=resized)
102/42: mtrx
102/43: resized = mask.resize((1015,753))
102/44: resized.putalpha(100)
102/45: resized
102/46: mtrx.resize((1015,753))
102/47: resized = mask.resize((1015,559))
102/48: resized.putalpha(100)
102/49: resized
102/50: mtrx
102/51: mtrx
102/52: mtrx = Image.open("word_matrix.png")
102/53: mtrx
102/54: mask = Image.open("mask.png")
102/55: mask
102/56: resized = mask.resize((1015,559))
102/57: resized.putalpha(100)
102/58: resized
102/59: mtrx
102/60: mtrx = mtrx.paste(resized,box=(0,0),resized)
102/61: mtrx = mtrx.paste(resized,box=(0,0),mask=resized)
102/62: mtrx
102/63: mtrx
102/64: mtrx.putalpha(50)
102/65: mtrx
102/66: mtrx
102/67: mtrx
102/68: resized = mask.resize((1015,559))
102/69: resized.putalpha(100)
102/70: resized
102/71: mtrx = mtrx.paste(im=resized,box=(0,0),mask=resized)
102/72: mtrx.paste(im=resized,box=(0,0),mask=resized)
102/73: mtrx.paste(im=resized,box=(0,0),mask=resized)
102/74: mtrx = Image.open("word_matrix.png")
102/75: mtrx
102/76: mask = Image.open("mask.png")
102/77: mask
102/78: mask.size
102/79: mtrx.size
102/80: resized = mask.resize((1015,559))
102/81: resized.putalpha(100)
102/82: resized
102/83: mtrx.paste(im=resized,box=(0,0),mask=resized)
102/84: mtrx
103/1:
#Pandas
#Openpyxl
#Google Sheets Python API
103/2: import csv
103/3:
#Open the file
data = open("example.csv")
#csv.reader
csv_data = csv.reader(data)
#Reformat it into a python object (list of lists)
data_lines = list(csv_data)
103/4:
#Open the file
data = open("example.csv")
#csv.reader
csv_data = csv.reader(data)
#Reformat it into a python object (list of lists)
data_lines = list(csv_data)
103/5:
#Open the file
data = open("example.csv",encoding="utf-8")
#csv.reader
csv_data = csv.reader(data)
#Reformat it into a python object (list of lists)
data_lines = list(csv_data)
103/6: data_lines
103/7: data_lines[0]
103/8: len(data_lines)
103/9: len(data_lines[0])
103/10: len(data_lines[])
103/11: len(data_lines)
103/12:
for line in data_lines:[:5]:
    print(line)
103/13:
for line in data_lines[:5]:
    print(line)
103/14: data_lines[10]
103/15: data_lines[10][3]
103/16: all_emails = data_lines[][3]
103/17: all_emails = data_lines[*][3]
103/18:
for email in data_lines:
    email[3]
103/19:
for email in data_lines:
    print(email[3])
103/20:
for email in data_lines[1:]:
    print(email[3])
103/21:
for full_name in data_lines[1:]:
    print(full_name[:1])
103/22:
for full_name in data_lines[1:]:
    print(full_name[2:3])
103/23:
for full_name in data_lines[1:]:
    print(full_name[2],full_name[3])
103/24:
for full_name in data_lines[1:]:
    print(full_name[1],full_name[2])
103/25: file_to_output = open("to_save_file.csv",mode='w',newline='')
103/26: csv_writer = csv.writer(file_to_output,delimiter = ',')
103/27: csv_writer.writerow(['a','b','c'])
103/28: csv_writer.writerows([[1,2,3],[4,5,6]])
103/29: file_to_output.close()
103/30: f = open('to_save_file.csv')
103/31: csv_writer = csv.writer(f)
103/32: csv_writer.writerow(['1','2','3'])
103/33: f = open('to_save_file.csv',mode='a',newline='') # Append mode
103/34: csv_writer = csv.writer(f)
103/35: csv_writer.writerow(['1','2','3'])
103/36: f.close()
104/1: !pip install PyPDF2
104/2: import PyPDF2
104/3: f = open("Working_Business_Proposal.pdf,'rb')
104/4: pdf_reader = PyPDF2.PdfFileReader(f)
104/5: f = open("Working_Business_Proposal.pdf",'rb')
104/6: pdf_reader = PyPDF2.PdfFileReader(f)
104/7: pdf_reader = PyPDF2.PdfFileReader(f)
104/8: pdf_reader = PyPDF2.PdfReader(f)
104/9: pdf_reader.numPages
104/10: pdf_reader.pages
104/11: len(pdf_reader.pages)
104/12: page_one = pdf_reader.get_page_number
104/13: page_one
104/14: page_one = pdf_reader.get_page_number()
104/15: page_one = pdf_reader.get_page_number(1)
104/16: page_one = pdf_reader.getPage(0)
104/17: page_one = pdf_reader.pages[0]
104/18: print(page_one)
104/19: page_one = pdf_reader.pages[1]
104/20: print(page_one)
104/21: page_one_text = page_one.extract_text()
104/22: page_one_text
104/23: page_one = pdf_reader.pages[0]
104/24: page_one_text = page_one.extract_text()
104/25: page_one_text
104/26: page_one_text = page_one.extractText()
104/27: page_one_text = page_one.extract_text()
104/28: page_one_text
104/29: !pip install PyPDF2
104/30: import PyPDF2
104/31: f = open("Working_Business_Proposal.pdf",'rb')
104/32: pdf_reader = PyPDF2.PdfReader(f)
104/33: len(pdf_reader.pages)
104/34: page_one = pdf_reader.pages[0]
104/35: page_one_text = page_one.extract_text()
104/36: page_one_text
104/37:
for i in range(len(pdf_reader)):
    page = pdf_reader.pages[i]
    page_text = page.extract_text()
    print(page_text)
104/38: pdf_reader = PyPDF2.PdfReader(f)
104/39: len(pdf_reader.pages)
104/40: page_one = pdf_reader.pages[0]
104/41: page_one_text = page_one.extract_text()
104/42:
for i in range(len(pdf_reader.pages)):
    page = pdf_reader.pages[i]
    page_text = page.extract_text()
    print(page_text)
104/43:
for i in range(len(pdf_reader.pages)):
    page = pdf_reader.pages[i]
    page_text = page.extract_text()
    print(page_text)
    print("-------------------------------------------------------------------------------------------")
104/44: f = open("Working_Business_Proposal.pdf",'rb')
104/45: pdf_reader = PyPDF2.PdfReader(f)
104/46: first_page = pdf_reader.pages[0]
104/47: pdf_writer = PyPDF2.PdfWriter()
104/48: type(first_page)
104/49: pdf_writer = PyPDF2.PdfWriter()
104/50: pdf_writer.add_page(first_page)
104/51: pdf_output = open("Some_BrandNew_Doc.pdf","wb")
104/52: pdf_writer.write(pdf_output)
104/53: f.close()
104/54: f.closed
104/55: pdf_output.closed
104/56: pdf_output.close()
104/57: pdf_output.closed
104/58:
pdf_texts = []

for page_nmbr in pdf_reader.pages:
    pdf_texts.append(page_nmbr)
104/59: pdf_texts
104/60: f = open("Working_Business_Proposal.pdf",'rb')
104/61: pdf_reader = PyPDF2.PdfReader(f)
104/62: len(pdf_reader.pages)
104/63: pdf_texts
104/64:
f = open("Working_Business_Proposal.pdf",'rb')
pdf_reader = PyPDF2.PdfReader(f)
104/65:
pdf_texts = []

for page_nmbr in pdf_reader.pages:
    pdf_texts.append(page_nmbr)
104/66: pdf_texts
104/67:
pdf_texts = []

for page_nmbr in pdf_reader.pages:
    pdf_texts.append(page_nmbr.extract_text())
104/68: pdf_texts
104/69: pdf_texts[0]
104/70: pdf_texts[1]
105/1: f = open("My_csv_file.csv","wb")
105/2: import requests
105/3: res = requests.get('https://drive.google.com/open?id=1G6SEgg018UB4_4xsAJJ5TdzrhmXipr4Q')
105/4: res.content
105/5: res.a
105/6: res.src
105/7: import bs4
105/8: soup = bs4.BeautifulSoup(res.text, "lxml")
105/9: soup.a
105/10: "src" in str(soup)
105/11: soup.find_all("a")
105/12: soup.find_all("p")
105/13: soup.find_all("b")
105/14: f = open("index.html", "w+")
105/15: f.writable
105/16: f.writable()
105/17: f.write(soup.text)
105/18: f.write(soup)
105/19: f.write(str(soup))
105/20: download_link = soup.find('a', {'id': 'uc-download-link'})
105/21: print(download_link)
105/22: print(soup.text)
105/23: print(soup)
105/24: f = open("./Exercise_Files/find_the_link.csv",encoding="utf-8")
105/25:
import csv
f = open("./Exercise_Files/find_the_link.csv",encoding="utf-8")
105/26: csv_data = csv.reader(f)
105/27: data_lines = list(csv_data)
105/28: data_lines
105/29: data_lines[0]
105/30: data_lines[1]
105/31:
for data in data_lines:
    print(data)
105/32:
for data in data_lines:
    for d in data:
        print(d)
105/33: csv_data.line_num
105/34:

for line in range(csv_data.line_num):
    print(data_lines[line])
105/35:
for line in range(csv_data.line_num):
    print(data_lines[line][line])
105/36:
link = ""
for line in range(csv_data.line_num):
    link += data_lines[line][line]
105/37:
link = ""
for line in range(csv_data.line_num):
    link += data_lines[line][line]
link
105/38: import re
105/39: import PyPDF2
105/40: f = open("Working_Business_Proposal.pdf",'rb')
105/41: pdf_reader = PyPDF2.PdfReader(f)
105/42: len(pdf_reader.pages)
105/43:
page_one = pdf_reader.pages[0]
page_one_text = page_one.extract_text()
print(type(page_one_text))
105/44:
phone_syntax = "\d\d\d \d\d\d \d\d\d\d"
page_count = 0
for page in pdf_reader.pages:
    page_count+=1
    page_text = page.extract_text
    if x:= re.findall(phone_syntax,page_text) != []:
        print("Yeah I found")
        print(x)
105/45:
phone_syntax = "\d\d\d \d\d\d \d\d\d\d"
page_count = 0
for page in pdf_reader.pages:
    page_count+=1
    page_text = page.extract_text
    if re.findall(phone_syntax,page_text) != []:
        print("Yeah I found")
        print(re.findall(phone_syntax,page_text))
105/46:
phone_syntax = "\d\d\d \d\d\d \d\d\d\d"
page_count = 0
for page in pdf_reader.pages:
    page_count+=1
    page_text = page.extract_text
    if re.findall(phone_syntax,page_text) != []:
        print("Yeah I found")
105/47:
phone_syntax = "\d\d\d \d\d\d \d\d\d\d"
page_count = 0
for page in pdf_reader.pages:
    page_count+=1
    page_text = page.extract_text
    if re.findall(phone_syntax,str(page_text)) != []:
        print("Yeah I found")
        print(re.findall(phone_syntax,page_text))
105/48:
phone_syntax = "\d\d\d \d\d\d \d\d\d\d"
page_count = 0
for page in pdf_reader.pages:
    page_count+=1
    page_text = page.extract_text
    if re.findall(phone_syntax,str(page_text)) != []:
        print("Yeah I found")
        print(re.findall(phone_syntax,page_text))
105/49:
phone_syntax = "\d\d\d-\d\d\d-\d\d\d\d"
page_count = 0
for page in pdf_reader.pages:
    page_count+=1
    page_text = page.extract_text
    if re.findall(phone_syntax,str(page_text)) != []:
        print("Yeah I found")
        print(re.findall(phone_syntax,page_text))
105/50:
phone_syntax = "\d\d\d-\d\d\d-\d\d\d\d"
page_count = 0
for page in pdf_reader.pages:
    page_count+=1
    page_text = page.extract_text
    if re.findall(phone_syntax,str(page_text)) != []:
        print("Yeah I found")
        print(re.findall(phone_syntax,page_text))
105/51:
phone_syntax = "\d\d\d-\d\d\d-\d\d\d\d"
page_count = 0
for page in pdf_reader.pages:
    page_count+=1
    page_text = page.extract_text
    print(page_text)
105/52:
phone_syntax = "\d\d\d-\d\d\d-\d\d\d\d"
page_count = 0
for page in pdf_reader.pages:
    page_count+=1
    page_text = page.extract_text()
    print(page_text)
105/53:
phone_syntax = "\d\d\d-\d\d\d \d\d\d\d"
page_count = 0
for page in pdf_reader.pages:
    page_count+=1
    page_text = page.extract_text()
    if re.findall(phone_syntax,str(page_text)) != []:
        print("Yeah I found")
        print(re.findall(phone_syntax,page_text))
105/54:
phone_syntax = "\d\d\d-\d\d\d \d\d\d\d"
page_count = 0
for page in pdf_reader.pages:
    page_count+=1
    page_text = page.extract_text()
    if re.findall(phone_syntax,page_text) != []:
        print("Yeah I found")
        print(re.findall(phone_syntax,page_text))
105/55:
phone_syntax = "\d\d\d-\d\d\d \d\d\d\d"
page_count = 0
for page in pdf_reader.pages:
    page_count+=1
    page_text = page.extract_text()
    print(page_text)
105/56:
phone_syntax = "\d\d\d \d\d\d \d\d\d\d"
page_count = 0
for page in pdf_reader.pages:
    page_count+=1
    page_text = page.extract_text()
    type(page_text)
105/57:
phone_syntax = "\d\d\d \d\d\d \d\d\d\d"
page_count = 0
for page in pdf_reader.pages:
    page_count+=1
    page_text = page.extract_text()
    print(type(page_text))
105/58:
phone_syntax = "\d\d\d \d\d\d \d\d\d\d"
page_count = 0
for page in pdf_reader.pages:
    page_count+=1
    page_text = page.extract_text()
    matches = re.findall(phone_syntax,page_text)
105/59:
phone_syntax = "\d\d\d \d\d\d \d\d\d\d"
page_count = 0
for page in pdf_reader.pages:
    page_count+=1
    page_text = page.extract_text()
    matches = re.findall(phone_syntax,page_text)
    print(matches)
105/60:
phone_syntax = "\d\d\d.\d\d\d.\d\d\d\d"
page_count = 0
for page in pdf_reader.pages:
    page_count+=1
    page_text = page.extract_text()
    matches = re.findall(phone_syntax,page_text)
    print(matches)
105/61:
phone_syntax = "\d{3}.\d{3}.\d{4}"
page_count = 0
for page in pdf_reader.pages:
    page_count+=1
    page_text = page.extract_text()
    matches = re.search(phone_syntax,page_text)
    print(matches)
105/62:
phone_syntax = "\d{3} \d{3} \d{4}"
page_count = 0
for page in pdf_reader.pages:
    page_count+=1
    page_text = page.extract_text()
    matches = re.search(phone_syntax,page_text)
    print(matches)
105/63:
phone_syntax = "\d{3} \d{3} \d{4}"
page_count = 0
for page in pdf_reader.pages:
    page_count+=1
    page_text = page.extract_text()
    matches = re.search(phone_syntax,str(page_text))
    print(matches)
105/64:
phone_syntax = re.compile("(\d{3}).(\d{3}).(\d{4})")
page_count = 0
for page in pdf_reader.pages:
    page_count+=1
    page_text = page.extract_text()
    matches = re.search(phone_syntax,page_text)
    print(matches)
105/65:
phone_syntax = re.compile('(\d{3}).(\d{3}).(\d{4})')
page_count = 0
for page in pdf_reader.pages:
    page_count+=1
    page_text = page.extract_text()
    matches = re.search(phone_syntax,page_text)
    print(matches)
105/66:
phone_syntax = re.compile(r'(\d{3}).(\d{3}).(\d{4})')
page_count = 0
for page in pdf_reader.pages:
    page_count+=1
    page_text = page.extract_text()
    matches = re.search(phone_syntax,page_text)
    print(matches)
105/67:
phone_syntax = re.compile(r'(\d{3}).(\d{3}).(\d{4}).')
page_count = 0
for page in pdf_reader.pages:
    page_count+=1
    page_text = page.extract_text()
    matches = re.search(phone_syntax,page_text)
    print(matches)
105/68:
phone_syntax = re.compile(r'(\d{3}).(\d{3}).(\d{4}).')
page_count = 0
for page in pdf_reader.pages:
    page_count+=1
    page_text = page.extract_text()
    print(page_text)
105/69:
phone_syntax = re.compile(r'(\d{3}).(\d{3}).(\d{4}).')
page_count = 0
empty_idiot = ""
for page in pdf_reader.pages:
    page_count+=1
    page_text = page.extract_text()
    empty_idiot +=page_text
results = re.search(phone_syntax,empty_idiot)
print(results)
105/70:
phone_syntax = re.compile(r'\d{3}.\d{3}.\d{4}.')
page_count = 0
empty_idiot = ""
for page in pdf_reader.pages:
    page_count+=1
    page_text = page.extract_text()
    empty_idiot +=page_text
results = re.search(phone_syntax,empty_idiot)
print(results)
105/71:
phone_syntax = re.compile(r' \d{3}.\d{3}.\d{4}.')
page_count = 0
empty_idiot = ""
for page in pdf_reader.pages:
    page_count+=1
    page_text = page.extract_text()
    empty_idiot +=page_text
results = re.search(phone_syntax,empty_idiot)
print(results)
105/72:
phone_syntax = re.compile(r'\d{3}.\d{3}.\d{4}.')
page_count = 0
empty_idiot = ""
for page in pdf_reader.pages:
    page_count+=1
    page_text = page.extract_text()
    empty_idiot +=page_text
results = re.search(phone_syntax,empty_idiot)
print(results.span)
105/73:
phone_syntax = re.compile(r'\d{3}.\d{3}.\d{4}.')
page_count = 0
empty_idiot = ""
for page in pdf_reader.pages:
    page_count+=1
    page_text = page.extract_text()
    empty_idiot +=page_text
results = re.search(phone_syntax,empty_idiot)
print(results.span())
105/74: f = open("./Exercise_Files/Find_the_Phone_Number.pdf",'rb')
105/75: pdf_reader = PyPDF2.PdfReader(f)
105/76: len(pdf_reader.pages)
105/77:
phone_syntax = re.compile(r'\d{3}.\d{3}.\d{4}.')
page_count = 0
empty_idiot = ""
for page in pdf_reader.pages:
    page_count+=1
    page_text = page.extract_text()
    empty_idiot +=page_text
results = re.search(phone_syntax,empty_idiot)
print(results.span())
105/78:
phone_syntax = re.compile(r'\d{3}.\d{3}.\d{4}.')
page_count = 0
empty_idiot = ""
for page in pdf_reader.pages:
    page_count+=1
    page_text = page.extract_text()
    empty_idiot +=page_text
results = re.findall(phone_syntax,empty_idiot)
105/79:
phone_syntax = re.compile(r'\d{3}.\d{3}.\d{4}')
page_count = 0
empty_idiot = ""
for page in pdf_reader.pages:
    page_count+=1
    page_text = page.extract_text()
    results = re.findall(phone_syntax,page_text)
    for result in results:
        if result != []:
            phone = "".join(result)
            phone.replace("."," ")
            print(phone)
105/80:
phone_syntax = re.compile(r'\d{3}.\d{3}.\d{4}')
page_count = 0
empty_idiot = ""
for page in pdf_reader.pages:
    page_count+=1
    page_text = page.extract_text()
    results = re.findall(phone_syntax,page_text)
    for result in results:
        if result != []:
            phone = "".join(result)
            phone = phone.replace("."," ")
            print(phone)
105/81:
phone_syntax = re.compile(r'\d{3}.\d{3}.\d{4}')
page_count = 0
empty_idiot = ""
for page in pdf_reader.pages:
    page_count+=1
    page_text = page.extract_text()
    results = re.findall(phone_syntax,page_text)
    for result in results:
        if result != []:
            phone = "".join(result)
            phone = phone.replace("."," ")
            print(f"I found a phone number in page{page_count}")
            print(phone)
105/82:
phone_syntax = re.compile(r'\d{3}.\d{3}.\d{4}')
page_count = 0
empty_idiot = ""
for page in pdf_reader.pages:
    page_count+=1
    page_text = page.extract_text()
    results = re.findall(phone_syntax,page_text)
    for result in results:
        if result != []:
            phone = "".join(result)
            phone = phone.replace("."," ")
            print(f"I found a phone number in page {page_count}")
            print(phone)
109/1: import smtplib
109/2: smtp_object = smtplib.SMTP('smtp')
109/3: smtp_object = smtplib.SMTP('smtp.gmail.com',587)
109/4: smtp_object.ehlo()
109/5: smtp_object.starttls()
109/6: input('what is your password')
109/7: import getpass
109/8: password = getpass.getpass('Password Please: ')
109/9: email = getpass.getpass("Email: ")
109/10: password = getpass.getpass("Password: ")
109/11: email = getpass.getpass("Email: ")
111/1: email = getpass.getpass("Email: ")
111/2: import smtplib
111/3: smtp_object = smtplib.SMTP('smtp.gmail.com',587)
111/4: smtp_object.ehlo()
111/5: smtp_object.starttls()
111/6: input('what is your password: ')
111/7: import getpass
111/8: password = getpass.getpass('Password Please: ')
111/9: email = getpass.getpass("Email: ")
111/10: password = getpass.getpass("Password: ")
111/11: email = getpass.getpass("Email: ")
111/12: password = getpass.getpass("Password: ")
111/13: smtp_object.login(email,password)
111/14:
from_adress = email
to_address = email
111/15:
from_adress = email
to_address = email
111/16:
from_adress = email
to_address = email
subject = input("enter the subject line: ")
message = input("enter the message: ")
msg = "Subject: "+subject+'\n'+message
smtp_object.sendmail(from_adress,to_address,msg)
111/17:
from_adress = email
to_address = email
subject = input("enter the subject line: ")
message = input("enter the message: ")
msg = "Subject: "+subject+'\n'+message
smtp_object.sendmail(from_adress,to_address,msg)
112/1:
from_adress = email
to_address = email
subject = input("enter the subject line: ")
message = input("enter the message: ")
msg = "Subject: "+subject+'\n'+message
smtp_object.sendmail(from_adress,to_address,msg)
112/2: import smtplib
112/3: smtp_object = smtplib.SMTP('smtp.gmail.com',587)
112/4: smtp_object.ehlo()
112/5: smtp_object.starttls()
112/6: input('what is your password: ')
112/7: input('what is your password: ')
112/8: input('what is your password: ')
112/9: import getpass
112/10: password = getpass.getpass('Password Please: ')
112/11: email = getpass.getpass("Email: ")
112/12: password = getpass.getpass("Password: ")
112/13: password = getpass.getpass("Password: ")
112/14: password = getpass.getpass("Password: ")
112/15: password = getpass.getpass("Password: ")
112/16: smtp_object.login(email,password)
112/17:
from_adress = email
to_address = email
subject = input("enter the subject line: ")
message = input("enter the message: ")
msg = "Subject: "+subject+'\n'+message
smtp_object.sendmail(from_adress,to_address,msg)
112/18:
from_adress = email
to_address = email
subject = input("enter the subject line: ")
message = input("enter the message: ")
msg = "Subject: "+subject+'\n'+message
smtp_object.sendmail(from_adress,to_address,msg)
112/19:
from_adress = email
to_address = "sariomer793@gmail.com"
subject = input("enter the subject line: ")
message = input("enter the message: ")
msg = "Subject: "+subject+'\n'+message
smtp_object.sendmail(from_adress,to_address,msg)
112/20:
from_adress = email
to_address = email
subject = input("enter the subject line: ")
message = input("enter the message: ")
msg = "Subject: "+subject+'\n'+message
smtp_object.sendmail(from_adress,to_address,msg)
112/21:
from_adress = email
to_address = "sariomer597@gmail.com"
subject = input("enter the subject line: ")
message = input("enter the message: ")
msg = "Subject: "+subject+'\n'+message
smtp_object.sendmail(from_adress,to_address,msg)
112/22:
from_adress = email
to_address = "sariomer793@gmail.com"
subject = input("enter the subject line: ")
message = input("enter the message: ")
msg = "Subject: "+subject+'\n'+message
smtp_object.sendmail(from_adress,to_address,msg)
112/23:
from_adress = email
to_address = "sariomer793@gmail.com"
subject = input("enter the subject line: ")
message = input("enter the message: ")
msg = "Subject: "+subject+'\n'+message
smtp_object.sendmail(from_adress,to_address,msg)
112/24: smtp_object.quit()
113/1:
#imaplib
#email
113/2: import imaplib
115/1:
#imaplib
#email
115/2: import imaplib
115/3: M =  imaplib.IMAP4_SSL('imap.gmail.com')
115/4: import getpass
115/5: email = getpass.getpass("Email: ")
115/6:
email = getpass.getpass("Email: ")
password = getpass.getpass("Password: ")
115/7:
email = getpass.getpass("Email: ")
password = getpass.getpass("Password: ")
115/8: M.login(email,password)
115/9: M.list()
115/10: M.select("INBOX")
115/11: typ,data = M.search(None, 'BEFORE 01-Nov-2000')
115/12: typ
115/13: data
115/14: type(data)
115/15: len(data(
115/16: len(data)
116/1: import smtplib
116/2: smtp_object = smtplib.SMTP('smtp.gmail.com',587)
116/3: smtp_object.ehlo()
116/4: smtp_object.starttls()
116/5: import getpass
116/6: email = getpass.getpass("Email: ")
116/7: password = getpass.getpass("Password: ")
116/8: smtp_object.login(email,password)
116/9:
from_adress = email
to_address = email
subject = input("enter the subject line: ")
message = input("enter the message: ")
msg = "Subject: "+subject+'\n'+message
smtp_object.sendmail(from_adress,to_address,msg)
115/17: typ,data = M.search(None, 'SUBJECT "NEW TEST PYTHON"')
115/18: typ
115/19: data
115/20: type(data)
115/21: len(data)
115/22: emai_id = data[0]
115/23: email_id = data[0]
115/24: typ,data = M.search(None, 'SUBJECT "NEW TEST PYTHON"')
115/25: typ
115/26: data
115/27: type(data)
115/28: len(data)
115/29: email_id = data[0]
115/30: result, email_data = M.fetch(email_id,'(RFC822)')
115/31: data
115/32: typ,data = M.search(None,'SUBJECT "NEW TEST PYTHON"')
115/33: typ
115/34: data
115/35: len(data)
115/36: email_id = data[0]
115/37: result, email_data = M.fetch(email_id,'(RFC822)')
115/38: typ,data = M.search(None,'SUBJECT "Yeni YÄ±l MesajÄ±"')
115/39: typ,data = M.search('SUBJECT "Yeni YÄ±l MesajÄ±"')
115/40: typ,data = M.search(None,'SUBJECT "[SPECIAL OFFER 2023 !] WEB DESIGN DEV: 7 Courses in 1"')
115/41: typ
115/42: data
115/43: type(data)
115/44: len(data)
115/45: email_id = data[0]
115/46: result, email_data = M.fetch(email_id,'(RFC822)')
115/47: email_data
115/48: email_data.text
115/49: type(email_data)
115/50: len(email_data)
115/51:
for i in email_data:
    print(i)
116/10:
from_adress = email
to_address = email
subject = input("enter the subject line: ")
message = input("enter the message: ")
msg = "Subject: "+subject+'\n'+message
smtp_object.sendmail(from_adress,to_address,msg)
115/52: typ,data = M.search(None,'SUBJECT "NEW TEST PYTHON"')
115/53: typ
115/54: data
115/55: type(data)
115/56: len(data)
115/57: typ,data = M.search(None,'SUBJECT "NEW TEST PYTHON"')
115/58: typ
115/59: data
115/60: type(data)
115/61: len(data)
115/62: result, email_data = M.fetch(email_id,'(RFC822)')
115/63: data
115/64: data[0]
115/65: typ,data = M.search(None,'SUBJECT "Python for Cyber Security (2024)"')
115/66: typ
115/67: data
115/68: type(data)
115/69: len(data)
115/70: data[0]
115/71: result, email_data = M.fetch(email_id,'(RFC822)')
115/72: email_data
115/73: raw_email =  email_data[0][1]
115/74:
raw_email =  email_data[0][1]
type(raw_email)
115/75:
raw_email =  email_data[0][1]
type(raw_email)
raw_email_string = raw_email.decode("utf-8")
115/76: import email
115/77: email_message = email.message_from_string(raw_email_string)
115/78: type(email_message)
115/79: email_message = email.message_from_string(raw_email_string)
115/80: email_message
115/81: print(email_message)
115/82:
for part in email_message.walk():

    if part.get_content_type() == "text/plain":
        body = part.get_payload(decode = True)
        print(body)
115/83:
for part in email_message.walk():

    if part.get_content_type() == "text/plain":
        body = part.get_payload(decode = True)
        print(body)
115/84: email_message
115/85:
for part in email_message.walk():

    if part.get_content_type() == "text/plain":
        body = part.get_payload(decode = True)
        print(body)
115/86:
for part in email_message.walk():

    if part.get_content_type() == "text/html":
        body = part.get_payload(decode = True)
        print(body)
117/1: hex(12)
117/2: hex(512)
117/3: bin(1234)
117/4: bin(128)
117/5: 2**4
117/6: pow(2,4)
117/7: pow(2,4,3)
117/8: abs(-2)
117/9: round(3.0)
117/10: round(3.141592,2)
118/1: s = 'hello world'
118/2: s[0]
118/3: s.capitalize()
118/4: s.upper()
118/5: s
118/6: s.lower()
118/7: s.count('o')
118/8: s.find('o')
118/9: len(s)
118/10: s.center(20,'z')
118/11: len(s.center(20,'z'))
118/12: print"hello\thi"
118/13: print "hello\thi"
118/14: print 'hello\thi'
118/15: print ('hello\thi')
118/16: 'hello\thi'.expandtabs()
118/17: 'hello\nhi'.expandtabs()
118/18: 'hello\t\thi'.expandtabs()
118/19: s = 'hello'
118/20: s.isalnum()
118/21: s.isalpha()
118/22: s.islower()
118/23: s.isspace()
118/24: s.istitle()
118/25: s.upper()
118/26: s.isupper()
118/27: s.upper().isupper()
118/28: s.endswith('o')
118/29: s[-1] == 'o'
118/30: s.split('e')
118/31: s = "hihihihihihihihihihihihih"
118/32: s.split('i')
118/33: s = "hihihihihihihihihihhhhihihih"
118/34: s.split('i')
118/35: s.partition('i')
118/36: type(s.partition('i'))
119/1: s = set()
119/2: s.add(1)
119/3: s.add(2)
119/4: s
119/5: s.add(2)
119/6: s
119/7: s.clear()
119/8: s
119/9: s = {1,2,3}
119/10: sc = s.copy()
119/11: sc
119/12: s.add(4)
119/13: s
119/14: sc
119/15: s.difference(sc)
119/16: s1 = {1,2,3}
119/17: s2 = {1,4,5}
119/18: s1.difference_update(s2)
119/19: s1
119/20: # S1 =  S1 - S2 gibi bir sonuÃ§ ortaya Ã§Ä±kÄ±yor
119/21: s
119/22: s.discard(2)
119/23: s
119/24: s.discard(12)
119/25: s
119/26: s1 = {1,2,3}
119/27: s2 = {1,2,4}
119/28: s1.intersection(s2)
119/29: s1
119/30: s1.intersection_update(s2)
119/31: s1
119/32: # S1 = S1 â© S2 gibi bir sonuÃ§ ortaya Ã§Ä±kÄ±yor
119/33: s1 = {1,2}
119/34:
s2 = {1,2,4}
s3 = {5}
119/35: s1.isdisjoint(s2)
119/36: s1.isdisjoint(s3)
119/37: s1.issubset(s2)
119/38: s2.issuperset(s1)
119/39: s1.issuperset(s2)
119/40: s1.symmetric_difference(s2)
119/41: s2.union(s5)
119/42: s2.union(s3)
119/43: s1.update(s3)
119/44: s1
119/45: s1.update(s2)
119/46: s1
120/1: d = { "k1":1,"k2":2}
120/2: {x:x**2 for x in range(10)}
120/3: {k:x**2 for k,x in zip(['a','b','c'],range(3))}
120/4: d = { "k1":1,"k2":2}
120/5: {x:x**2 for x in range(10)}
120/6: {k:x**2 for k,x in zip(['a','b','c'],range(3))}
120/7: d["k1"]
120/8: d.values
120/9: d.values()
120/10: type(d.values)
121/1: l = [1,2,3]
121/2: l.append(4)
121/3: l
121/4: l.count(10)
121/5: l.count(1)
121/6:
x = [1,2,3]
x.append([4,5])
121/7: print(x)
121/8: len(x)
121/9:
x = [1,2,3]
x.extend([4,5])
121/10: x
121/11: l.index(2)
121/12: l.index(10)
121/13: l
121/14: l.insert(2,'insert')
121/15: l.remove('insert')
121/16: l
121/17: l.insert(2,'inserted')
121/18: l
121/19: ele = l.pop()
121/20: ele
121/21: l
121/22: l.pop(0)
121/23: L
121/24: l
121/25: k = [1,2,3,4,3]
121/26: k.remove(3)
121/27: k
121/28: k.reverse()
121/29: k
121/30: k.sort()
121/31: k
122/1: bin(1024)
122/2: bin(1024),hex(1024)
122/3: round(5.23222,2)
122/4: round(5.23222,1)
122/5: round(5.23222,0)
122/6:
s = 'hello how are you Mary, are you feeling okay?'

s.islower()
122/7:
s = 'twywywtwywbwhsjhwuwshshwuwwwjdjdid'
s.count('w')
122/8:
set1 = {2,3,1,5,6,8}
set2 = {3,1,7,5,6,8}

set1.difference(set2)
122/9: set1.union(set2)
122/10: dict = {a:b**3 for a,b in range(5),range(5)}
122/11: dict = {a:b**3 for a,b in zip(range(5),range(5))}
122/12:
dict = {a:b**3 for a,b in zip(range(5),range(5))}
dict
122/13:
list1 = [1,2,3,4]

list1.reverse()
122/14:
list1 = [1,2,3,4]

list1.reverse()
list1
122/15:
list2 = [3,4,2,5,1]
list2.sort()
122/16:
list2 = [3,4,2,5,1]
list2.sort()
list2
123/1: !pip install nltk
123/2: !pip install 1q wordcloud
123/3: !pip install -q wordcloud
123/4: !pip install -q wordcloud
123/5:
import wordcloud
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
123/6:
import wordcloud
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
123/7:
import wordcloud
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
123/8:
import pandas as pd
import numpy as np
import matplotlib.matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score,classification_report,confusion_matrix
from sklearn.svm import SVC
import warnings
warnings.filterwarnings('ignore')
123/9:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score,classification_report,confusion_matrix
from sklearn.svm import SVC
import warnings
warnings.filterwarnings('ignore')
123/10:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score,classification_report,confusion_matrix
from sklearn.svm import SVC
import warnings
warnings.filterwarnings('ignore')
123/11: pwd
126/1: from math import pi
126/2: print(pi)
126/3: int(pi)
126/4: pi - int(pi)
126/5: print("Geeks : %2d, Portal : %5.2f" % (1, 05.333))
126/6: print("Geeks : %2d, Portal : %4.2f" % (1, 05.333))
126/7: print("Geeks : %2d, Portal : %3.2f" % (1, 05.333))
126/8: print("Geeks : %2d, Portal : %2.2f" % (1, 05.333))
126/9: print("Geeks : %2d, Portal : %10.2f" % (1, 05.333))
126/10: print("Geeks : %2d, Portal : %10.6f" % (1, 05.333))
126/11:
after_dot = int(input("Enter a number that displays pi after dot"))

print(f"Pi : %2.{after_dot}" % (pi))
126/12: from math import pi
126/13: print(pi)
126/14: int(pi)
126/15: pi - int(pi)
126/16: print("Geeks : %2d, Portal : %10.6f" % (1, 05.333))
126/17:
after_dot = int(input("Enter a number that displays pi after dot"))

print(f"Pi : %2.{after_dot}" % (pi))
126/18:
after_dot = int(input("Enter a number that displays pi after dot"))

print(f"Pi : %d %2.{after_dot}" % (3,pi))
126/19:
after_dot = int(input("Enter a number that displays pi after dot"))

print(f"Pi : %d %2.{after_dot}f" % (3,pi))
126/20:
after_dot = int(input("Enter a number that displays pi after dot"))

print(f"Pi: %2.{after_dot}f" % (3,pi))
126/21:
after_dot = int(input("Enter a number that displays pi after dot"))

print(f"Pi: %2.{after_dot}f" % (3,pi))
126/22:
after_dot = int(input("Enter a number that displays pi after dot"))

print(f"Pi: %2.{after_dot}f" % (pi))
126/23:
after_dot = int(input("Enter a number that displays pi after dot"))

print(f"Pi: %2.{after_dot}f" % (pi))
126/24:
while True:  
    try:
        after_dot = int(input("Enter a number that displays pi after dot"))
    
    if after_dot >= 100:
        except:
            print("Number has to be below of 100")
            


print(f"Pi: %2.{after_dot}f" % (pi))
126/25:
while True:  
    try:
        after_dot = int(input("Enter a number that displays pi after dot"))

    except:
        if after_dot >=100:
            print(f"Number has to be below of {after_dot}")
            
    

print(f"Pi: %2.{after_dot}f" % (pi))
124/1:
a = ['a','b','c']
b = [1,2,3]

for element in zip(a,b):
    print(element)
124/2:
a = ['a','b','c']
b = [1,2,3]

for element in zip(a,b):
    print(element[0])
124/3:
a = [4,5,6]
b = [1,2,3]

for element in zip(a,b):
    print(element[0])
124/4:
def compareTriplets(a, b):
    alice = 0
    bob = 0
    for element in zip(a,b):
        if element[0] > element[1]:
            alice+=1
        elif element [0] < element[1]:
            bob+=1
        else:
            pass 
    return list([alice,bob])

compareTriplets(a,b
124/5:
def compareTriplets(a, b):
    alice = 0
    bob = 0
    for element in zip(a,b):
        if element[0] > element[1]:
            alice+=1
        elif element [0] < element[1]:
            bob+=1
        else:
            pass 
    return list([alice,bob])

compareTriplets(a,b)
124/6:
rows, cols = (5, 5)
arr = [[0]*cols]*rows
print(len(arr))
127/1:
a = [1,2,3,4,5]
type(a[1:3])
127/2:
a = [1,2,3,4,5]
sum(a[1:3])
127/3:
def miniMaxSum(arr):
    arr.sort()
    return sum(arr[0:len(arr)-1])
miniMaxSum([1,2,3,4,5])
127/4:
def miniMaxSum(arr):
    arr.sort()
    return sum(arr[0:len(arr)-1]), sum(arr[1:])
miniMaxSum([1,2,3,4,5])
128/1:
a = [4,5,6]
b = [1,2,3]

for element in zip(a,b):
    print(element[0])
128/2:
def compareTriplets(a, b):
    alice = 0
    bob = 0
    for element in zip(a,b):
        if element[0] > element[1]:
            alice+=1
        elif element [0] < element[1]:
            bob+=1
        else:
            pass 
    return list([alice,bob])

compareTriplets(a,b)
128/3:
rows, cols = (5, 5)
arr = [[0]*cols]*rows
print(len(arr))
128/4: print(arr)
131/1: print(test)
131/2:
# Run the "Hello World" in the cell below to print "Hello World". 
test = "Hello World"
131/3: print(test)
131/4: import numpy as np
131/5:
one_dimensional_arr = np.array([10, 12])
print(one_dimensional_arr)
131/6:
# Create an array with 3 integers, starting from the default integer 0.
b = np.arange(3)
print(b)
131/7:
lin_spaced_arr = np.linspace(0, 100, 5)
print(lin_spaced_arr)
131/8:
lin_spaced_arr_int = np.linspace(0, 100, 5, dtype=int)
print(lin_spaced_arr_int)
131/9:
b_float = np.arange(3, dtype=float)
print(b_float)
131/10:
char_arr = np.array(['Welcome to Math for ML!'])
print(char_arr)
print(char_arr.dtype) # Prints the data type of the array
131/11:
# Return a new array of shape 3, filled with ones. 
ones_arr = np.ones(3)
print(ones_arr)
131/12:
# Return a new array of shape 3, filled with ones. 
ones_arr = np.ones(3, dtype=int)
print(ones_arr)
131/13:
# Return a new array of shape 3, filled with ones. 
ones_arr = np.ones(3, dtype=char)
print(ones_arr)
131/14:
# Return a new array of shape 3, filled with ones. 
ones_arr = np.ones(3, dtype=b)
print(ones_arr)
131/15:
# Return a new array of shape 3, filled with ones. 
ones_arr = np.ones(3, dtype=float)
print(ones_arr)
131/16:
# Return a new array of shape 3, filled with zeroes.
zeros_arr = np.zeros(3)
print(zeros_arr)
131/17:
# Return a new array of shape 3, without initializing entries.
empt_arr = np.empty(3)
print(empt_arr)
131/18:
# Return a new array of shape 3 with random numbers between 0 and 1.
rand_arr = np.random.rand(3)
print(rand_arr)
131/19:
# Create a 2 dimensional array (2-D)
two_dim_arr = np.array([[1,2,3], [4,5,6]])
print(two_dim_arr)
131/20:
# 1-D array 
one_dim_arr = np.array([1, 2, 3, 4, 5, 6])

# Multidimensional array using reshape()
multi_dim_arr = np.reshape(
                    one_dim_arr, # the array to be reshaped
                    (2,3) # dimensions of the new array
                )
# Print the new 2-D array with two rows and three columns
print(multi_dim_arr)
131/21:
# Dimension of the 2-D array multi_dim_arr
multi_dim_arr.ndim
131/22:
# Shape of the 2-D array multi_dim_arr
# Returns shape of 2 rows and 3 columns
multi_dim_arr.shape
131/23:
# Shape of the 2-D array multi_dim_arr
# Returns shape of 2 rows and 3 columns
multi_dim_arr.shape
type(multi_dim_arr)
131/24:
# Size of the array multi_dim_arr
# Returns total number of elements
multi_dim_arr.size
133/1:
platform = {}

 a = [1,2,3,4,5]
 

for i in range(1,101):
    platform[f"p{i}"] = a
133/2:
platform = {}

a = [1,2,3,4,5]
 

for i in range(1,101):
    platform[f"p{i}"] = a
133/3: platform
135/1:
def myfunc(*args):
    print(args) # Any other keyword you want
    return sum(args) * 0.05
133/4:

def task1(platform=3,*pills):
    start = 0
    for i in pills:
        start += i
        if (start == platform):
             print(platform)
        else:
            continue
133/5:
from itertools import combinations

def find_shortest_path(platform, pills):
    for r in range(1, len(pills) + 1):
        # TÃ¼m kombinasyonlarÄ± kontrol et
        for combo in combinations(pills, r):
            if sum(combo) == platform:
                return combo

    return None

# Ãrnek kullanÄ±m
platform_value = 5
pills_list = [1, 2, 3, 4, 5]
result = find_shortest_path(platform_value, pills_list)

if result:
    print(f"En kÄ±sa yol: {result}")
else:
    print("UlaÅÄ±lamadÄ±.")
133/6:
from itertools import combinations

def find_combinations(platform, pills, num_of_numbers):
    count = 0
    for r in range(1, num_of_numbers + 1):
        for combo in combinations(pills, r):
            if sum(combo) == platform:
                print(f"Kombinasyon {count + 1}: {combo}")
                count += 1
    if count == 0:
        print("UlaÅÄ±lamadÄ±.")

# Ãrnek kullanÄ±m
platform_value = 5
pills_list = [2, 3, 1, 1, 4]
num_of_numbers_to_use = 2

find_combinations(platform_value, pills_list, num_of_numbers_to_use)
133/7:
def find_combinations(platform, pills, num_of_numbers):
    count = 0
    for i in range(1, 2**len(pills)):
        current_combination = [pills[0]]  # Her kombinasyona pills_list'in ilk elemanÄ±nÄ± ekliyoruz
        for j in range(len(pills)):
            if (i >> j) & 1:
                current_combination.append(pills[j + 1])

        if len(current_combination) == num_of_numbers and sum(current_combination) == platform:
            print(f"Kombinasyon {count + 1}: {current_combination}")
            count += 1

    if count == 0:
        print("UlaÅÄ±lamadÄ±.")

# Ãrnek kullanÄ±m
platform_value = 5
pills_list = [2, 3, 1, 1, 4]
num_of_numbers_to_use = 2

find_combinations(platform_value, pills_list, num_of_numbers_to_use)
133/8:
def find_shortest_path_helper(platform, pills, current_combo, remaining_numbers, index):
    if index == len(pills):
        return None

    # Mevcut kombinasyonu kontrol et
    current_combo.append(pills[index])

    if sum(current_combo) == platform and len(current_combo) == remaining_numbers:
        return current_combo

    # Ä°ndex'i artÄ±rarak recursive Ã§aÄrÄ± yap
    with_index = find_shortest_path_helper(platform, pills, current_combo.copy(), remaining_numbers, index + 1)
    without_index = find_shortest_path_helper(platform, pills, current_combo[:-1], remaining_numbers, index + 1)

    if with_index and without_index:
        # KÄ±yaslama yapÄ±p en kÄ±sa yolu seÃ§
        return with_index if len(with_index) < len(without_index) else without_index
    elif with_index:
        return with_index
    elif without_index:
        return without_index
    else:
        return None

def find_shortest_path(platform, pills):
    for num_of_numbers in range(1, len(pills) + 1):
        result = find_shortest_path_helper(platform, pills, [], num_of_numbers, 0)
        if result:
            return result

    return None

# Ãrnek kullanÄ±m
platform_value = 5
pills_list = [1, 2, 3, 4, 5]
result = find_shortest_path(platform_value, pills_list)

if result:
    print(f"En kÄ±sa yol: {result}")
else:
    print("UlaÅÄ±lamadÄ±.")
133/9:
def find_shortest_path_helper(platform, pills, current_combo, remaining_numbers, index):
    if index == len(pills):
        return None

    # Mevcut kombinasyonu kontrol et
    current_combo.append(pills[index])

    if sum(current_combo) == platform and len(current_combo) == remaining_numbers:
        return current_combo

    # Ä°ndex'i artÄ±rarak recursive Ã§aÄrÄ± yap
    with_index = find_shortest_path_helper(platform, pills, current_combo.copy(), remaining_numbers, index + 1)
    without_index = find_shortest_path_helper(platform, pills, current_combo[:-1], remaining_numbers, index + 1)

    if with_index and without_index:
        # KÄ±yaslama yapÄ±p en kÄ±sa yolu seÃ§
        return with_index if len(with_index) < len(without_index) else without_index
    elif with_index:
        return with_index
    elif without_index:
        return without_index
    else:
        return None

def find_shortest_path(platform, pills):
    for num_of_numbers in range(1, len(pills)):
        result = find_shortest_path_helper(platform, pills[1:], [pills[0]], num_of_numbers, 0)
        if result:
            return result

    return None

# Ãrnek kullanÄ±m
platform_value = 5
pills_list = [2, 3, 1, 1, 4]
result = find_shortest_path(platform_value, pills_list)

if result:
    print(f"En kÄ±sa yol: {result}")
else:
    print("UlaÅÄ±lamadÄ±.")
133/10:
def find_shortest_path_helper(platform, pills, current_combo, remaining_numbers, index):
    if index == len(pills):
        return None

    # Mevcut kombinasyonu kontrol et
    current_combo.append(pills[index])

    if sum(current_combo) == platform and len(current_combo) == remaining_numbers:
        return current_combo

    # Ä°ndex'i artÄ±rarak recursive Ã§aÄrÄ± yap
    with_index = find_shortest_path_helper(platform, pills, current_combo.copy(), remaining_numbers, index + 1)
    without_index = find_shortest_path_helper(platform, pills, current_combo[:-1], remaining_numbers, index + 1)

    if with_index and without_index:
        # KÄ±yaslama yapÄ±p en kÄ±sa yolu seÃ§
        return with_index if len(with_index) < len(without_index) else without_index
    elif with_index:
        return with_index
    elif without_index:
        return without_index
    else:
        return None

def find_shortest_path(platform, pills):
    for num_of_numbers in range(1, len(pills)):
        result = find_shortest_path_helper(platform, pills[1:], [pills[0]], num_of_numbers, 0)
        if result:
            return result

    return None

# Ãrnek kullanÄ±m
platform_value = 5
pills_list = [2, 4, 1, 1, 4]
result = find_shortest_path(platform_value, pills_list)

if result:
    print(f"En kÄ±sa yol: {result}")
else:
    print("UlaÅÄ±lamadÄ±.")
133/11:
def find_shortest_path_helper(platform, pills, current_combo, remaining_numbers, index):
    if index == len(pills):
        return None

    # Mevcut kombinasyonu kontrol et
    current_combo.append(pills[index])

    if sum(current_combo) == platform and len(current_combo) == remaining_numbers:
        return current_combo

    # Ä°ndex'i artÄ±rarak recursive Ã§aÄrÄ± yap
    with_index = find_shortest_path_helper(platform, pills, current_combo.copy(), remaining_numbers, index + 1)
    without_index = find_shortest_path_helper(platform, pills, current_combo[:-1], remaining_numbers, index + 1)

    if with_index and without_index:
        # KÄ±yaslama yapÄ±p en kÄ±sa yolu seÃ§
        return with_index if len(with_index) < len(without_index) else without_index
    elif with_index:
        return with_index
    elif without_index:
        return without_index
    else:
        return None

def find_shortest_path(platform, pills):
    for num_of_numbers in range(1, len(pills)):
        result = find_shortest_path_helper(platform, pills[1:], [pills[0]], num_of_numbers, 0)
        if result:
            return result

    return None

# Ãrnek kullanÄ±m
platform_value = 5
pills_list = [2, 4, 1, 2, 4]
result = find_shortest_path(platform_value, pills_list)

if result:
    print(f"En kÄ±sa yol: {result}")
else:
    print("UlaÅÄ±lamadÄ±.")
133/12:
def find_shortest_path_helper(platform, pills, current_combo, remaining_numbers, index):
    if index == len(pills):
        return []

    # Mevcut kombinasyonu kontrol et
    current_combo.append(pills[index])

    if sum(current_combo) == platform and len(current_combo) == remaining_numbers:
        return current_combo

    # Ä°ndex'i artÄ±rarak recursive Ã§aÄrÄ± yap
    with_index = find_shortest_path_helper(platform, pills, current_combo.copy(), remaining_numbers, index + 1)
    without_index = find_shortest_path_helper(platform, pills, current_combo[:-1], remaining_numbers, index + 1)

    if with_index and without_index:
        # KÄ±yaslama yapÄ±p en kÄ±sa yolu seÃ§
        return with_index if len(with_index) < len(without_index) else without_index
    elif with_index:
        return with_index
    elif without_index:
        return without_index
    else:
        return []

def find_shortest_path(platform, pills):
    for num_of_numbers in range(1, len(pills)):
        result = find_shortest_path_helper(platform, pills[1:], [pills[0]], num_of_numbers, 0)
        if result:
            return len(result)

    return 0

# Ãrnek kullanÄ±m
platform_value = 5
pills_list = [2, 3, 1, 1, 4]
result = find_shortest_path(platform_value, pills_list)

if result:
    print(f"En kÄ±sa yol: {result}")
else:
    print("UlaÅÄ±lamadÄ±.")
133/13:
def find_shortest_path_helper(platform, pills, current_combo, remaining_numbers, index):
    if index == len(pills):
        return []

    # Mevcut kombinasyonu kontrol et
    current_combo.append(pills[index])

    if sum(current_combo) == platform and len(current_combo) == remaining_numbers:
        return current_combo

    # Ä°ndex'i artÄ±rarak recursive Ã§aÄrÄ± yap
    with_index = find_shortest_path_helper(platform, pills, current_combo.copy(), remaining_numbers, index + 1)
    without_index = find_shortest_path_helper(platform, pills, current_combo[:-1], remaining_numbers, index + 1)

    if with_index and without_index:
        # KÄ±yaslama yapÄ±p en kÄ±sa yolu seÃ§
        return with_index if len(with_index) < len(without_index) else without_index
    elif with_index:
        return with_index
    elif without_index:
        return without_index
    else:
        return []

def find_shortest_path(platform, pills):
    for num_of_numbers in range(1, len(pills)):
        result = find_shortest_path_helper(platform, pills[1:], [pills[0]], num_of_numbers, 0)
        if result:
            return len(result)

    return 0

# Ãrnek kullanÄ±m
platform_value = 5
pills_list = [2, 3, 1, 2, 4]
result = find_shortest_path(platform_value, pills_list)

if result:
    print(f"En kÄ±sa yol: {result}")
else:
    print("UlaÅÄ±lamadÄ±.")
133/14:
def find_shortest_path_helper(platform, pills, current_combo, remaining_numbers, index):
    if index == len(pills):
        return []

    # Mevcut kombinasyonu kontrol et
    current_combo.append(pills[index])

    if sum(current_combo) == platform and len(current_combo) == remaining_numbers:
        return current_combo

    # Ä°ndex'i artÄ±rarak recursive Ã§aÄrÄ± yap
    with_index = find_shortest_path_helper(platform, pills, current_combo.copy(), remaining_numbers, index + 1)
    without_index = find_shortest_path_helper(platform, pills, current_combo[:-1], remaining_numbers, index + 1)

    if with_index and without_index:
        # KÄ±yaslama yapÄ±p en kÄ±sa yolu seÃ§
        return with_index if len(with_index) < len(without_index) else without_index
    elif with_index:
        return with_index
    elif without_index:
        return without_index
    else:
        return []

def find_shortest_path(platform, pills):
    for num_of_numbers in range(1, len(pills)):
        result = find_shortest_path_helper(platform, pills[1:], [pills[0]], num_of_numbers, 0)
        if result:
            return len(result)

    return 0

# Ãrnek kullanÄ±m
platform_value = 5
pills_list = [2, 4, 2, 1, 4]
result = find_shortest_path(platform_value, pills_list)

if result:
    print(f"En kÄ±sa yol: {result}")
else:
    print("UlaÅÄ±lamadÄ±.")
137/1:
plt.figure()
plt.hist(ar_A,6,range=(1,7),align='left',density=True, rwidth=0.8)
plt.figure()
plt.hist(ar_B,4,range=(1,5),align='left',density=True, rwidth=0.8)
plt.figure()
plt.hist(ar_C,3,range=(-1,2),align='left',density=True, rwidth=0.8)
plt.figure()
plt.hist(ar_X,14,range=(-3,11),align='left',density=True, rwidth=0.8)
137/2:
import numpy as np
import random
from matplotlib import pyplot as plt
137/3:
ar_A = []
ar_B = []
ar_C = []
ar_X = []
137/4:
av_A = []
av_B = []
av_C = []
av_X = []
vr_X = []
137/5:
plt.figure()
plt.hist(ar_A,6,range=(1,7),align='left',density=True, rwidth=0.8)
plt.figure()
plt.hist(ar_B,4,range=(1,5),align='left',density=True, rwidth=0.8)
plt.figure()
plt.hist(ar_C,3,range=(-1,2),align='left',density=True, rwidth=0.8)
plt.figure()
plt.hist(ar_X,14,range=(-3,11),align='left',density=True, rwidth=0.8)
138/1:
#%%
# Part b (Rejection Method)
Xb = []
av_Xb = []
vr_Xb = []
FX=[]
avg1=0
mean1=0
var1=0
# Populate the given arrays.
### YOUR CODE HERE ###
def f(x):
    if 0<=x<=1:
        fx=(x**2)
    else:
        fx=0
    return fx
for i in range(-200,200):
    FX.append(f(i/100))
a=0
b=1
c=0.6
j=0
while j <50000:
    u=np.random.rand()
    v=np.random.rand()
    x=(b-a)*u+a
    y=c*v
    if y<=f(x):
        Xb.append(x)
        j=j+1
        avg1=avg1+x
        av_Xb.append(avg1/(j+1))
        mean1=avg1/(j+1)
        var1=(1/(j+1))*((x-mean1)**2)
        vr_Xb.append(var1)
# Inspect the following plots.
plt.figure()
hXb = plt.hist(Xb,100,density=True)
plt.figure()
plt.plot(np.cumsum(hXb[0]))
# Plot the average and variance values.
### YOUR CODE HERE ###
#average
plt.figure()
av1 = plt.hist(av_Xb,density=True,bins=np.linspace(0,1,30))
#variance
plt.show()
plt.plot(vr_Xa)
139/1:
#%%
# Part b (Rejection Method)
Xb = []
av_Xb = []
vr_Xb = []
FX=[]
avg1=0
mean1=0
var1=0
# Populate the given arrays.
### YOUR CODE HERE ###
def f(x):
    if 0<=x<=1:
        fx=(x**2)
    else:
        fx=0
    return fx
for i in range(-200,200):
    FX.append(f(i/100))
a=0
b=1
c=0.6
j=0
while j <50000:
    u=np.random.rand()
    v=np.random.rand()
    x=(b-a)*u+a
    y=c*v
    if y<=f(x):
        Xb.append(x)
        j=j+1
        avg1=avg1+x
        av_Xb.append(avg1/(j+1))
        mean1=avg1/(j+1)
        var1=(1/(j+1))*((x-mean1)**2)
        vr_Xb.append(var1)
# Inspect the following plots.
plt.figure()
hXb = plt.hist(Xb,100,density=True)
plt.figure()
plt.plot(np.cumsum(hXb[0]))
# Plot the average and variance values.
### YOUR CODE HERE ###
#average
plt.figure()
av1 = plt.hist(av_Xb,density=True,bins=np.linspace(0,1,30))
#variance
plt.show()
plt.plot(vr_Xa)
139/2:
#%%
import numpy as np
from matplotlib import pyplot as plt

# Part a (Inverse Transform Method)
U = []
Xa = []
av_Xa = []
vr_Xa = []
avg=0 #average
mean=0 #mean
var=0 #variance
# Populate the given arrays.
### YOUR CODE HERE ###
for i in range(50000):
    u=np.random.rand()
    U.append(u)
    x=(np.sqrt(U[i]))
    Xa.append(x)
    avg=(avg+x)
    av_Xa.append(avg/(i+1))
    mean=avg/(i+1)
    var=(1/(i+1))*((x-mean)**2)
    vr_Xa.append(var)
#print(av_Xa)
# Inspect the following plots.
plt.figure()
for i in range(len(Xa)):
    plt.plot([Xa[i],U[i]],[1,1.2])
plt.figure()
hU = plt.hist(U,100,alpha=0.5,density=True)
hXa = plt.hist(Xa,100,alpha=0.5,density=True)
plt.figure()
plt.plot(np.cumsum(hU[0]))
plt.plot(np.cumsum(hXa[0]))
# Plot the average and variance values.
### YOUR CODE HERE ###
#average 
plt.figure()
av = plt.hist(av_Xa,density=True,bins=np.linspace(0,1,30,endpoint=False))
#variance
plt.show()
plt.plot(vr_Xa)
140/1:
#%%
# Part b (Rejection Method)
import numpy as np
from matplotlib import pyplot as plt
Xb = []
av_Xb = []
vr_Xb = []
FX=[]
avg1=0
mean1=0
var1=0
# Populate the given arrays.
### YOUR CODE HERE ###
def f(x):
    if 0<=x<=1:
        fx=(x**2)
    else:
        fx=0
    return fx
for i in range(-200,200):
    FX.append(f(i/100))
a=0
b=1
c=0.6
j=0
while j <50000:
    u=np.random.rand()
    v=np.random.rand()
    x=(b-a)*u+a
    y=c*v
    if y<=f(x):
        Xb.append(x)
        j=j+1
        avg1=avg1+x
        av_Xb.append(avg1/(j+1))
        mean1=avg1/(j+1)
        var1=(1/(j+1))*((x-mean1)**2)
        vr_Xb.append(var1)
# Inspect the following plots.
plt.figure()
hXb = plt.hist(Xb,100,density=True)
plt.figure()
plt.plot(np.cumsum(hXb[0]))
# Plot the average and variance values.
### YOUR CODE HERE ###
#average
plt.figure()
av1 = plt.hist(av_Xb,density=True,bins=np.linspace(0,1,30))
#variance
plt.show()
plt.plot(vr_Xa)
140/2:
#%%
import numpy as np
from matplotlib import pyplot as plt

# Part a (Inverse Transform Method)
U = []
Xa = []
av_Xa = []
vr_Xa = []
avg=0 #average
mean=0 #mean
var=0 #variance
# Populate the given arrays.
### YOUR CODE HERE ###
for i in range(50000):
    u=np.random.rand()
    U.append(u)
    x=(np.sqrt(U[i]))
    Xa.append(x)
    avg=(avg+x)
    av_Xa.append(avg/(i+1))
    mean=avg/(i+1)
    var=(1/(i+1))*((x-mean)**2)
    vr_Xa.append(var)
#print(av_Xa)
# Inspect the following plots.
plt.figure()
for i in range(len(Xa)):
    plt.plot([Xa[i],U[i]],[1,1.2])
plt.figure()
hU = plt.hist(U,100,alpha=0.5,density=True)
hXa = plt.hist(Xa,100,alpha=0.5,density=True)
plt.figure()
plt.plot(np.cumsum(hU[0]))
plt.plot(np.cumsum(hXa[0]))
# Plot the average and variance values.
### YOUR CODE HERE ###
#average 
plt.figure()
av = plt.hist(av_Xa,density=True,bins=np.linspace(0,1,30,endpoint=False))
#variance
plt.show()
plt.plot(vr_Xa)
140/3:
#%%
import numpy as np
from matplotlib import pyplot as plt

# Part a (Inverse Transform Method)
U = []
Xa = []
av_Xa = []
vr_Xa = []
avg=0 #average
mean=0 #mean
var=0 #variance
# Populate the given arrays.
### YOUR CODE HERE ###
for i in range(50000):
    u=np.random.rand()
    U.append(u)
    x=(np.sqrt(U[i]))
    Xa.append(x)
    avg=(avg+x)
    av_Xa.append(avg/(i+1))
    mean=avg/(i+1)
    var=(1/(i+1))*((x-mean)**2)
    vr_Xa.append(var)
#print(av_Xa)
# Inspect the following plots.
plt.figure()
for i in range(len(Xa)):
    plt.plot([Xa[i],U[i]],[1,1.2])
plt.figure()
hU = plt.hist(U,100,alpha=0.5,density=True)
hXa = plt.hist(Xa,100,alpha=0.5,density=True)
plt.figure()
plt.plot(np.cumsum(hU[0]))
plt.plot(np.cumsum(hXa[0]))
# Plot the average and variance values.
### YOUR CODE HERE ###
#average 
plt.figure()
av = plt.hist(av_Xa,density=True,bins=np.linspace(0,1,30,endpoint=False))
#variance
plt.show()
plt.plot(vr_Xa)
141/1:
#%%
# Part b (Rejection Method)
Xb = []
av_Xb = []
vr_Xb = []

# Populate the given arrays.
### YOUR CODE HERE ###



def F(x):
    return x**2

for i in range(30000):

    while True:
        
        x = random.random()
        fx = F(x)

        y = random.random()

        if y<=fx:
            Xb.append(x)
            break

    av_Xb.append(np.mean(Xb))
    vr_Xb.append(np.var(Xb))

# Inspect the following plots.
plt.figure()
hXb = plt.hist(Xb,100,normed=True)
plt.figure()
plt.plot(np.cumsum(hXb[0]))

# Plot the average and variance values.
### YOUR CODE HERE ###

#Average
plt.figure()
plt.plot(av_Xb)

#Variance
plt.figure()
plt.plot(vr_Xb)
141/2:
#%%
import numpy as np
import random
from matplotlib import pyplot as plt

# Part b (Rejection Method)
Xb = []
av_Xb = []
vr_Xb = []

# Populate the given arrays.
### YOUR CODE HERE ###



def F(x):
    return x**2

for i in range(30000):

    while True:
        
        x = random.random()
        fx = F(x)

        y = random.random()

        if y<=fx:
            Xb.append(x)
            break

    av_Xb.append(np.mean(Xb))
    vr_Xb.append(np.var(Xb))

# Inspect the following plots.
plt.figure()
hXb = plt.hist(Xb,100,normed=True)
plt.figure()
plt.plot(np.cumsum(hXb[0]))

# Plot the average and variance values.
### YOUR CODE HERE ###

#Average
plt.figure()
plt.plot(av_Xb)

#Variance
plt.figure()
plt.plot(vr_Xb)
141/3:
#%%
import numpy as np
import random
from matplotlib import pyplot as plt

# Part b (Rejection Method)
Xb = []
av_Xb = []
vr_Xb = []

# Populate the given arrays.
### YOUR CODE HERE ###



def F(x):
    return x**2

for i in range(30000):

    while True:
        
        x = random.random()
        fx = F(x)

        y = random.random()

        if y<=fx:
            Xb.append(x)
            break

    av_Xb.append(np.mean(Xb))
    vr_Xb.append(np.var(Xb))

# Inspect the following plots.
plt.figure()
hXb = plt.hist(Xb,100,density=True)
plt.figure()
plt.plot(np.cumsum(hXb[0]))

# Plot the average and variance values.
### YOUR CODE HERE ###

#Average
plt.figure()
plt.plot(av_Xb)

#Variance
plt.figure()
plt.plot(vr_Xb)
142/1:
#%%
# Part b (Rejection Method)
import numpy as np
from matplotlib import pyplot as plt
Xb = []
av_Xb = []
vr_Xb = []
FX=[]
avg1=0
mean1=0
var1=0
# Populate the given arrays.
### YOUR CODE HERE ###
def f(x):
    if 0<=x<=1:
        fx=(x**2)
    else:
        fx=0
    return fx
for i in range(-200,200):
    FX.append(f(i/100))
a=0
b=1
c=0.6
j=0
while j <50000:
    u=np.random.rand()
    v=np.random.rand()
    x=(b-a)*u+a
    y=c*v
    if y<=f(x):
        Xb.append(x)
        j=j+1
        avg1=avg1+x
        av_Xb.append(avg1/(j+1))
        mean1=avg1/(j+1)
        var1=(1/(j+1))*((x-mean1)**2)
        vr_Xb.append(var1)
# Inspect the following plots.
plt.figure()
hXb = plt.hist(Xb,100,density=True)
plt.figure()
plt.plot(np.cumsum(hXb[0]))
# Plot the average and variance values.
### YOUR CODE HERE ###
#average
plt.figure()
av1 = plt.hist(av_Xb,density=True,bins=np.linspace(0,1,30))
#variance
plt.show()
plt.plot(vr_Xa)
143/1:
#%%
# Part b (Rejection Method)
import numpy as np
from matplotlib import pyplot as plt
Xb = []
av_Xb = []
vr_Xb = []
FX=[]
avg1=0
mean1=0
var1=0
# Populate the given arrays.
### YOUR CODE HERE ###
def f(x):
    if 0<=x<=1:
        fx=(x**2)
    else:
        fx=0
    return fx
for i in range(-200,200):
    FX.append(f(i/100))
a=0
b=1
c=0.6
j=0
while j <50000:
    u=np.random.rand()
    v=np.random.rand()
    x=(b-a)*u+a
    y=c*v
    if y<=f(x):
        Xb.append(x)
        j=j+1
        avg1=avg1+x
        av_Xb.append(avg1/(j+1))
        mean1=avg1/(j+1)
        var1=(1/(j+1))*((x-mean1)**2)
        vr_Xb.append(var1)
# Inspect the following plots.
plt.figure()
hXb = plt.hist(Xb,100,density=True)
plt.figure()
plt.plot(np.cumsum(hXb[0]))
# Plot the average and variance values.
### YOUR CODE HERE ###
#average
plt.figure()
av1 = plt.hist(av_Xb,density=True,bins=np.linspace(0,1,30))
#variance
plt.show()
plt.plot(vr_Xb)
144/1:
#%%
# Part b (Rejection Method)
import numpy as np
from matplotlib import pyplot as plt
Xb = []
av_Xb = []
vr_Xb = []
FX=[]
avg1=0
mean1=0
var1=0
# Populate the given arrays.
### YOUR CODE HERE ###
def f(x):
    if 0<=x<=1:
        fx=(x**2)
    else:
        fx=0
    return fx
for i in range(-200,200):
    FX.append(f(i/100))
a=0
b=1
c=0.6
j=0
while j <50000:
    u=np.random.rand()
    v=np.random.rand()
    x=(b-a)*u+a
    y=c*v
    if y<=f(x):
        Xb.append(x)
        j=j+1
        avg1=avg1+x
        av_Xb.append(avg1/(j+1))
        mean1=avg1/(j+1)
        var1=(1/(j+1))*((x-mean1)**2)
        vr_Xb.append(var1)
# Inspect the following plots.
plt.figure()
hXb = plt.hist(Xb,100,density=True)
plt.figure()
plt.plot(np.cumsum(hXb[0]))
# Plot the average and variance values.
### YOUR CODE HERE ###
#average
plt.figure()
av1 = plt.hist(av_Xb,density=True,bins=30)
#variance
plt.show()
plt.plot(vr_Xb)
144/2:
#%%
# Part b (Rejection Method)
import numpy as np
from matplotlib import pyplot as plt
Xb = []
av_Xb = []
vr_Xb = []
FX=[]
avg1=0
mean1=0
var1=0
# Populate the given arrays.
### YOUR CODE HERE ###
def f(x):
    if 0<=x<=1:
        fx=(x**2)
    else:
        fx=0
    return fx
for i in range(-200,200):
    FX.append(f(i/100))
a=0
b=1
c=0.6
j=0
while j <50000:
    u=np.random.rand()
    v=np.random.rand()
    x=(b-a)*u+a
    y=c*v
    if y<=f(x):
        Xb.append(x)
        j=j+1
        avg1=avg1+x
        av_Xb.append(avg1/(j+1))
        mean1=avg1/(j+1)
        var1=(1/(j+1))*((x-mean1)**2)
        vr_Xb.append(var1)
# Inspect the following plots.
plt.figure()
hXb = plt.hist(Xb,100,density=True)
plt.figure()
plt.plot(np.cumsum(hXb[0]))
# Plot the average and variance values.
### YOUR CODE HERE ###
#average
plt.figure()
plt.plot(av_Xa)
#variance
plt.show()
plt.plot(vr_Xb)
144/3:
#%%
# Part b (Rejection Method)
import numpy as np
from matplotlib import pyplot as plt
Xb = []
av_Xb = []
vr_Xb = []
FX=[]
avg1=0
mean1=0
var1=0
# Populate the given arrays.
### YOUR CODE HERE ###
def f(x):
    if 0<=x<=1:
        fx=(x**2)
    else:
        fx=0
    return fx
for i in range(-200,200):
    FX.append(f(i/100))
a=0
b=1
c=0.6
j=0
while j <50000:
    u=np.random.rand()
    v=np.random.rand()
    x=(b-a)*u+a
    y=c*v
    if y<=f(x):
        Xb.append(x)
        j=j+1
        avg1=avg1+x
        av_Xb.append(avg1/(j+1))
        mean1=avg1/(j+1)
        var1=(1/(j+1))*((x-mean1)**2)
        vr_Xb.append(var1)
# Inspect the following plots.
plt.figure()
hXb = plt.hist(Xb,100,density=True)
plt.figure()
plt.plot(np.cumsum(hXb[0]))
# Plot the average and variance values.
### YOUR CODE HERE ###
#average
plt.figure()
plt.plot(av_Xb)
#variance
plt.show()
plt.plot(vr_Xb)
145/1:
# %% Imports
import random
import numpy as np
from matplotlib import pyplot as plt
145/2:
# %% Functions

# Function to generate a population with given parameter and size using the
# inverse transformation method.
def gen_inverse(k, M):
    u = [random.random() for i in range(M)]
    return [x ** (1 /(k+1)) for x in u] # pdf to cdf 

# Function to generate a population with given parameter and size using the
# rejection method.
def gen_rejection(k, M):
    sample_array = []
    c = k + 1

    while len(sample_array) < M:
        u1 = random.random()
        u2 = random.random()

        if u2 <= (k+1) * u1**k:
            sample_array.append(u1)
        
    return np.array(sample_array)
 

# Function to calculate the population mean using k.
def calc_population_mean(k):
    return (k+1) / (k+2)



# Function to calculate the population variance using k.
def calc_population_variance(k):
    mean = calc_population_mean(k)
    return (k + 1) / (k + 3) - mean**2 # Answer of E(X^2) - (E(x))^2


# Function to randomly take samples of size N from a population.
def random_sample(population, N):
    return np.array(random.sample(list(population),N))


# Function to calculate the sample mean.
def calc_sample_mean(sample):
    return np.mean(sample)


# Function to calculate the sample variance (biased/unbiased).
def calc_sample_variance(sample, unbiased=True):
    return np.var(sample)


# Function to estimate the parameter k using method of moments
def estimate_k_mom(sample):
    sample_mean = calc_sample_mean(sample)
    return (sample_mean * (2 - sample_mean)) / (sample_mean -1) 


# Function to estimate the parameter k using maximum likelihood
def estimate_k_mle(sample):
    log_sum = np.sum(np.log(sample))
    return len(sample) / log_sum -1 


# Function to calculate the confidence interval for population mean given the
# sample and the required confidence level. If population standard deviation is
# not provided, use sample standard deviation as its estimator. As confidence
# level, it should only accept 95, 96, 97, 98 and 99 for which the z values are
# hard-coded in the function.
def calc_conf_int_mean(sample, confidence_lvl, pop_std=0):
    z_values = {95: 1.96, 96: 2.05, 97: 2.17, 98: 2.33, 99: 2.58}
    sample_mean = calc_sample_mean(sample)
    std_dev = pop_std if pop_std else np.std(sample,ddof=1)
    margin_error = z_values[confidence_lvl] * (std_dev / np.sqrt(len(sample)))
    
    return sample_mean - margin_error, sample_mean + margin_error
145/3:
# %% Functions

# Function to generate a population with given parameter and size using the
# inverse transformation method.
def gen_inverse(k, M):
    u = [random.random() for i in range(M)]
    return [x ** (1 /(k+1)) for x in u] # pdf to cdf 

# Function to generate a population with given parameter and size using the
# rejection method.
def gen_rejection(k, M):
    sample_array = []
    c = k + 1

    while len(sample_array) < M:
        u1 = random.random()
        u2 = random.random()

        if u2 <= (k+1) * u1**k:
            sample_array.append(u1)
        
    return np.array(sample_array)
 

# Function to calculate the population mean using k.
def calc_population_mean(k):
    return (k+1) / (k+2)



# Function to calculate the population variance using k.
def calc_population_variance(k):
    mean = calc_population_mean(k)
    return (k + 1) / (k + 3) - mean**2 # Answer of E(X^2) - (E(x))^2


# Function to randomly take samples of size N from a population.
def random_sample(population, N):
    return np.array(random.sample(list(population),N))


# Function to calculate the sample mean.
def calc_sample_mean(sample):
    return np.mean(sample)


# Function to calculate the sample variance (biased/unbiased).
def calc_sample_variance(sample, unbiased=True):
    return np.var(sample)


# Function to estimate the parameter k using method of moments
def estimate_k_mom(sample):
    sample_mean = calc_sample_mean(sample)
    return (sample_mean * (2 - sample_mean)) / (sample_mean -1) 


# Function to estimate the parameter k using maximum likelihood
def estimate_k_mle(sample):
    log_sum = np.sum(np.log(sample))
    return len(sample) / log_sum -1 


# Function to calculate the confidence interval for population mean given the
# sample and the required confidence level. If population standard deviation is
# not provided, use sample standard deviation as its estimator. As confidence
# level, it should only accept 95, 96, 97, 98 and 99 for which the z values are
# hard-coded in the function.
def calc_conf_int_mean(sample, confidence_lvl, pop_std=0):
    z_values = {95: 1.96, 96: 2.05, 97: 2.17, 98: 2.33, 99: 2.58}
    sample_mean = calc_sample_mean(sample)
    std_dev = pop_std if pop_std else np.std(sample,ddof=1)
    margin_error = z_values[confidence_lvl] * (std_dev / np.sqrt(len(sample)))
    
    return sample_mean - margin_error, sample_mean + margin_error
145/4:
# %% Experiments

# Generate the two populations of size 1000000, calculate and print their means
# and variances and plot the population histograms.
M = 1000000
k_1 = 2.1
k_2 = 3.7
conf_lvl = 97


# Inverse Transformation Method for first population
pop_1 = gen_inverse(k_1,M)

# Rejection Method for second population
pop_2 = gen_rejection(k_2,M)

# Calculation mean and variances
mean_1 = calc_population_mean(k_1)
var_1 = calc_population_variance(k_1)
mean_2 = calc_population_mean(k_2)
var_2 = calc_population_variance(k_2)

print(f"Population 1 - Mean: {mean_1}, Variance: {var_1}")
print(f"Population 2 - Mean: {mean_2}, Variance: {var_2}")


plt.figure()
plt.subplot(2, 1, 1)
plt.hist(pop_1, bins=50, alpha=0.5, color='blue', label=f'Population 1 (k={k_1})')
plt.legend()
plt.title('Population Histograms')
plt.xlabel('Value')
plt.ylabel('Frequency')

plt.subplot(2, 1, 2)
plt.hist(pop_2, bins=50, alpha=0.5, color='orange', label=f'Population 2 (k={k_2})')
plt.legend()
plt.xlabel('Value')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

# Collect 100000 random samples of size 25 from both populations, calculate
# sample means, biased and unbiased sample variances, MoM and MLE estimates of
# the parameter k and population mean intervals with 97% confidence with and
# without the population standard deviation for each sample of each population.
N = 25
R = 100000

samples_pop1 = random_sample(pop_1,N)
samples_pop2 = random_sample(pop_2,N)


# Initialize arrays for Population 1
means_pop1 = np.zeros(R)
var_biased_pop1 = np.zeros(R)
var_unbiased_pop1 = np.zeros(R)
k_mom_pop1 = np.zeros(R)
k_mle_pop1 = np.zeros(R)
mean_conf_int_pop1 = np.zeros((R, 2))


# Initialize arrays for Population 2
means_pop2 = np.zeros(R)
var_biased_pop2 = np.zeros(R)
var_unbiased_pop2 = np.zeros(R)
k_mom_pop2 = np.zeros(R)
k_mle_pop2 = np.zeros(R)
mean_conf_int_pop2 = np.zeros((R, 2))

for i, sample in enumerate(samples_pop1):
    means_pop1[i] = calc_sample_mean(sample)
    var_biased_pop1[i] = calc_sample_variance(sample, unbiased=False)
    var_unbiased_pop1[i] = calc_sample_variance(sample, unbiased=True)
    k_mom_pop1[i] = estimate_k_mom(sample)
    k_mle_pop1[i] = estimate_k_mle(sample)
    mean_conf_int_pop1[i] = calc_conf_int_mean(sample, confidence_lvl=97)

for i, sample in enumerate(samples_pop2):
    means_pop2[i] = calc_sample_mean(sample)
    var_biased_pop2[i] = calc_sample_variance(sample, unbiased=False)
    var_unbiased_pop2[i] = calc_sample_variance(sample, unbiased=True)
    k_mom_pop2[i] = estimate_k_mom(sample)
    k_mle_pop2[i] = estimate_k_mle(sample)
    mean_conf_int_pop2[i] = calc_conf_int_mean(sample, confidence_lvl=97)


# Print the results for Population 1
print("Population 1 Results:")
print(f"Sample Mean: {np.mean(means_pop1)}")
print(f"Biased Sample Variance: {np.mean(var_biased_pop1)}")
print(f"Unbiased Sample Variance: {np.mean(var_unbiased_pop1)}")
print(f"MoM Estimate of k: {np.mean(k_mom_pop1)}")
print(f"MLE Estimate of k: {np.mean(k_mle_pop1)}")
print(f"Mean Confidence Interval (97%): {np.mean(mean_conf_int_pop1, axis=0)}")

# Print the results for Population 2
print("\nPopulation 2 Results:")
print(f"Sample Mean: {np.mean(means_pop2)}")
print(f"Biased Sample Variance: {np.mean(var_biased_pop2)}")
print(f"Unbiased Sample Variance: {np.mean(var_unbiased_pop2)}")
print(f"MoM Estimate of k: {np.mean(k_mom_pop2)}")
print(f"MLE Estimate of k: {np.mean(k_mle_pop2)}")
print(f"Mean Confidence Interval (97%): {np.mean(mean_conf_int_pop2, axis=0)}")

# YOUR CODE HERE

# Calculate and print means of sample means, biased and unbiased sample
# variances, MoM and MLE estimates of parameter k and plot the histograms of
# sample means, k estimates using MoM and MLE for both populations.

# YOUR CODE HERE

#plt.figure()
# YOUR CODE HERE

#plt.figure()
# YOUR CODE HERE

#plt.figure()
# YOUR CODE HERE

# Calculate and print the ratio of confidence intervals computed with and
# without using the population standard deviation that contains the population
# mean for both populations.

# YOUR CODE HERE

print('*'*50)
# Collect a sample of length 100000*25 from both populations, calculate and
# print their sample means, biased and unbiased sample variances, MoM and MLE
# estimates of parameter k and confidence intervals with and without using the
# population standard deviation.

# YOUR CODE HERE
146/1:
# %% Imports
import random
import numpy as np
from matplotlib import pyplot as plt
146/2:
# %% Functions

# Function to generate a population with given parameter and size using the
# inverse transformation method.
def gen_inverse(k, M):
    u = [random.random() for i in range(M)]
    return [x ** (1 /(k+1)) for x in u] # pdf to cdf 

# Function to generate a population with given parameter and size using the
# rejection method.
def gen_rejection(k, M):
    sample_array = []
    c = k + 1

    while len(sample_array) < M:
        u1 = random.random()
        u2 = random.random()

        if u2 <= (k+1) * u1**k:
            sample_array.append(u1)
        
    return np.array(sample_array)
 

# Function to calculate the population mean using k.
def calc_population_mean(k):
    return (k+1) / (k+2)



# Function to calculate the population variance using k.
def calc_population_variance(k):
    mean = calc_population_mean(k)
    return (k + 1) / (k + 3) - mean**2 # Answer of E(X^2) - (E(x))^2


# Function to randomly take samples of size N from a population.
def random_sample(population, N):
    return np.array(random.sample(list(population),N))


# Function to calculate the sample mean.
def calc_sample_mean(sample):
    return np.mean(sample)


# Function to calculate the sample variance (biased/unbiased).
def calc_sample_variance(sample, unbiased=True):
    return np.var(sample)


# Function to estimate the parameter k using method of moments
def estimate_k_mom(sample):
    sample_mean = calc_sample_mean(sample)
    return (sample_mean * (2 - sample_mean)) / (sample_mean -1) 


# Function to estimate the parameter k using maximum likelihood
def estimate_k_mle(sample):
    log_sum = np.sum(np.log(sample))
    return len(sample) / log_sum -1 


# Function to calculate the confidence interval for population mean given the
# sample and the required confidence level. If population standard deviation is
# not provided, use sample standard deviation as its estimator. As confidence
# level, it should only accept 95, 96, 97, 98 and 99 for which the z values are
# hard-coded in the function.
def calc_conf_int_mean(sample, confidence_lvl, pop_std=0):
    z_values = {95: 1.96, 96: 2.05, 97: 2.17, 98: 2.33, 99: 2.58}
    sample_mean = calc_sample_mean(sample)
    std_dev = pop_std if pop_std else np.std(sample,ddof=1)
    margin_error = z_values[confidence_lvl] * (std_dev / np.sqrt(len(sample)))
    
    return sample_mean - margin_error, sample_mean + margin_error
146/3:
# %% Experiments

# Generate the two populations of size 1000000, calculate and print their means
# and variances and plot the population histograms.
M = 1000000
k_1 = 2.1
k_2 = 3.7
conf_lvl = 97


# Inverse Transformation Method for first population
pop_1 = gen_inverse(k_1,M)

# Rejection Method for second population
pop_2 = gen_rejection(k_2,M)

# Calculation mean and variances
mean_1 = calc_population_mean(k_1)
var_1 = calc_population_variance(k_1)
mean_2 = calc_population_mean(k_2)
var_2 = calc_population_variance(k_2)

print(f"Population 1 - Mean: {mean_1}, Variance: {var_1}")
print(f"Population 2 - Mean: {mean_2}, Variance: {var_2}")


plt.figure()
plt.subplot(2, 1, 1)
plt.hist(pop_1, bins=50, alpha=0.5, color='blue', label=f'Population 1 (k={k_1})')
plt.legend()
plt.title('Population Histograms')
plt.xlabel('Value')
plt.ylabel('Frequency')

plt.subplot(2, 1, 2)
plt.hist(pop_2, bins=50, alpha=0.5, color='orange', label=f'Population 2 (k={k_2})')
plt.legend()
plt.xlabel('Value')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

# Collect 100000 random samples of size 25 from both populations, calculate
# sample means, biased and unbiased sample variances, MoM and MLE estimates of
# the parameter k and population mean intervals with 97% confidence with and
# without the population standard deviation for each sample of each population.
N = 25
R = 100000


# Initialize arrays for Population 1
means_pop1 = np.zeros(R)
var_biased_pop1 = np.zeros(R)
var_unbiased_pop1 = np.zeros(R)
k_mom_pop1 = np.zeros(R)
k_mle_pop1 = np.zeros(R)
mean_conf_int_pop1 = np.zeros((R, 2))


# Initialize arrays for Population 2
means_pop2 = np.zeros(R)
var_biased_pop2 = np.zeros(R)
var_unbiased_pop2 = np.zeros(R)
k_mom_pop2 = np.zeros(R)
k_mle_pop2 = np.zeros(R)
mean_conf_int_pop2 = np.zeros((R, 2))

for i in range(R):
    sample_pop1 = random_sample(pop_1, N)
    sample_pop2 = random_sample(pop_2, N)
    
    means_pop1[i] = calc_sample_mean(sample_pop1)
    var_biased_pop1[i] = calc_sample_variance(sample_pop1, unbiased=False)
    var_unbiased_pop1[i] = calc_sample_variance(sample_pop1, unbiased=True)
    k_mom_pop1[i] = estimate_k_mom(sample_pop1)
    k_mle_pop1[i] = estimate_k_mle(sample_pop1)
    mean_conf_int_pop1[i] = calc_conf_int_mean(sample_pop1, conf_lvl)
    
    means_pop2[i] = calc_sample_mean(sample_pop2)
    var_biased_pop2[i] = calc_sample_variance(sample_pop2, unbiased=False)
    var_unbiased_pop2[i] = calc_sample_variance(sample_pop2, unbiased=True)
    k_mom_pop2[i] = estimate_k_mom(sample_pop2)
    k_mle_pop2[i] = estimate_k_mle(sample_pop2)
    mean_conf_int_pop2[i] = calc_conf_int_mean(sample_pop2, conf_lvl)

'''
for i, sample in enumerate(samples_pop1):
    means_pop1[i] = calc_sample_mean(sample)
    var_biased_pop1[i] = calc_sample_variance(sample, unbiased=False)
    var_unbiased_pop1[i] = calc_sample_variance(sample, unbiased=True)
    k_mom_pop1[i] = estimate_k_mom(sample)
    k_mle_pop1[i] = estimate_k_mle(sample)
    mean_conf_int_pop1[i] = calc_conf_int_mean(sample, confidence_lvl=97)

for i, sample in enumerate(samples_pop2):
    means_pop2[i] = calc_sample_mean(sample)
    var_biased_pop2[i] = calc_sample_variance(sample, unbiased=False)
    var_unbiased_pop2[i] = calc_sample_variance(sample, unbiased=True)
    k_mom_pop2[i] = estimate_k_mom(sample)
    k_mle_pop2[i] = estimate_k_mle(sample)
    mean_conf_int_pop2[i] = calc_conf_int_mean(sample, confidence_lvl=97)
'''

# Print the results for Population 1
print("Population 1 Results:")
print(f"Sample Mean: {np.mean(means_pop1)}")
print(f"Biased Sample Variance: {np.mean(var_biased_pop1)}")
print(f"Unbiased Sample Variance: {np.mean(var_unbiased_pop1)}")
print(f"MoM Estimate of k: {np.mean(k_mom_pop1)}")
print(f"MLE Estimate of k: {np.mean(k_mle_pop1)}")
print(f"Mean Confidence Interval (97%): {np.mean(mean_conf_int_pop1, axis=0)}")

# Print the results for Population 2
print("\nPopulation 2 Results:")
print(f"Sample Mean: {np.mean(means_pop2)}")
print(f"Biased Sample Variance: {np.mean(var_biased_pop2)}")
print(f"Unbiased Sample Variance: {np.mean(var_unbiased_pop2)}")
print(f"MoM Estimate of k: {np.mean(k_mom_pop2)}")
print(f"MLE Estimate of k: {np.mean(k_mle_pop2)}")
print(f"Mean Confidence Interval (97%): {np.mean(mean_conf_int_pop2, axis=0)}")

# YOUR CODE HERE

# Calculate and print means of sample means, biased and unbiased sample
# variances, MoM and MLE estimates of parameter k and plot the histograms of
# sample means, k estimates using MoM and MLE for both populations.

# YOUR CODE HERE

#plt.figure()
# YOUR CODE HERE

#plt.figure()
# YOUR CODE HERE

#plt.figure()
# YOUR CODE HERE

# Calculate and print the ratio of confidence intervals computed with and
# without using the population standard deviation that contains the population
# mean for both populations.

# YOUR CODE HERE

print('*'*50)
# Collect a sample of length 100000*25 from both populations, calculate and
# print their sample means, biased and unbiased sample variances, MoM and MLE
# estimates of parameter k and confidence intervals with and without using the
# population standard deviation.

# YOUR CODE HERE
146/4:
# %% Experiments

# Generate the two populations of size 1000000, calculate and print their means
# and variances and plot the population histograms.
M = 1000000
k_1 = 2.1
k_2 = 3.7
conf_lvl = 97


# Inverse Transformation Method for first population
pop_1 = gen_inverse(k_1,M)

# Rejection Method for second population
pop_2 = gen_rejection(k_2,M)

# Calculation mean and variances
mean_1 = calc_population_mean(k_1)
var_1 = calc_population_variance(k_1)
mean_2 = calc_population_mean(k_2)
var_2 = calc_population_variance(k_2)

print(f"Population 1 - Mean: {mean_1}, Variance: {var_1}")
print(f"Population 2 - Mean: {mean_2}, Variance: {var_2}")


plt.figure()
plt.subplot(2, 1, 1)
plt.hist(pop_1, bins=50, alpha=0.5, color='blue', label=f'Population 1 (k={k_1})')
plt.legend()
plt.title('Population Histograms')
plt.xlabel('Value')
plt.ylabel('Frequency')

plt.subplot(2, 1, 2)
plt.hist(pop_2, bins=50, alpha=0.5, color='orange', label=f'Population 2 (k={k_2})')
plt.legend()
plt.xlabel('Value')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

# Collect 100000 random samples of size 25 from both populations, calculate
# sample means, biased and unbiased sample variances, MoM and MLE estimates of
# the parameter k and population mean intervals with 97% confidence with and
# without the population standard deviation for each sample of each population.
N = 25
R = 100000

samples_pop1 = np.zeros((R, N))
samples_pop2 = np.zeros((R, N))

for i in range(R):
    samples_pop1[i] = random_sample(pop_1, N)
    samples_pop2[i] = random_sample(pop_2, N)

# Initialize arrays for Population 1
means_pop1 = np.mean(samples_pop1, axis=1)
var_biased_pop1 = np.var(samples_pop1, axis=1)
var_unbiased_pop1 = np.var(samples_pop1, axis=1, ddof=1)
k_mom_pop1 = np.array([estimate_k_mom(sample) for sample in samples_pop1])
k_mle_pop1 = np.array([estimate_k_mle(sample) for sample in samples_pop1])
mean_conf_int_pop1 = np.array([calc_conf_int_mean(sample, conf_lvl) for sample in samples_pop1])

# Initialize arrays for Population 2
means_pop2 = np.mean(samples_pop2, axis=1)
var_biased_pop2 = np.var(samples_pop2, axis=1)
var_unbiased_pop2 = np.var(samples_pop2, axis=1, ddof=1)
k_mom_pop2 = np.array([estimate_k_mom(sample) for sample in samples_pop2])
k_mle_pop2 = np.array([estimate_k_mle(sample) for sample in samples_pop2])
mean_conf_int_pop2 = np.array([calc_conf_int_mean(sample, conf_lvl) for sample in samples_pop2])


# YOUR CODE HERE

# Calculate and print means of sample means, biased and unbiased sample
# variances, MoM and MLE estimates of parameter k and plot the histograms of
# sample means, k estimates using MoM and MLE for both populations.

# YOUR CODE HERE

#plt.figure()
# YOUR CODE HERE

#plt.figure()
# YOUR CODE HERE

#plt.figure()
# YOUR CODE HERE

# Calculate and print the ratio of confidence intervals computed with and
# without using the population standard deviation that contains the population
# mean for both populations.

# YOUR CODE HERE

print('*'*50)
# Collect a sample of length 100000*25 from both populations, calculate and
# print their sample means, biased and unbiased sample variances, MoM and MLE
# estimates of parameter k and confidence intervals with and without using the
# population standard deviation.

# YOUR CODE HERE
147/1:
# imports

import os
import requests
from dotenv import load_dotenv
from bs4 import BeautifulSoup
from IPython.display import Markdown, display
from openai import OpenAI
147/2:
# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
openai = OpenAI()

# Uncomment the below line if this gives you any problems:
# openai = OpenAI(api_key="your-key-here")
147/3:
# imports

import os
import requests
from dotenv import load_dotenv
from bs4 import BeautifulSoup
from IPython.display import Markdown, display
from openai import OpenAI
quansad
148/1:
# imports

import os
import requests
from dotenv import load_dotenv
from bs4 import BeautifulSoup
from IPython.display import Markdown, display
from openai import OpenAI
148/2:
# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
openai = OpenAI()

# Uncomment the below line if this gives you any problems:
# openai = OpenAI(api_key="your-key-here")
148/3:
# A class to represent a Webpage

class Website:
    url: str
    title: str
    text: str

    def __init__(self, url):
        self.url = url
        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')
        self.title = soup.title.string if soup.title else "No title found"
        for irrelevant in soup.body(["script", "style", "img", "input"]):
            irrelevant.decompose()
        self.text = soup.body.get_text(separator="\n", strip=True)
148/4:
# Let's try one out

ed = Website("https://edwarddonner.com")
print(ed.title)
print(ed.text)
148/5:
system_prompt = "You are an assistant that analyzes the contents of a website \
and provides a short summary, ignoring text that might be navigation related. \
Respond in markdown."
148/6:
def user_prompt_for(website):
    user_prompt = f"You are looking at a website titled {website.title}"
    user_prompt += "The contents of this website is as follows; \
please provide a short summary of this website in markdown. \
If it includes news or announcements, then summarize these too.\n\n"
    user_prompt += website.text
    return user_prompt
148/7:
def messages_for(website):
    return [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt_for(website)}
    ]
148/8:
def summarize(url):
    website = Website(url)
    response = openai.chat.completions.create(
        model = "gpt-4o-mini",
        messages = messages_for(website)
    )
    return response.choices[0].message.content
148/9: summarize("https://edwarddonner.com")
148/10:
def summarize(url):
    website = Website(url)
    response = openai.chat.completions.create(
        model = "gpt-4o-mini",
        messages = messages_for(website)
    )
    return response.choices[0].message.content
148/11: summarize("https://edwarddonner.com")
148/12: display_summary("https://cnn.com")
148/13:
def display_summary(url):
    summary = summarize(url)
    display(Markdown(summary))
148/14: display_summary("https://cnn.com")
148/15: display_summary("https://edwarddonner.com")
148/16: display_summary("https://anthropic.com")
150/1: a = 3
150/2: a
150/3: clear
152/1:
with open('wizard_of_oz.txt','r', encoding='utf-8') as f:
    text = f.read()
print([:200])
152/2:
with open('wizard_of_oz.txt','r', encoding='utf-8') as f:
    text = f.read()
print(text[:200])
152/3:
with open('wizard_of_oz.txt','r', encoding='utf-8') as f:
    text = f.read()
print(text[:200])
152/4:
with open('wizard_of_oz.txt','r', encoding='utf-8') as f:
    text = f.read()
chars = sorted(set(text))
152/5:
with open('wizard_of_oz.txt','r', encoding='utf-8') as f:
    text = f.read()
chars = sorted(set(text))
print(chars)
152/6:
with open('wizard_of_oz.txt','r', encoding='utf-8') as f:
    text = f.read()
chars = sorted(set(text))
print(chars)
print(len(chars))
152/7: string_to_int = {ch:i for i,ch in enumerate(chars)}
152/8:
string_to_int = {ch:i for i,ch in enumerate(chars)}
print(string_to_int)
152/9:
string_to_int = {ch:i for i,ch in enumerate(chars)}
int_to_string = {i:ch for i,ch in enumerate(chars)}
152/10: import torch
152/11: import pytorch
152/12: import torch
152/13: import torch
152/14:
with open('wizard_of_oz.txt','r', encoding='utf-8') as f:
    text = f.read()
chars = sorted(set(text))
print(chars)
print(len(chars))
152/15:
string_to_int = {ch:i for i,ch in enumerate(chars)}
int_to_string = {i:ch for i,ch in enumerate(chars)}
152/16:
string_to_int = {ch:i for i,ch in enumerate(chars)}
int_to_string = {i:ch for i,ch in enumerate(chars)}
encode = lambda s: [string_to_int[c] for c in s]
decode = lambda 1: ''.join([int_to_string[i] for i in 1])
data=torch.tensor(encode(text),dtype = torch.long)
152/17:
string_to_int = {ch:i for i,ch in enumerate(chars)}
int_to_string = {i:ch for i,ch in enumerate(chars)}
encode = lambda s: [string_to_int[c] for c in s]
decode = lambda l: ''.join([int_to_string[i] for i in l)
data=torch.tensor(encode(text),dtype = torch.long)
152/18:
string_to_int = {ch:i for i,ch in enumerate(chars)}
int_to_string = {i:ch for i,ch in enumerate(chars)}
encode = lambda s: [string_to_int[c] for c in s]
decode = lambda l: ''.join([int_to_string[i] for i in l])
data=torch.tensor(encode(text),dtype = torch.long)
152/19:
string_to_int = {ch:i for i,ch in enumerate(chars)}
int_to_string = {i:ch for i,ch in enumerate(chars)}
encode = lambda s: [string_to_int[c] for c in s]
decode = lambda l: ''.join([int_to_string[i] for i in l])
data=torch.tensor(encode(text),dtype = torch.long)
print(data[:100])
152/20:
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(device)
154/1:
# imports

import os
import requests
from dotenv import load_dotenv
from bs4 import BeautifulSoup
from IPython.display import Markdown, display
from openai import OpenAI
154/2:
# A class to represent a Webpage

class Website:
    url: str
    title: str
    text: str

    def __init__(self, url):
        self.url = url
        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')
        self.title = soup.title.string if soup.title else "No title found"
        for irrelevant in soup.body(["script", "style", "img", "input"]):
            irrelevant.decompose()
        self.text = soup.body.get_text(separator="\n", strip=True)
154/3:
# Let's try one out

ed = Website("https://edwarddonner.com")
print(ed.title)
print(ed.text)
154/4:
# A class to represent a Webpage

class Website:
    url: str
    title: str
    text: str

    def __init__(self, url):
        self.url = url
        response = requests.get(url)
        print(response)
        soup = BeautifulSoup(response.content, 'html.parser')
        self.title = soup.title.string if soup.title else "No title found"
        for irrelevant in soup.body(["script", "style", "img", "input"]):
            irrelevant.decompose()
        self.text = soup.body.get_text(separator="\n", strip=True)
154/5:
# A class to represent a Webpage

class Website:
    url: str
    title: str
    text: str

    def __init__(self, url):
        self.url = url
        response = requests.get(url)
        print(type(response))
        soup = BeautifulSoup(response.content, 'html.parser')
        self.title = soup.title.string if soup.title else "No title found"
        for irrelevant in soup.body(["script", "style", "img", "input"]):
            irrelevant.decompose()
        self.text = soup.body.get_text(separator="\n", strip=True)
154/6:
# Let's try one out

ed = Website("https://edwarddonner.com")
print(ed.title)
print(ed.text)
154/7:
# A class to represent a Webpage

class Website:
    url: str
    title: str
    text: str

    def __init__(self, url):
        self.url = url
        response = requests.get(url)
       print(response)
        soup = BeautifulSoup(response.content, 'html.parser')
        self.title = soup.title.string if soup.title else "No title found"
        for irrelevant in soup.body(["script", "style", "img", "input"]):
            irrelevant.decompose()
        self.text = soup.body.get_text(separator="\n", strip=True)
154/9:
# A class to represent a Webpage

class Website:
    url: str
    title: str
    text: str

    def __init__(self, url):
        self.url = url
        response = requests.get(url)
        print(response)
        soup = BeautifulSoup(response.content, 'html.parser')
        self.title = soup.title.string if soup.title else "No title found"
        for irrelevant in soup.body(["script", "style", "img", "input"]):
            irrelevant.decompose()
        self.text = soup.body.get_text(separator="\n", strip=True)
154/10:
# A class to represent a Webpage

class Website:
    url: str
    title: str
    text: str

    def __init__(self, url):
        self.url = url
        response = requests.get(url)
    
        soup = BeautifulSoup(response.content, 'html.parser')
        self.title = soup.title.string if soup.title else "No title found"
        for irrelevant in soup.body(["script", "style", "img", "input"]):
            irrelevant.decompose()
        self.text = soup.body.get_text(separator="\n", strip=True)
154/11:
# Let's try one out

ed = Website("https://edwarddonner.com")
print(ed.title)
print(ed.text)
154/12:
system_prompt = "You are an assistant that analyzes the contents of a website \
and provides a short summary, ignoring text that might be navigation related. \
Respond in markdown."
154/13:
def user_prompt_for(website):
    user_prompt = f"You are looking at a website titled {website.title}"
    user_prompt += "The contents of this website is as follows; \
please provide a short summary of this website in markdown. \
If it includes news or announcements, then summarize these too.\n\n"
    user_prompt += website.text
    return user_prompt
154/14:
def messages_for(website):
    return [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt_for(website)}
    ]
154/15:
def summarize(url):
    website = Website(url)
    response = openai.chat.completions.create(
        model = "gpt-4o-mini",
        messages = messages_for(website)
    )
    return response.choices[0].message.content
154/16: summarize("https://edwarddonner.com")
154/17:
# imports

import os
import requests
from dotenv import load_dotenv
from bs4 import BeautifulSoup
from IPython.display import Markdown, display
from openai import OpenAI
154/18: summarize("https://edwarddonner.com")
154/19:
def summarize(url):
    website = Website(url)
    response = openai.chat.completions.create(
        model = "gpt-4o-mini",
        messages = messages_for(website)
    )
    return response.choices[0].message.content
154/20: summarize("https://edwarddonner.com")
154/21:
# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY','your-key-if-not-using-env')
openai = OpenAI()
154/22:
# A class to represent a Webpage

class Website:
    url: str
    title: str
    text: str

    def __init__(self, url):
        self.url = url
        response = requests.get(url)
    
        soup = BeautifulSoup(response.content, 'html.parser')
        self.title = soup.title.string if soup.title else "No title found"
        for irrelevant in soup.body(["script", "style", "img", "input"]):
            irrelevant.decompose()
        self.text = soup.body.get_text(separator="\n", strip=True)
154/23:
# Let's try one out

ed = Website("https://edwarddonner.com")
print(ed.title)
print(ed.text)
154/24:
system_prompt = "You are an assistant that analyzes the contents of a website \
and provides a short summary, ignoring text that might be navigation related. \
Respond in markdown."
154/25:
def user_prompt_for(website):
    user_prompt = f"You are looking at a website titled {website.title}"
    user_prompt += "The contents of this website is as follows; \
please provide a short summary of this website in markdown. \
If it includes news or announcements, then summarize these too.\n\n"
    user_prompt += website.text
    return user_prompt
154/26:
def messages_for(website):
    return [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt_for(website)}
    ]
154/27:
def summarize(url):
    website = Website(url)
    response = openai.chat.completions.create(
        model = "gpt-4o-mini",
        messages = messages_for(website)
    )
    return response.choices[0].message.content
154/28: summarize("https://edwarddonner.com")
154/29:
def display_summary(url):
    summary = summarize(url)
    display(Markdown(summary))
154/30:
def messages_for(website):
    return [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt_for(website)}
    ]
154/31:
def summarize(url):
    website = Website(url)
    response = openai.chat.completions.create(
        model = "gpt-4o-mini",
        messages = messages_for(website)
    )
    return response.choices[0].message.content
154/32:
def display_summary(url):
    summary = summarize(url)
    display(Markdown(summary))
154/33: display_summary("https://edwarddonner.com")
154/34: display_summary("https://cnn.com")
154/35: display_summary("https://anthropic.com")
154/36: display_summary("https://openai.com")
154/37:
#Parse webpages which is designed using JavaScript heavely
# download the chorme driver from here as per your version of chrome - https://developer.chrome.com/docs/chromedriver/downloads
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options

PATH_TO_CHROME_DRIVER = '..\\path\\to\\chromedriver.exe'

class Website:
    url: str
    title: str
    text: str

    def __init__(self, url):
        self.url = url

        options = Options()

        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")

        service = Service(PATH_TO_CHROME_DRIVER)
        driver = webdriver.Chrome(service=service, options=options)
        driver.get(url)

        input("Please complete the verification in the browser and press Enter to continue...")
        page_source = driver.page_source
        driver.quit()

        soup = BeautifulSoup(page_source, 'html.parser')
        self.title = soup.title.string if soup.title else "No title found"
        for irrelevant in soup(["script", "style", "img", "input"]):
            irrelevant.decompose()
        self.text = soup.get_text(separator="\n", strip=True)
154/38: display_summary("https://openai.com")
155/1:
# imports

import requests
from bs4 import BeautifulSoup
from IPython.display import Markdown, display
155/2:
# Constants

OLLAMA_API = "http://localhost:11434/api/chat"
HEADERS = {"Content-Type": "application/json"}
MODEL = "llama3.2"
155/3:
# Create a messages list using the same format that we used for OpenAI

messages = [
    {"role": "user", "content": "Describe some of the business applications of Generative AI"}
]
155/4:
payload = {
        "model": MODEL,
        "messages": messages,
        "stream": False
    }
155/5:
payload = {
        "model": MODEL,
        "messages": messages,
        "stream": False
    }
155/6:
response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
print(response.json()['message']['content'])
155/7:
import ollama

response = ollama.chat(model=MODEL, messages=messages)
print(response['message']['content'])
155/8:
!pip install ollama
import ollama

response = ollama.chat(model=MODEL, messages=messages)
print(response['message']['content'])
155/9:
# Let's try one out. Change the website and add print statements to follow along.

ed = Website("https://edwarddonner.com")
print(ed.title)
print(ed.text)
155/10:
# Let's try one out. Change the website and add print statements to follow along.
import ollama
import day1.ipynb
ed = Website("https://edwarddonner.com")
print(ed.title)
print(ed.text)
155/11:
# Let's try one out. Change the website and add print statements to follow along.
import ollama
import day1
ed = Website("https://edwarddonner.com")
print(ed.title)
print(ed.text)
155/12:
# Let's try one out. Change the website and add print statements to follow along.
import ollama
%run day1.ipynb
ed = Website("https://edwarddonner.com")
print(ed.title)
print(ed.text)
153/1:
# A class to represent a Webpage
# If you're not familiar with Classes, check out the "Intermediate Python" notebook

class Website:

    def __init__(self, url):
        """
        Create this Website object from the given url using the BeautifulSoup library
        """
        self.url = url
        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')
        self.title = soup.title.string if soup.title else "No title found"
        for irrelevant in soup.body(["script", "style", "img", "input"]):
            irrelevant.decompose()
        self.text = soup.body.get_text(separator="\n", strip=True)
153/2:
# Let's try one out. Change the website and add print statements to follow along.

ed = Website("https://edwarddonner.com")
print(ed.title)
print(ed.text)
153/3:
# imports

import os
import requests
from dotenv import load_dotenv
from bs4 import BeautifulSoup
from IPython.display import Markdown, display
from openai import OpenAI

# If you get an error running this cell, then please head over to the troubleshooting notebook!
153/4:
# Let's try one out. Change the website and add print statements to follow along.

ed = Website("https://edwarddonner.com")
print(ed.title)
print(ed.text)
153/5:
# A class to represent a Webpage
# If you're not familiar with Classes, check out the "Intermediate Python" notebook

class Website:

    def __init__(self, url):
        """
        Create this Website object from the given url using the BeautifulSoup library
        """
        self.url = url
        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')
        self.title = soup.title.string if soup.title else "No title found"
        for irrelevant in soup.body(["script", "style", "img", "input"]):
            irrelevant.decompose()
        self.text = soup.body.get_text(separator="\n", strip=True)
155/13:
# Let's try one out. Change the website and add print statements to follow along.
import ollama
%run day1.ipynb
ed = Website("https://edwarddonner.com")
print(ed.title)
print(ed.text)
155/14:


response = requests.post(OLLAMA_API,json=payload,headers=HEADERS)

# Create a messages list using the same format that we used for OpenAI

messages = [
    {"role": "user", "content": "Get a summary of this website"}
]
print(response.json()['message']['content'])
153/6:
# A class to represent a Webpage
# If you're not familiar with Classes, check out the "Intermediate Python" notebook

class Website:

    def __init__(self, url):
        """
        Create this Website object from the given url using the BeautifulSoup library
        """
        self.url = url
        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')
        self.title = soup.title.string if soup.title else "No title found"
        for irrelevant in soup.body(["script", "style", "img", "input"]):
            irrelevant.decompose()
        self.text = soup.body.get_text(separator="\n", strip=True)
153/7:
# Let's try one out. Change the website and add print statements to follow along.

ed = Website("https://edwarddonner.com")
print(ed.title)
print(ed.text)
155/15:
# A class to represent a Webpage
# If you're not familiar with Classes, check out the "Intermediate Python" notebook

class Website:

    def __init__(self, url):
        """
        Create this Website object from the given url using the BeautifulSoup library
        """
        self.url = url
        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')
        self.title = soup.title.string if soup.title else "No title found"
        for irrelevant in soup.body(["script", "style", "img", "input"]):
            irrelevant.decompose()
        self.text = soup.body.get_text(separator="\n", strip=True)
155/16:
# A class to represent a Webpage
# If you're not familiar with Classes, check out the "Intermediate Python" notebook

class Website:

    def __init__(self, url):
        """
        Create this Website object from the given url using the BeautifulSoup library
        """
        self.url = url
        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')
        self.title = soup.title.string if soup.title else "No title found"
        for irrelevant in soup.body(["script", "style", "img", "input"]):
            irrelevant.decompose()
        self.text = soup.body.get_text(separator="\n", strip=True)
155/17:
# Let's try one out. Change the website and add print statements to follow along.
ed = Website("https://edwarddonner.com")
print(ed.title)
print(ed.text)
155/18:
# Define our system prompt - you can experiment with this later, changing the last sentence to 'Respond in markdown in Spanish."

system_prompt = "You are an assistant that analyzes the contents of a website \
and provides a short summary, ignoring text that might be navigation related. \
Respond in markdown."
153/8:
# A function that writes a User Prompt that asks for summaries of websites:

def user_prompt_for(website):
    user_prompt = f"You are looking at a website titled {website.title}"
    user_prompt += "\nThe contents of this website is as follows; \
please provide a short summary of this website in markdown. \
If it includes news or announcements, then summarize these too.\n\n"
    user_prompt += website.text
    return user_prompt
153/9: print(user_prompt_for(ed))
155/19:
# Define our system prompt - you can experiment with this later, changing the last sentence to 'Respond in markdown in Spanish."

system_prompt = "You are an assistant that analyzes the contents of a website \
and provides a short summary, ignoring text that might be navigation related. \
Respond in markdown."
155/20:
# See how this function creates exactly the format above

def messages_for(website):
    return [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt_for(website)}
    ]
155/21:
# Try this out, and then try for a few more websites

messages_for(ed)
155/22:
# A function that writes a User Prompt that asks for summaries of websites:

def user_prompt_for(website):
    user_prompt = f"You are looking at a website titled {website.title}"
    user_prompt += "\nThe contents of this website is as follows; \
please provide a short summary of this website in markdown. \
If it includes news or announcements, then summarize these too.\n\n"
    user_prompt += website.text
    return user_prompt
155/23: print(user_prompt_for(ed))
155/24:
# See how this function creates exactly the format above

def messages_for(website):
    return [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt_for(website)}
    ]
155/25:
# Try this out, and then try for a few more websites

messages_for(ed)
155/26:
# And now: call the OpenAI API. You will get very familiar with this!

def summarize(url):
    website = Website(url)
    response = ollama.chat(model=MODEL, messages=messages)
    return response.choices[0].message.cont
155/27: summarize("https://edwarddonner.com")
155/28:
# And now: call the OpenAI API. You will get very familiar with this!

def summarize(url):
    website = Website(url)
    response = ollama.chat(model=MODEL, messages=messages)
    return response.choices[0].message.content
155/29: summarize("https://edwarddonner.com")
155/30:
# And now: call the OpenAI API. You will get very familiar with this!

def summarize(url):
    website = Website(url)
    response = ollama.chat(model=MODEL, messages=messages)
    response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
print(response.json()['message']['content'])
155/31:
# See how this function creates exactly the format above

def messages_for(website):
    return [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt_for(website)}
    ]
155/32:
# Try this out, and then try for a few more websites

messages_for(ed)
155/33: summarize("https://edwarddonner.com")
155/34:
# And now: call the OpenAI API. You will get very familiar with this!

def summarize(url):
    website = Website(url)
    response = ollama.chat(model=MODEL, messages=messages)
    response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
print(response.json()['message']['content'])
155/35:
# And now: call the OpenAI API. You will get very familiar with this!

def summarize(url):
    website = Website(url)
    response = ollama.chat(model=MODEL, messages=messages)
    response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
print(response.json()['message']['content'])
155/36:

import ollama

response = ollama.chat(model=MODEL, messages=messages)
print(response['message']['content'])
155/37:

import ollama
155/38:
# A class to represent a Webpage
# If you're not familiar with Classes, check out the "Intermediate Python" notebook

class Website:

    def __init__(self, url):
        """
        Create this Website object from the given url using the BeautifulSoup library
        """
        self.url = url
        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')
        self.title = soup.title.string if soup.title else "No title found"
        for irrelevant in soup.body(["script", "style", "img", "input"]):
            irrelevant.decompose()
        self.text = soup.body.get_text(separator="\n", strip=True)
155/39:
# Let's try one out. Change the website and add print statements to follow along.
ed = Website("https://edwarddonner.com")
print(ed.title)
print(ed.text)
155/40:
# Define our system prompt - you can experiment with this later, changing the last sentence to 'Respond in markdown in Spanish."

system_prompt = "You are an assistant that analyzes the contents of a website \
and provides a short summary, ignoring text that might be navigation related. \
Respond in markdown."
155/41:
# A function that writes a User Prompt that asks for summaries of websites:

def user_prompt_for(website):
    user_prompt = f"You are looking at a website titled {website.title}"
    user_prompt += "\nThe contents of this website is as follows; \
please provide a short summary of this website in markdown. \
If it includes news or announcements, then summarize these too.\n\n"
    user_prompt += website.text
    return user_prompt
155/42: print(user_prompt_for(ed))
155/43:
# See how this function creates exactly the format above

def messages_for(website):
    return [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt_for(website)}
    ]
155/44:
# Try this out, and then try for a few more websites

messages_for(ed)
155/45:
# And now: call the OpenAI API. You will get very familiar with this!

def summarize(url):
    website = Website(url)
  #  response = ollama.chat(model=MODEL, messages=messages)
    response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
print(response.json()['message']['content'])
155/46:
# And now: call the OpenAI API. You will get very familiar with this!

def summarize(url):
    website = Website(url)
  #  response = ollama.chat(model=MODEL, messages=messages)
    response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
    print(response.json()['message']['content'])
155/47:
# And now: call the OpenAI API. You will get very familiar with this!

def summarize(url):
    website = Website(url)
  #  response = ollama.chat(model=MODEL, messages=messages)
    response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
    print(response.json()['message']['content'])
155/48:
# And now: call the OpenAI API. You will get very familiar with this!

def summarize(url):
    website = Website(url)
  #  response = ollama.chat(model=MODEL, messages=messages)
    response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
    print(response.json()['message']['content'])
155/49: summarize("https://edwarddonner.com")
155/50:
# And now: call the OpenAI API. You will get very familiar with this!

def summarize(url):
    website = Website(url)
  #  response = ollama.chat(model=MODEL, messages=messages)
    response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
    print(response.json()['messages_for(ed)]['content'])
155/51:
# And now: call the OpenAI API. You will get very familiar with this!

def summarize(url):
    website = Website(url)
  #  response = ollama.chat(model=MODEL, messages=messages)
    response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
    print(response.json()['messages_for(ed)']['content'])
155/52: summarize("https://edwarddonner.com")
155/53:
# And now: call the OpenAI API. You will get very familiar with this!

def summarize(url):
    website = Website(url)
  #  response = ollama.chat(model=MODEL, messages=messages)
    response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
    print(response.json()[messages_for(ed)]['content'])
155/54: summarize("https://edwarddonner.com")
155/55:
# And now: call the OpenAI API. You will get very familiar with this!

def summarize(url):
    website = Website(url)
  #  response = ollama.chat(model=MODEL, messages=messages)
    response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
    print(response.json()['message']['content'])
155/56: summarize("https://edwarddonner.com")
155/57:
# See how this function creates exactly the format above

def messages_for(website):
    return [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt_for(website)}
    ]
155/58:
# Try this out, and then try for a few more websites
messages = messages_for(ed)
155/59:
# And now: call the OpenAI API. You will get very familiar with this!

def summarize(url):
    website = Website(url)
  #  response = ollama.chat(model=MODEL, messages=messages)
    response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
    print(response.json()['message']['content'])
155/60: summarize("https://edwarddonner.com")
155/61:
# Try this out, and then try for a few more websites
messages = messages_for(ed)
155/62:
payload = {
        "model": MODEL,
        "messages": messages,
        "stream": False
    }
155/63:
payload =  { "model": MODEL,
             "messages": messages,
             "stream": False
155/64:
payload =  { "model": MODEL,
             "messages": messages,
             "stream": False
           }
155/65:
# imports

import requests
from bs4 import BeautifulSoup
from IPython.display import Markdown, display
import ollama
155/66:
# imports

import requests
from bs4 import BeautifulSoup
from IPython.display import Markdown, display
import ollama
155/67:
# A class to represent a Webpage
# If you're not familiar with Classes, check out the "Intermediate Python" notebook

class Website:

    def __init__(self, url):
        """
        Create this Website object from the given url using the BeautifulSoup library
        """
        self.url = url
        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')
        self.title = soup.title.string if soup.title else "No title found"
        for irrelevant in soup.body(["script", "style", "img", "input"]):
            irrelevant.decompose()
        self.text = soup.body.get_text(separator="\n", strip=True)
155/68:
# Let's try one out. Change the website and add print statements to follow along.
ed = Website("https://edwarddonner.com")
print(ed.title)
print(ed.text)
155/69:
# Define our system prompt - you can experiment with this later, changing the last sentence to 'Respond in markdown in Spanish."

system_prompt = "You are an assistant that analyzes the contents of a website \
and provides a short summary, ignoring text that might be navigation related. \
Respond in markdown."
155/70:
# A function that writes a User Prompt that asks for summaries of websites:

def user_prompt_for(website):
    user_prompt = f"You are looking at a website titled {website.title}"
    user_prompt += "\nThe contents of this website is as follows; \
please provide a short summary of this website in markdown. \
If it includes news or announcements, then summarize these too.\n\n"
    user_prompt += website.text
    return user_prompt
155/71: print(user_prompt_for(ed))
155/72:
# Try this out, and then try for a few more websites
messages = messages_for(ed)
155/73:
payload = {
        "model": MODEL,
        "messages": messages,
        "stream": False
    }
155/74:
# And now: call the OpenAI API. You will get very familiar with this!

def summarize(url):
    website = Website(url)
  #  response = ollama.chat(model=MODEL, messages=messages)
    response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
    print(response.json()['message']['content'])
155/75: summarize("https://edwarddonner.com")
155/76:
# Let's try one out. Change the website and add print statements to follow along.
ed = Website("https://odine.com/")
print(ed.title)
print(ed.text)
155/77:
# Let's try one out. Change the website and add print statements to follow along.
ed = Website("https://tryhackme.com/")
print(ed.title)
print(ed.text)
155/78:
# Define our system prompt - you can experiment with this later, changing the last sentence to 'Respond in markdown in Spanish."

system_prompt = "You are an assistant that analyzes the contents of a website \
and provides a short summary, ignoring text that might be navigation related. \
Respond in markdown."
155/79:
# A function that writes a User Prompt that asks for summaries of websites:

def user_prompt_for(website):
    user_prompt = f"You are looking at a website titled {website.title}"
    user_prompt += "\nThe contents of this website is as follows; \
please provide a short summary of this website in markdown. \
If it includes news or announcements, then summarize these too.\n\n"
    user_prompt += website.text
    return user_prompt
155/80: print(user_prompt_for(ed))
155/81:
# Try this out, and then try for a few more websites
messages = messages_for(ed)
155/82:
payload = {
        "model": MODEL,
        "messages": messages,
        "stream": False
    }
155/83:
# And now: call the OpenAI API. You will get very familiar with this!

def summarize(url):
    website = Website(url)
  #  response = ollama.chat(model=MODEL, messages=messages)
    response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
    print(response.json()['message']['content'])
155/84: summarize("https://edwarddonner.com")
155/85:
# And now: call the OpenAI API. You will get very familiar with this!

def summarize(urlx):
    website = Website(urlx)
  #  response = ollama.chat(model=MODEL, messages=messages)
    response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
    print(response.json()['message']['content'])
155/86: summarize("https://edwarddonner.com")
155/87:
# Try this out, and then try for a few more websites
site = "https://projecteuler.net/"
messages = messages_for(site)
155/88:
# Let's try one out. Change the website and add print statements to follow along.
ed = Website("https://projecteuler.net")

print(ed.title)
print(ed.text)
155/89:
# Define our system prompt - you can experiment with this later, changing the last sentence to 'Respond in markdown in Spanish."

system_prompt = "You are an assistant that analyzes the contents of a website \
and provides a short summary, ignoring text that might be navigation related. \
Respond in markdown."
155/90:
# A function that writes a User Prompt that asks for summaries of websites:

def user_prompt_for(website):
    user_prompt = f"You are looking at a website titled {website.title}"
    user_prompt += "\nThe contents of this website is as follows; \
please provide a short summary of this website in markdown. \
If it includes news or announcements, then summarize these too.\n\n"
    user_prompt += website.text
    return user_prompt
161/1:
# imports
# If these fail, please check you're running from an 'activated' environment with (llms) in the command prompt

import os
import requests
import json
from typing import List
from dotenv import load_dotenv
from bs4 import BeautifulSoup
from IPython.display import Markdown, display, update_display
from openai import OpenAI
161/2:
# Initialize and constants

load_dotenv()
api_key = os.getenv('OPENAI_API_KEY')

if api_key and api_key.startswith('sk-proj-') and len(api_key)>10:
    print("API key looks good so far")
else:
    print("There might be a problem with your API key? Please visit the troubleshooting notebook!")
    
MODEL = 'gpt-4o-mini'
openai = OpenAI()
161/3:
# A class to represent a Webpage

class Website:
    """
    A utility class to represent a Website that we have scraped, now with links
    """

    def __init__(self, url):
        self.url = url
        response = requests.get(url)
        self.body = response.content
        soup = BeautifulSoup(self.body, 'html.parser')
        self.title = soup.title.string if soup.title else "No title found"
        if soup.body:
            for irrelevant in soup.body(["script", "style", "img", "input"]):
                irrelevant.decompose()
            self.text = soup.body.get_text(separator="\n", strip=True)
        else:
            self.text = ""
        links = [link.get('href') for link in soup.find_all('a')]
        self.links = [link for link in links if link]

    def get_contents(self):
        return f"Webpage Title:\n{self.title}\nWebpage Contents:\n{self.text}\n\n"
161/4:
ed = Website("https://edwarddonner.com")
ed.links
161/5:
# imports
# If these fail, please check you're running from an 'activated' environment with (llms) in the command prompt

import os
import requests
import json
from typing import List
from dotenv import load_dotenv
from bs4 import BeautifulSoup
from IPython.display import Markdown, display, update_display
from openai import OpenAI
161/6:
# Initialize and constants

load_dotenv()
api_key = os.getenv('OPENAI_API_KEY')

if api_key and api_key.startswith('sk-proj-') and len(api_key)>10:
    print("API key looks good so far")
else:
    print("There might be a problem with your API key? Please visit the troubleshooting notebook!")
    
MODEL = 'gpt-4o-mini'
openai = OpenAI()
162/1:
# imports
# If these fail, please check you're running from an 'activated' environment with (llms) in the command prompt

import os
import requests
import json
from typing import List
from dotenv import load_dotenv
from bs4 import BeautifulSoup
from IPython.display import Markdown, display, update_display
from openai import OpenAI
162/2:
# Initialize and constants

load_dotenv()
api_key = os.getenv('OPENAI_API_KEY')

if api_key and api_key.startswith('sk-proj-') and len(api_key)>10:
    print("API key looks good so far")
else:
    print("There might be a problem with your API key? Please visit the troubleshooting notebook!")
    
MODEL = 'gpt-4o-mini'
openai = OpenAI()
162/3:
# A class to represent a Webpage

class Website:
    """
    A utility class to represent a Website that we have scraped, now with links
    """

    def __init__(self, url):
        self.url = url
        response = requests.get(url)
        self.body = response.content
        soup = BeautifulSoup(self.body, 'html.parser')
        self.title = soup.title.string if soup.title else "No title found"
        if soup.body:
            for irrelevant in soup.body(["script", "style", "img", "input"]):
                irrelevant.decompose()
            self.text = soup.body.get_text(separator="\n", strip=True)
        else:
            self.text = ""
        links = [link.get('href') for link in soup.find_all('a')]
        self.links = [link for link in links if link]

    def get_contents(self):
        return f"Webpage Title:\n{self.title}\nWebpage Contents:\n{self.text}\n\n"
162/4:
ed = Website("https://edwarddonner.com")
ed.links
162/5:
ed = Website("https://odine.com/")
ed.links
162/6:
# A class to represent a Webpage

class Website:
    """
    A utility class to represent a Website that we have scraped, now with links
    """
    url: str
    title: str
    body: str
    links: List[str]
    text: str
    
    def __init__(self, url):
        self.url = url
        response = requests.get(url)
        self.body = response.content
        soup = BeautifulSoup(self.body, 'html.parser')
        self.title = soup.title.string if soup.title else "No title found"
        if soup.body:
            for irrelevant in soup.body(["script", "style", "img", "input"]):
                irrelevant.decompose()
            self.text = soup.body.get_text(separator="\n", strip=True)
        else:
            self.text = ""
        links = [link.get('href') for link in soup.find_all('a')]
        self.links = [link for link in links if link]

    def get_contents(self):
        return f"Webpage Title:\n{self.title}\nWebpage Contents:\n{self.text}\n\n"
162/7:
ed = Website("https://edwarddonner.com")
ed.links
162/8:
ed = Website("https://edwarddonner.com")
print(ed.get_contents)
162/9:
# A class to represent a Webpage

class Website:
    """
    A utility class to represent a Website that we have scraped, now with links
    """
    url: str
    title: str
    body: str
    links: List[str]
    text: str
    
    def __init__(self, url):
        self.url = url
        response = requests.get(url)
        self.body = response.content
        soup = BeautifulSoup(self.body, 'html.parser')
        self.title = soup.title.string if soup.title else "No title found"
        if soup.body:
            for irrelevant in soup.body(["script", "style", "img", "input"]):
                irrelevant.decompose()
            self.text = soup.body.get_text(separator="\n", strip=True)
        else:
            self.text = ""
        links = [link.get('href') for link in soup.find_all('a')]
        self.links = [link for link in links if link]

    def get_contents(self):
        return f"Webpage Title:\n{self.title}\nWebpage Contents:\n{self.text}\n\n"
162/10:
ed = Website("https://edwarddonner.com")
print(ed.get_contents)
162/11:
ed = Website("https://edwarddonner.com")
print(ed.get_contents())
162/12:
ed = Website("https://edwarddonner.com")
ed.links
162/13: print(link_system_prompt)
162/14:
link_system_prompt = "You are provided with a list of links found on a webpage. \
You are able to decide which of the links would be most relevant to include in a brochure about the company, \
such as links to an About page, or a Company page, or Careers/Jobs pages.\n"
link_system_prompt += "You should respond in JSON as in this example:"
link_system_prompt += """
{
    "links": [
        {"type": "about page", "url": "https://full.url/goes/here/about"},
        {"type": "careers page": "url": "https://another.full.url/careers"}
    ]
}
"""
162/15: print(link_system_prompt)
162/16:
def get_links_user_prompt(website):
    user_prompt = f"Here is the list of links on the website of {website.url} - "
    user_prompt += "please decide which of these are relevant web links for a brochure about the company, respond with the full https URL in JSON format. \
Do not include Terms of Service, Privacy, email links.\n"
    user_prompt += "Links (some might be relative links):\n"
    user_prompt += "\n".join(website.links)
    return user_prompt
162/17: print(get_links_user_prompt(ed))
162/18:
def get_links(url):
    website = Website(url)
    response = openai.chat.completions.create(
        model=MODEL,
        messages=[
            {"role": "system", "content": link_system_prompt},
            {"role": "user", "content": get_links_user_prompt(website)}
      ],
        response_format={"type": "json_object"}
    )
    result = response.choices[0].message.content
    return json.loads(result)
162/19:
anthropic = Website("https://anthropic.com")
anthropic.links
162/20: get_links("https://anthropic.com")
162/21:
# A class to represent a Webpage

class Website:
    """
    A utility class to represent a Website that we have scraped, now with links
    """
    url: str
    title: str
    body: str
    links: List[str]
    text: str
    
    def __init__(self, url):
        self.url = url
        response = requests.get(url)
        self.body = response.content
        soup = BeautifulSoup(self.body, 'html.parser')
        self.title = soup.title.string if soup.title else "No title found"
        if soup.body:
            for irrelevant in soup.body(["script", "style", "img", "input"]):
                irrelevant.decompose()
            self.text = soup.body.get_text(separator="\n", strip=True)
        else:
            self.text = ""
        links = [link.get('href') for link in soup.find_all('a')]
        self.links = [link for link in links if link]

    def get_contents(self):
        return f"Webpage Title:\n{self.title}\nWebpage Contents:\n{self.text}\n\n"
162/22:
ed = Website("https://edwarddonner.com")
ed.links
162/23:
link_system_prompt = "You are provided with a list of links found on a webpage. \
You are able to decide which of the links would be most relevant to include in a brochure about the company, \
such as links to an About page, or a Company page, or Careers/Jobs pages.\n"
link_system_prompt += "You should respond in JSON as in this example:"
link_system_prompt += """
{
    "links": [
        {"type": "about page", "url": "https://full.url/goes/here/about"},
        {"type": "careers page": "url": "https://another.full.url/careers"}
    ]
}
"""
162/24: print(link_system_prompt)
162/25:
def get_links_user_prompt(website):
    user_prompt = f"Here is the list of links on the website of {website.url} - "
    user_prompt += "please decide which of these are relevant web links for a brochure about the company, respond with the full https URL in JSON format. \
Do not include Terms of Service, Privacy, email links.\n"
    user_prompt += "Links (some might be relative links):\n"
    user_prompt += "\n".join(website.links)
    return user_prompt
162/26: print(get_links_user_prompt(ed))
162/27:
def get_links(url):
    website = Website(url)
    response = openai.chat.completions.create(
        model=MODEL,
        messages=[
            {"role": "system", "content": link_system_prompt},
            {"role": "user", "content": get_links_user_prompt(website)}
      ],
        response_format={"type": "json_object"}
    )
    result = response.choices[0].message.content
    return json.loads(result)
162/28:
anthropic = Website("https://anthropic.com")
anthropic.links
162/29: get_links("https://anthropic.com")
162/30:
def get_all_details(url):
    result = "Landing page:\n"
    result += Website(url).get_contents()
    links = get_links(url)
    print("Found links:", links)
    for link in links["links"]:
        result += f"\n\n{link['type']}\n"
        result += Website(link["url"]).get_contents()
    return result
162/31: print(get_all_details("https://anthropic.com"))
162/32:
system_prompt = "You are an assistant that analyzes the contents of several relevant pages from a company website \
and creates a short brochure about the company for prospective customers, investors and recruits. Respond in markdown.\
Include details of company culture, customers and careers/jobs if you have the information."

# Or uncomment the lines below for a more humorous brochure - this demonstrates how easy it is to incorporate 'tone':

# system_prompt = "You are an assistant that analyzes the contents of several relevant pages from a company website \
# and creates a short humorous, entertaining, jokey brochure about the company for prospective customers, investors and recruits. Respond in markdown.\
# Include details of company culture, customers and careers/jobs if you have the information."
162/33:
def get_brochure_user_prompt(company_name, url):
    user_prompt = f"You are looking at a company called: {company_name}\n"
    user_prompt += f"Here are the contents of its landing page and other relevant pages; use this information to build a short brochure of the company in markdown.\n"
    user_prompt += get_all_details(url)
    user_prompt = user_prompt[:20_000] # Truncate if more than 20,000 characters
    return user_prompt
162/34: get_brochure_user_prompt("Anthropic", "https://anthropic.com")
162/35:
def create_brochure(company_name, url):
    response = openai.chat.completions.create(
        model=MODEL,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": get_brochure_user_prompt(company_name, url)}
          ],
    )
    result = response.choices[0].message.content
    display(Markdown(result))
162/36: create_brochure("Anthropic", "https://anthropic.com")
162/37:
def stream_brochure(company_name, url):
    stream = openai.chat.completions.create(
        model=MODEL,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": get_brochure_user_prompt(company_name, url)}
          ],
        stream=True
    )
    
    response = ""
    display_handle = display(Markdown(""), display_id=True)
    for chunk in stream:
        response += chunk.choices[0].delta.content or ''
        response = response.replace("```","").replace("markdown", "")
        update_display(Markdown(response), display_id=display_handle.display_id)
162/38:
def stream_brochure(company_name, url):
    stream = openai.chat.completions.create(
        model=MODEL,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": get_brochure_user_prompt(company_name, url)}
          ],
        stream=True
    )
    
    response = ""
    display_handle = display(Markdown(""), display_id=True)
    for chunk in stream:
        response += chunk.choices[0].delta.content or ''
        response = response.replace("```","").replace("markdown", "")
        update_display(Markdown(response), display_id=display_handle.display_id)
162/39: stream_brochure("Anthropic", "https://anthropic.com")
163/1:
# imports

import os
from dotenv import load_dotenv
from openai import OpenAI
import anthropic
from IPython.display import Markdown, display, update_display
163/2:
# import for google
# in rare cases, this seems to give an error on some systems. Please reach out to me if this happens,
# or you can feel free to skip Gemini - it's the lowest priority of the frontier models that we use

import google.generativeai
163/3:
# Load environment variables in a file called .env
# Print the key prefixes to help with any debugging

load_dotenv()
openai_api_key = os.getenv('OPENAI_API_KEY')
anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')
google_api_key = os.getenv('GOOGLE_API_KEY')

if openai_api_key:
    print(f"OpenAI API Key exists and begins {openai_api_key[:8]}")
else:
    print("OpenAI API Key not set")
    
if anthropic_api_key:
    print(f"Anthropic API Key exists and begins {anthropic_api_key[:7]}")
else:
    print("Anthropic API Key not set")

if google_api_key:
    print(f"Google API Key exists and begins {google_api_key[:8]}")
else:
    print("Google API Key not set")
163/4:
# Connect to OpenAI, Anthropic and Google
# All 3 APIs are similar
# Having problems with API files? You can use openai = OpenAI(api_key="your-key-here") and same for claude
# Having problems with Google Gemini setup? Then just skip Gemini; you'll get all the experience you need from GPT and Claude.

openai = OpenAI()

claude = anthropic.Anthropic()

google.generativeai.configure()
163/5:
system_message = "You are an assistant that is great at telling jokes"
user_prompt = "Tell a light-hearted joke for an audience of Data Scientists"
163/6:
prompts = [
    {"role": "system", "content": system_message},
    {"role": "user", "content": user_prompt}
  ]
163/7:
# GPT-4o-mini
# Temperature setting controls creativity

completion = openai.chat.completions.create(
    model='gpt-4o-mini',
    messages=prompts,
    temperature=0.7,
    stream=True
)
print(completion.choices[0].message.content)
163/8:
# GPT-4o-mini
# Temperature setting controls creativity

completion = openai.chat.completions.create(
    model='gpt-4o-mini',
    messages=prompts,
    temperature=0.7,
)
print(completion.choices[0].message.content)
163/9:
# GPT-4o-mini
# Temperature setting controls creativity

completion = openai.chat.completions.create(
    model='gpt-4o-mini',
    messages=prompts,
    temperature=0.02,
)
print(completion.choices[0].message.content)
163/10:
# GPT-4o-mini
# Temperature setting controls creativity

completion = openai.chat.completions.create(
    model='gpt-4o-mini',
    messages=prompts,
    temperature=0.123,
)
print(completion.choices[0].message.content)
163/11:
# GPT-4o-mini
# Temperature setting controls creativity

completion = openai.chat.completions.create(
    model='gpt-4o-mini',
    messages=prompts,
    temperature=0.123,
)
print(completion.choices[1].message.content)
163/12:
# GPT-3.5-Turbo

completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)
print(completion.choices[0].message.content)
163/13:
# GPT-3.5-Turbo

completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts,stream= True)
print(completion.choices[0].message.content)
163/14:
# GPT-3.5-Turbo

#completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)
#print(completion.choices[0].message.content)

import openai

# Assume `prompts` is defined correctly
completion = openai.ChatCompletion.create(
    model='gpt-3.5-turbo',
    messages=prompts,
    stream=True
)

for chunk in completion:
    if "choices" in chunk and chunk["choices"]:
        content = chunk["choices"][0].get("delta", {}).get("content")
        if content:
            print(content, end="")  # Print without a newline to simulate streaming
163/15:
# GPT-3.5-Turbo

#completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)
#print(completion.choices[0].message.content)

import openai

# Assume `prompts` is defined correctly
completion = openai.chatcompletions.create(
    model='gpt-3.5-turbo',
    messages=prompts,
    stream=True
)

for chunk in completion:
    if "choices" in chunk and chunk["choices"]:
        content = chunk["choices"][0].get("delta", {}).get("content")
        if content:
            print(content, end="")  # Print without a newline to simulate streaming
163/16:
# GPT-3.5-Turbo

#completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)
#print(completion.choices[0].message.content)

import openai

# Assume `prompts` is defined correctly
completion = openai.chat.completions.create(
    model='gpt-3.5-turbo',
    messages=prompts,
    stream=True
)

for chunk in completion:
    if "choices" in chunk and chunk["choices"]:
        content = chunk["choices"][0].get("delta", {}).get("content")
        if content:
            print(content, end="")  # Print without a newline to simulate streaming
163/17:
# GPT-3.5-Turbo

#completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)
#print(completion.choices[0].message.content)

import openai

# Assume `prompts` is defined correctly
completion = openai.chat.completions.create(
    model='gpt-3.5-turbo',
    messages=prompts,
    stream=True
)

for chunk in completion:
    if "choices" in chunk and chunk["choices"]:
        content = chunk["choices"][0].message.content
        if content:
            print(content, end="")  # Print without a newline to simulate streaming
163/18:
# GPT-3.5-Turbo

#completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)
#print(completion.choices[0].message.content)

import openai

# Assume `prompts` is defined correctly
completion = openai.chat.completions.create(
    model='gpt-3.5-turbo',
    messages=prompts,
    stream=True
)

for chunk in completion:
        content = chunk["choices"][0].message.content
        if content:
            print(content, end="")  # Print without a newline to simulate streaming
163/19:
# GPT-3.5-Turbo

completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)
print(completion.choices[0].message.content)
163/20:
# GPT-4o-mini
# Temperature setting controls creativity

completion = openai.chat.completions.create(
    model='gpt-4o-mini',
    messages=prompts,
    temperature=0.7,
)
print(completion.choices[0].message.content)
163/21:
# GPT-4o

completion = openai.chat.completions.create(
    model='gpt-4o',
    messages=prompts,
    temperature=0.4
)
print(completion.choices[0].message.content)
163/22:
# GPT-4o

completion = openai.chat.completions.create(
    model='gpt-4o',
    messages=prompts,
    temperature=0.7
)
print(completion.choices[0].message.content)
163/23:
# Claude 3.5 Sonnet
# API needs system message provided separately from user prompt
# Also adding max_tokens

message = claude.messages.create(
    model="claude-3-5-sonnet-20240620",
    max_tokens=200,
    temperature=0.7,
    system=system_message,
    messages=[
        {"role": "user", "content": user_prompt},
    ],
)

print(message.content[0].text)
163/24:
# Claude 3.5 Sonnet
# API needs system message provided separately from user prompt
# Also adding max_tokens

message = claude.messages.create(
    model="claude-3-5-sonnet-20240620",
    max_tokens=200,
    temperature=0.7,
    system=system_message,
    messages=[
        {"role": "user", "content": user_prompt},
    ],
)

print(message.content[0].text)
163/25:
# Claude 3.5 Sonnet again
# Now let's add in streaming back results

result = claude.messages.stream(
    model="claude-3-5-sonnet-20240620",
    max_tokens=200,
    temperature=0.7,
    system=system_message,
    messages=[
        {"role": "user", "content": user_prompt},
    ],
)

with result as stream:
    for text in stream.text_stream:
            print(text, end="", flush=True)
163/26:
# The API for Gemini has a slightly different structure

gemini = google.generativeai.GenerativeModel(
    model_name='gemini-1.5-flash',
    system_instruction=system_message
)
response = gemini.generate_content(user_prompt)
print(response.text)
163/27:
# To be serious! GPT-4o-mini with the original question

prompts = [
    {"role": "system", "content": "You are a helpful assistant that responds in Markdown"},
    {"role": "user", "content": "How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown."}
  ]
163/28:
# Have it stream back results in markdown

stream = openai.chat.completions.create(
    model='gpt-4o',
    messages=prompts,
    temperature=0.7,
    stream=True
)

reply = ""
display_handle = display(Markdown(""), display_id=True)
for chunk in stream:
    reply += chunk.choices[0].delta.content or ''
    reply = reply.replace("```","").replace("markdown","")
    update_display(Markdown(reply), display_id=display_handle.display_id)
163/29:
# To be serious! GPT-4o-mini with the original question

prompts = [
    {"role": "system", "content": "You are a helpful assistant that responds in Markdown"},
    {"role": "user", "content": "How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown."}
  ]
163/30:
# Have it stream back results in markdown

stream = openai.chat.completions.create(
    model='gpt-4o',
    messages=prompts,
    temperature=0.7,
    stream=True
)

reply = ""
display_handle = display(Markdown(""), display_id=True)
for chunk in stream:
    reply += chunk.choices[0].delta.content or ''
    reply = reply.replace("```","").replace("markdown","")
    update_display(Markdown(reply), display_id=display_handle.display_id)
164/1:
# Load environment variables in a file called .env
# Print the key prefixes to help with any debugging

load_dotenv()
openai_api_key = os.getenv('OPENAI_API_KEY')
anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')
google_api_key = os.getenv('GOOGLE_API_KEY')

if openai_api_key:
    print(f"OpenAI API Key exists and begins {openai_api_key[:8]}")
else:
    print("OpenAI API Key not set")
    
if anthropic_api_key:
    print(f"Anthropic API Key exists and begins {anthropic_api_key[:7]}")
else:
    print("Anthropic API Key not set")

if google_api_key:
    print(f"Google API Key exists and begins {google_api_key[:8]}")
else:
    print("Google API Key not set")
164/2:
# imports

import os
import requests
from bs4 import BeautifulSoup
from typing import List
from dotenv import load_dotenv
from openai import OpenAI
import google.generativeai
import anthropic
164/3: import gradio as gr # oh yeah!
164/4:
# Load environment variables in a file called .env
# Print the key prefixes to help with any debugging

load_dotenv()
openai_api_key = os.getenv('OPENAI_API_KEY')
anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')
google_api_key = os.getenv('GOOGLE_API_KEY')

if openai_api_key:
    print(f"OpenAI API Key exists and begins {openai_api_key[:8]}")
else:
    print("OpenAI API Key not set")
    
if anthropic_api_key:
    print(f"Anthropic API Key exists and begins {anthropic_api_key[:7]}")
else:
    print("Anthropic API Key not set")

if google_api_key:
    print(f"Google API Key exists and begins {google_api_key[:8]}")
else:
    print("Google API Key not set")
164/5:
# Connect to OpenAI, Anthropic and Google; comment out the Claude or Google lines if you're not using them

openai = OpenAI()

claude = anthropic.Anthropic()

google.generativeai.configure()
164/6:
# A generic system message - no more snarky adversarial AIs!

system_message = "You are a helpful assistant"
164/7: message_gpt("What is today's date?")
164/8: message_gpt("What is today's date?")
164/9:
# Let's wrap a call to GPT-4o-mini in a simple function

def message_gpt(prompt):
    messages = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": prompt}
      ]
    completion = openai.chat.completions.create(
        model='gpt-4o-mini',
        messages=messages,
    )
    return completion.choices[0].message.content
164/10: message_gpt("What is today's date?")
164/11:
# here's a simple function

def shout(text):
    print(f"Shout has been called with input {text}")
    return text.upper()
164/12: shout("hello")
164/13:
# The simplicty of gradio. This might appear in "light mode" - I'll show you how to make this in dark mode later.

gr.Interface(fn=shout, inputs="textbox", outputs="textbox").launch()
164/14:
# The simplicty of gradio. This might appear in "light mode" - I'll show you how to make this in dark mode later.

gr.Interface(fn=shout, inputs="textbox", outputs="textbox").launch(share=True)
164/15:
# Define this variable and then pass js=force_dark_mode when creating the Interface

force_dark_mode = """
function refresh() {
    const url = new URL(window.location);
    if (url.searchParams.get('__theme') !== 'dark') {
        url.searchParams.set('__theme', 'dark');
        window.location.href = url.href;
    }
}
"""
gr.Interface(fn=shout, inputs="textbox", outputs="textbox", flagging_mode="never", js=force_dark_mode).launch()
164/16:
# And now - changing the function from "shout" to "message_gpt"

view = gr.Interface(
    fn=message_gpt,
    inputs=[gr.Textbox(label="Your message:", lines=6)],
    outputs=[gr.Textbox(label="Response:", lines=8)],
    flagging_mode="never"
)
view.launch()
171/1:
a = [1,2,3,4,5,6,7,8]
print(a[:-3])
171/2:
a = [1,2,3,4,5,6,7,8]
print(a[:3])
171/3:
a = [1,2,3,4,5,6,7,8]
print(a[:-3])
171/4: context.keys()
171/5:
# imports

import os
import glob
from dotenv import load_dotenv
import gradio as gr
from openai import OpenAI
171/6:
# price is a factor for our company, so we're going to use a low cost model

MODEL = "gpt-4o-mini"
171/7:
# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
openai = OpenAI()
171/8:
# With massive thanks to student Dr John S. for fixing a bug in the below for Windows users!

context = {}

employees = glob.glob("knowledge-base/employees/*")

for employee in employees:
    name = employee.split(' ')[-1][:-3]
    doc = ""
    with open(employee, "r", encoding="utf-8") as f:
        doc = f.read()
    context[name]=doc
171/9: employees
171/10: type(employees)
171/11: employee
171/12: employee.split(' ')[-1][-3]
171/13: employee
171/14: employee.split(' ')[-1][-3]
171/15: employee.split(' ')[-1][:-3]
171/16:
# With massive thanks to student Dr John S. for fixing a bug in the below for Windows users!

context = {}

employees = glob.glob("knowledge-base/employees/*")

for employee in employees:
    name = employee.split(' ')[-1][:-3]
    doc = ""
    with open(employee, "r", encoding="utf-8") as f:
        doc = f.read()
    context[name]=doc
171/17: employee.split(' ')[-1][:-3]
171/18: context["Lancaster"]
171/19:
# With massive thanks to student Dr John S. for fixing a bug in the below for Windows users!

context = {}

employees = glob.glob("knowledge-base/employees/*")

for employee in employees:
    name = employee.split(' ')[-1][:-3]
    doc = ""
    with open(employee, "r", encoding="utf-8") as f:
        doc = f.read()
    context[name]=doc
171/20: employee.split(' ')[-1][:-3]
171/21: context["Lancaster"]
171/22:
products = glob.glob("knowledge-base/products/*")

for product in products:
    name = product.split(os.sep)[-1][:-3]
    doc = ""
    with open(product, "r", encoding="utf-8") as f:
        doc = f.read()
    context[name]=doc
171/23: context.keys()
171/24: context
171/25: employee.split(' ')[-1][:-3]
171/26: context["Lancaster"]
171/27:
products = glob.glob("knowledge-base/products/*")

for product in products:
    name = product.split(os.sep)[-1][:-3]
    doc = ""
    with open(product, "r", encoding="utf-8") as f:
        doc = f.read()
    context[name]=doc
171/28: context
171/29: context.keys()
171/30: context.items()
171/31: context.keys()
171/32:
exDic= {"a":1,"b",2}
exDic.items()
171/33:
exDic= {"a":1,"b":2}
exDic.items()
171/34:
exDic= {"a":1,"b":2}
type(exDic.items())
171/35:
exDic= {"a":1,"b":2}
exDic.items()
171/36: get_relevant_context("Who is Avery and what is carllm?")
171/37: system_message = "You are an expert in answering accurate questions about Insurellm, the Insurance Tech company. Give brief, accurate answers. If you don't know the answer, say so. Do not make anything up if you haven't been provided with relevant context."
171/38:
def get_relevant_context(message):
    relevant_context = []
    for context_title, context_details in context.items():
        if context_title.lower() in message.lower():
            relevant_context.append(context_details)
    return relevant_context
171/39: get_relevant_context("Who is lancaster?")
171/40:
def get_relevant_context(message):
    relevant_context = []
    for context_title, context_details in context.items():
        if context_title.lower() in message.lower():
            relevant_context.append(context_details)
    return relevant_context
171/41: get_relevant_context("Who is lancaster?")
171/42: get_relevant_context("Who is lancaster?")
171/43: get_relevant_context("Who is Avery and what is carllm?")
171/44: get_relevant_context("Who is Avery and what is arllm?")
171/45: get_relevant_context("Who is Avery and what is carllm?")
171/46: get_relevant_context("Who is Avery and what is crllm?")
171/47:
def add_context(message):
    relevant_context = get_relevant_context(message)
    if relevant_context:
        message += "\n\nThe following additional context might be relevant in answering this question:\n\n"
        for relevant in relevant_context:
            message += relevant + "\n\n"
    return message
171/48:
def add_context(message):
    relevant_context = get_relevant_context(message)
    if relevant_context:
        message += "\n\nThe following additional context might be relevant in answering this question:\n\n"
        for relevant in relevant_context:
            message += relevant + "\n\n"
    return message
171/49: print(add_context("Who is Alex Lancaster?"))
171/50:
def chat(message, history):
    messages = [{"role": "system", "content": system_message}] + history
    message = add_context(message)
    messages.append({"role": "user", "content": message})

    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)

    response = ""
    for chunk in stream:
        response += chunk.choices[0].delta.content or ''
        yield response
171/51: view = gr.ChatInterface(chat, type="messages").launch()
171/52: get_relevant_context("Who is c")
171/53:
def chat(message, history):
    messages = [{"role": "system", "content": system_message}] + history
    message = add_context(message)
    messages.append({"role": "user", "content": message})

    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)

    response = ""
    for chunk in stream:
        response += chunk.choices[0].delta.content or ''
        yield response
171/54: view = gr.ChatInterface(chat, type="messages").launch()
174/1:
# imports

import os
import glob

from dotenv import load_dotenv
import gradio as gr
174/2:
# imports

import os
import glob
from dotenv import load_dotenv
import gradio as gr
174/3:
# imports for langchain

from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
174/4:
# price is a factor for our company, so we're going to use a low cost model

MODEL = "gpt-4o-mini"
db_name = "vector_db"
174/5:
# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
174/6:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(documents)
174/7:
# Read in documents using LangChain's loaders
# Take everything in all the sub-folders of our knowledgebase

folders = glob.glob("knowledge-base/*")

# With thanks to CG and Jon R, students on the course, for this fix needed for some users 
text_loader_kwargs = {'encoding': 'utf-8'}
# If that doesn't work, some Windows users might need to uncomment the next line instead
# text_loader_kwargs={'autodetect_encoding': True}

documents = []
for folder in folders:
    doc_type = os.path.basename(folder)
    loader = DirectoryLoader(folder, glob="**/*.md", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)
    folder_docs = loader.load()
    for doc in folder_docs:
        doc.metadata["doc_type"] = doc_type
        documents.append(doc)
174/8: len(documents)
174/9: documents[24]
174/10:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(documents)
174/11: len(chunks)
174/12: chunks[6]
174/13:
doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)
print(f"Document types found: {', '.join(doc_types)}")
174/14:
for chunk in chunks:
    if 'CEO' in chunk.page_content:
        print(chunk)
        print("_________")
174/15: chunks[3]
174/16: chunks[4]
175/1:
# imports for langchain and Chroma and plotly

from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
import numpy as np
from sklearn.manifold import TSNE
import plotly.graph_objects as go
175/2:
# price is a factor for our company, so we're going to use a low cost model

MODEL = "gpt-4o-mini"
db_name = "vector_db"
175/3:
# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
175/4:
# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
175/5:
# imports

import os
import glob
from dotenv import load_dotenv
import gradio as gr
175/6:
# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
175/7:
# Read in documents using LangChain's loaders
# Take everything in all the sub-folders of our knowledgebase

folders = glob.glob("knowledge-base/*")

# With thanks to CG and Jon R, students on the course, for this fix needed for some users 
text_loader_kwargs = {'encoding': 'utf-8'}
# If that doesn't work, some Windows users might need to uncomment the next line instead
# text_loader_kwargs={'autodetect_encoding': True}

documents = []
for folder in folders:
    doc_type = os.path.basename(folder)
    loader = DirectoryLoader(folder, glob="**/*.md", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)
    folder_docs = loader.load()
    for doc in folder_docs:
        doc.metadata["doc_type"] = doc_type
        documents.append(doc)
175/8:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(documents)
175/9: len(chunks)
175/10:
doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)
print(f"Document types found: {', '.join(doc_types)}")
175/11:
doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)
print(f"Document types found: {', '.join(doc_types)}")
175/12:
doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)
print(f"Document types found: {', '.join(doc_types)}")
175/13:
# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk

embeddings = OpenAIEmbeddings()
175/14:
# Check if a Chroma Datastore already exists - if so, delete the collection to start from scratch

if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()
175/15:
# Create our Chroma vectorstore!

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)
print(f"Vectorstore created with {vectorstore._collection.count()} documents")
175/16:
# Get one vector and find how many dimensions it has

collection = vectorstore._collection
sample_embedding = collection.get(limit=1, include=["embeddings"])["embeddings"][0]
dimensions = len(sample_embedding)
print(f"The vectors have {dimensions:,} dimensions")
175/17:
# Get one vector and find how many dimensions it has

collection = vectorstore._collection
sample_embedding = collection.get(limit=1, include=["embeddings"])["embeddings"][0]
dimensions = len(sample_embedding)
print(sample_embedding)
print(f"The vectors have {dimensions:,} dimensions")
175/18:
# Get one vector and find how many dimensions it has

collection = vectorstore._collection
sample_embedding = collection.get(limit=1, include=["embeddings"])["embeddings"][0]
dimensions = len(sample_embedding)
print(f"The vectors have {dimensions:,} dimensions")
175/19:
# Prework

result = collection.get(include=['embeddings', 'documents', 'metadatas'])
vectors = np.array(result['embeddings'])
documents = result['documents']
doc_types = [metadata['doc_type'] for metadata in result['metadatas']]
colors = [['blue', 'green', 'red', 'orange'][['products', 'employees', 'contracts', 'company'].index(t)] for t in doc_types]
175/20:
# We humans find it easier to visalize things in 2D!
# Reduce the dimensionality of the vectors to 2D using t-SNE
# (t-distributed stochastic neighbor embedding)

tsne = TSNE(n_components=2, random_state=42)
reduced_vectors = tsne.fit_transform(vectors)

# Create the 2D scatter plot
fig = go.Figure(data=[go.Scatter(
    x=reduced_vectors[:, 0],
    y=reduced_vectors[:, 1],
    mode='markers',
    marker=dict(size=5, color=colors, opacity=0.8),
    text=[f"Type: {t}<br>Text: {d[:100]}..." for t, d in zip(doc_types, documents)],
    hoverinfo='text'
)])

fig.update_layout(
    title='2D Chroma Vector Store Visualization',
    scene=dict(xaxis_title='x',yaxis_title='y'),
    width=800,
    height=600,
    margin=dict(r=20, b=10, l=10, t=40)
)

fig.show()
175/21:
# Let's try 3D!

tsne = TSNE(n_components=3, random_state=42)
reduced_vectors = tsne.fit_transform(vectors)

# Create the 3D scatter plot
fig = go.Figure(data=[go.Scatter3d(
    x=reduced_vectors[:, 0],
    y=reduced_vectors[:, 1],
    z=reduced_vectors[:, 2],
    mode='markers',
    marker=dict(size=5, color=colors, opacity=0.8),
    text=[f"Type: {t}<br>Text: {d[:100]}..." for t, d in zip(doc_types, documents)],
    hoverinfo='text'
)])

fig.update_layout(
    title='3D Chroma Vector Store Visualization',
    scene=dict(xaxis_title='x', yaxis_title='y', zaxis_title='z'),
    width=900,
    height=700,
    margin=dict(r=20, b=10, l=10, t=40)
)

fig.show()
176/1:
# imports

import os
import glob
from dotenv import load_dotenv
import gradio as gr
176/2:
# imports for langchain

from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
import numpy as np
from sklearn.manifold import TSNE
import plotly.graph_objects as go
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
176/3:
# price is a factor for our company, so we're going to use a low cost model

MODEL = "gpt-4o-mini"
db_name = "vector_db"
176/4:
# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
176/5:
# Read in documents using LangChain's loaders
# Take everything in all the sub-folders of our knowledgebase

folders = glob.glob("knowledge-base/*")

# With thanks to CG and Jon R, students on the course, for this fix needed for some users 
text_loader_kwargs = {'encoding': 'utf-8'}
# If that doesn't work, some Windows users might need to uncomment the next line instead
# text_loader_kwargs={'autodetect_encoding': True}

documents = []
for folder in folders:
    doc_type = os.path.basename(folder)
    loader = DirectoryLoader(folder, glob="**/*.md", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)
    folder_docs = loader.load()
    for doc in folder_docs:
        doc.metadata["doc_type"] = doc_type
        documents.append(doc)
176/6:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(documents)
176/7: len(chunks)
176/8:
doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)
print(f"Document types found: {', '.join(doc_types)}")
176/9:
# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk
# Chroma is a popular open source Vector Database based on SQLLite

embeddings = OpenAIEmbeddings()

# Delete if already exists

if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

# Create vectorstore

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)
print(f"Vectorstore created with {vectorstore._collection.count()} documents")
176/10:
# Get one vector and find how many dimensions it has

collection = vectorstore._collection
sample_embedding = collection.get(limit=1, include=["embeddings"])["embeddings"][0]
dimensions = len(sample_embedding)
print(f"The vectors have {dimensions:,} dimensions")
176/11:
# Prework

result = collection.get(include=['embeddings', 'documents', 'metadatas'])
vectors = np.array(result['embeddings'])
documents = result['documents']
doc_types = [metadata['doc_type'] for metadata in result['metadatas']]
colors = [['blue', 'green', 'red', 'orange'][['products', 'employees', 'contracts', 'company'].index(t)] for t in doc_types]
176/12:
# We humans find it easier to visalize things in 2D!
# Reduce the dimensionality of the vectors to 2D using t-SNE
# (t-distributed stochastic neighbor embedding)

tsne = TSNE(n_components=2, random_state=42)
reduced_vectors = tsne.fit_transform(vectors)

# Create the 2D scatter plot
fig = go.Figure(data=[go.Scatter(
    x=reduced_vectors[:, 0],
    y=reduced_vectors[:, 1],
    mode='markers',
    marker=dict(size=5, color=colors, opacity=0.8),
    text=[f"Type: {t}<br>Text: {d[:100]}..." for t, d in zip(doc_types, documents)],
    hoverinfo='text'
)])

fig.update_layout(
    title='2D Chroma Vector Store Visualization',
    scene=dict(xaxis_title='x',yaxis_title='y'),
    width=800,
    height=600,
    margin=dict(r=20, b=10, l=10, t=40)
)

fig.show()
176/13:
# Let's try 3D!

tsne = TSNE(n_components=3, random_state=42)
reduced_vectors = tsne.fit_transform(vectors)

# Create the 3D scatter plot
fig = go.Figure(data=[go.Scatter3d(
    x=reduced_vectors[:, 0],
    y=reduced_vectors[:, 1],
    z=reduced_vectors[:, 2],
    mode='markers',
    marker=dict(size=5, color=colors, opacity=0.8),
    text=[f"Type: {t}<br>Text: {d[:100]}..." for t, d in zip(doc_types, documents)],
    hoverinfo='text'
)])

fig.update_layout(
    title='3D Chroma Vector Store Visualization',
    scene=dict(xaxis_title='x', yaxis_title='y', zaxis_title='z'),
    width=900,
    height=700,
    margin=dict(r=20, b=10, l=10, t=40)
)

fig.show()
176/14:
query = "Can you describe Insurellm in a few sentences"
result = conversation_chain.invoke({"question":query})
print(result["answer"])
176/15:
# create a new Chat with OpenAI
llm = ChatOpenAI(temperature=0.7, model_name=MODEL)

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)
176/16:
query = "Can you describe Insurellm in a few sentences"
result = conversation_chain.invoke({"question":query})
print(result["answer"])
176/17:
# set up a new conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)
176/18:
# Wrapping in a function - note that history isn't used, as the memory is in the conversation_chain

def chat(message, history):
    result = conversation_chain.invoke({"question": message})
    return result["answer"]
176/19:
# And in Gradio:

view = gr.ChatInterface(chat, type="messages").launch(inbrowser=True)
177/1:
# imports

import os
import glob
from dotenv import load_dotenv
import gradio as gr
177/2:
# imports for langchain, plotly and Chroma

from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
import numpy as np
import plotly.graph_objects as go
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
177/3:
# price is a factor for our company, so we're going to use a low cost model

MODEL = "gpt-4o-mini"
db_name = "vector_db"
177/4:
# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
178/1:
# imports

import os
import glob
from dotenv import load_dotenv
import gradio as gr
178/2:
# imports for langchain

from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
# from langchain_chroma import Chroma
from langchain.vectorstores import FAISS
import numpy as np
from sklearn.manifold import TSNE
import plotly.graph_objects as go
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
178/3:
# price is a factor for our company, so we're going to use a low cost model

MODEL = "gpt-4o-mini"
db_name = "vector_db"
178/4:
# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
178/5:
# Read in documents using LangChain's loaders
# Take everything in all the sub-folders of our knowledgebase

folders = glob.glob("knowledge-base/*")

# With thanks to CG and Jon R, students on the course, for this fix needed for some users 
text_loader_kwargs = {'encoding': 'utf-8'}
# If that doesn't work, some Windows users might need to uncomment the next line instead
# text_loader_kwargs={'autodetect_encoding': True}

documents = []
for folder in folders:
    doc_type = os.path.basename(folder)
    loader = DirectoryLoader(folder, glob="**/*.md", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)
    folder_docs = loader.load()
    for doc in folder_docs:
        doc.metadata["doc_type"] = doc_type
        documents.append(doc)
178/6:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(documents)
178/7: len(chunks)
178/8:
doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)
print(f"Document types found: {', '.join(doc_types)}")
178/9:
# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk
# Chroma is a popular open source Vector Database based on SQLLite

embeddings = OpenAIEmbeddings()

# Create vectorstore

# BEFORE
# vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)

# AFTER
vectorstore = FAISS.from_documents(chunks, embedding=embeddings)

total_vectors = vectorstore.index.ntotal
dimensions = vectorstore.index.d

print(f"There are {total_vectors} vectors with {dimensions:,} dimensions in the vector store")
178/10:
# Prework
vectors = []
documents = []
doc_types = []
colors = []
color_map = {'products':'blue', 'employees':'green', 'contracts':'red', 'company':'orange'}

for i in range(total_vectors):
    vectors.append(vectorstore.index.reconstruct(i))
    doc_id = vectorstore.index_to_docstore_id[i]
    document = vectorstore.docstore.search(doc_id)
    documents.append(document.page_content)
    doc_type = document.metadata['doc_type']
    doc_types.append(doc_type)
    colors.append(color_map[doc_type])
    
vectors = np.array(vectors)
178/11:
# We humans find it easier to visalize things in 2D!
# Reduce the dimensionality of the vectors to 2D using t-SNE
# (t-distributed stochastic neighbor embedding)

tsne = TSNE(n_components=2, random_state=42)
reduced_vectors = tsne.fit_transform(vectors)

# Create the 2D scatter plot
fig = go.Figure(data=[go.Scatter(
    x=reduced_vectors[:, 0],
    y=reduced_vectors[:, 1],
    mode='markers',
    marker=dict(size=5, color=colors, opacity=0.8),
    text=[f"Type: {t}<br>Text: {d[:100]}..." for t, d in zip(doc_types, documents)],
    hoverinfo='text'
)])

fig.update_layout(
    title='2D FAISS Vector Store Visualization',
    scene=dict(xaxis_title='x',yaxis_title='y'),
    width=800,
    height=600,
    margin=dict(r=20, b=10, l=10, t=40)
)

fig.show()
178/12:
# Let's try 3D!

tsne = TSNE(n_components=3, random_state=42)
reduced_vectors = tsne.fit_transform(vectors)

# Create the 3D scatter plot
fig = go.Figure(data=[go.Scatter3d(
    x=reduced_vectors[:, 0],
    y=reduced_vectors[:, 1],
    z=reduced_vectors[:, 2],
    mode='markers',
    marker=dict(size=5, color=colors, opacity=0.8),
    text=[f"Type: {t}<br>Text: {d[:100]}..." for t, d in zip(doc_types, documents)],
    hoverinfo='text'
)])

fig.update_layout(
    title='3D FAISS Vector Store Visualization',
    scene=dict(xaxis_title='x', yaxis_title='y', zaxis_title='z'),
    width=900,
    height=700,
    margin=dict(r=20, b=10, l=10, t=40)
)

fig.show()
178/13:
# create a new Chat with OpenAI
llm = ChatOpenAI(temperature=0.7, model_name=MODEL)

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)
178/14:
query = "Can you describe Insurellm in a few sentences"
result = conversation_chain.invoke({"question":query})
print(result["answer"])
177/5:
# imports

import os
import glob
from dotenv import load_dotenv
import gradio as gr
177/6:
# imports for langchain, plotly and Chroma

from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
import numpy as np
import plotly.graph_objects as go
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
177/7:
# price is a factor for our company, so we're going to use a low cost model

MODEL = "gpt-4o-mini"
db_name = "vector_db"
177/8:
# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
177/9:
# Read in documents using LangChain's loaders
# Take everything in all the sub-folders of our knowledgebase

folders = glob.glob("knowledge-base/*")

def add_metadata(doc, doc_type):
    doc.metadata["doc_type"] = doc_type
    return doc

# With thanks to CG and Jon R, students on the course, for this fix needed for some users 
text_loader_kwargs = {'encoding': 'utf-8'}
# If that doesn't work, some Windows users might need to uncomment the next line instead
# text_loader_kwargs={'autodetect_encoding': True}

documents = []
for folder in folders:
    doc_type = os.path.basename(folder)
    loader = DirectoryLoader(folder, glob="**/*.md", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)
    folder_docs = loader.load()
    documents.extend([add_metadata(doc, doc_type) for doc in folder_docs])

text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(documents)

print(f"Total number of chunks: {len(chunks)}")
print(f"Document types found: {set(doc.metadata['doc_type'] for doc in documents)}")
177/10:
# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk
# Chroma is a popular open source Vector Database based on SQLLite

embeddings = OpenAIEmbeddings()

# Delete if already exists

if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

# Create vectorstore

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)
print(f"Vectorstore created with {vectorstore._collection.count()} documents")
177/11:
# Let's investigate the vectors

collection = vectorstore._collection
count = collection.count()

sample_embedding = collection.get(limit=1, include=["embeddings"])["embeddings"][0]
dimensions = len(sample_embedding)
print(f"There are {count:,} vectors with {dimensions:,} dimensions in the vector store")
177/12:
# create a new Chat with OpenAI
llm = ChatOpenAI(temperature=0.7, model_name=MODEL)

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)
177/13:
# Let's try a simple question

query = "Please explain what Insurellm is in a couple of sentences"
result = conversation_chain.invoke({"question": query})
print(result["answer"])
177/14:
# set up a new conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)
177/15:
# Wrapping that in a function

def chat(question, history):
    result = conversation_chain.invoke({"question": question})
    return result["answer"]
177/16:
# And in Gradio:

view = gr.ChatInterface(chat, type="messages").launch(inbrowser=True)
177/17:
# Let's investigate what gets sent behind the scenes

from langchain_core.callbacks import StdOutCallbackHandler

llm = ChatOpenAI(temperature=0.7, model_name=MODEL)

memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

retriever = vectorstore.as_retriever()

conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory, callbacks=[StdOutCallbackHandler()])

query = "Who received the prestigious IIOTY award in 2023?"
result = conversation_chain.invoke({"question": query})
answer = result["answer"]
print("\nAnswer:", answer)
179/1: !pip install pypdf
179/2: from langchain_community.document_loaders import PyPDFLoader
179/3: loader = PyPDFLoader.("pdfdata/monopoly.pdf")
179/4:
loader = PyPDFLoader("pdfdata/monopoly.pdf")
type(loader)
179/5: pages
179/6:
loader = PyPDFLoader("pdfdata/monopoly.pdf")
pages = loader.load_and_split()
179/7: pages
179/8: pages[0]
179/9: An advantage of this approach is that documents can be retrieved with page numbers.
179/10: #An advantage of this approach is that documents can be retrieved with page numbers.
179/11: from dotenv import load_dotenv
179/12: load_dotenv()
179/13: load_dotenv()
179/14:
from dotenv import load_dotenv
import os
179/15: api_key = os.getenv("OPENAI_API_KEY")
179/16: apikey
179/17: api_key
179/18:
from dotenv import load_dotenv
import os
import ollama
179/19: OLLAMA_API
181/1: MODEL
181/2:
# And now: call the OpenAI API. You will get very familiar with this!

def summarize(url):
    website = Website(url)
  #  response = ollama.chat(model=MODEL, messages=messages)
    response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
    print(response.json()['message']['content'])
181/3: summarize()
181/4: summarize("https://www.tryhackme.com")
181/5: summarize("https://tryhackme.com")
181/6:
# imports

import requests
from bs4 import BeautifulSoup
from IPython.display import Markdown, display
181/7:
# Create a messages list using the same format that we used for OpenAI

messages = [
    {"role": "user", "content": "Describe some of the business applications of Generative AI"}
]
181/8:
payload =  { "model": MODEL,
             "messages": messages,
             "stream": False
           }
181/9:
payload =  { "model": "llama3.2",
             "messages": messages,
             "stream": False
           }
181/10:
response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
print(response.json()['message']['content'])
181/11:
messages = mess
payload = { "model":MODEL,
            "messages": messages,
            "stream": False
          }
181/12: OLLAMA_API =  "http://localhost:11434"
181/13:
response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
print(response.json()['message']['content'])
181/14:
OLLAMA_API =  "http://localhost:11434/api/chat"
HEADERS = {"Content-Type": "application/json"}
MODEL = "llama3.2"
181/15:
response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
print(response.json()['message']['content'])
181/16:
response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
print(response.json()['message']['content'])
181/17:
# imports

import requests
from bs4 import BeautifulSoup
from IPython.display import Markdown, display
181/18:
# Create a messages list using the same format that we used for OpenAI

messages = [
    {"role": "user", "content": "Describe some of the business applications of Generative AI"}
]
181/19:
payload =  { "model": "llama3.2",
             "messages": messages,
             "stream": False
           }
181/20:
OLLAMA_API =  "http://localhost:11434/api/chat"
HEADERS = {"Content-Type": "application/json"}
MODEL = "llama3.2"
181/21:
response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
print(response.json()['message']['content'])
181/22:
import ollama
response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
print(response.json()['message']['content'])
182/1: !pip install -U deepeval
182/2: !deepeval login
186/1:
%pip install --q unstructured langchain langchain-community
%pip install --q "unstructured[all-docs]" ipywidgets tqdm
187/1:
%pip install --q unstructured langchain langchain-community
%pip install --q "unstructured[all-docs]" ipywidgets tqdm
187/2:
from langchain_community.document_loaders import UnstructuredPDFLoader
from IPython.display import display as Markdown
from tqdm.autonotebook import tqdm as notebook_tqdm
188/1:
%pip install --q unstructured langchain langchain-community
%pip install --q "unstructured[all-docs]" ipywidgets tqdm
188/2:
from langchain_community.document_loaders import UnstructuredPDFLoader
from IPython.display import display as Markdown
from tqdm.autonotebook import tqdm as notebook_tqdm
188/3:
local_path = "WEF_The_Global_Cooperation_Barometer_2024.pdf"

# Local PDF file uploads
if local_path:
  loader = UnstructuredPDFLoader(file_path=local_path)
  data = loader.load()
else:
  print("Upload a PDF file")
188/4:
# Preview first page
Markdown(data[0].page_content)
188/5: !ollama list
188/6:
# 1. First clean up any existing ChromaDB installations
%pip uninstall -y chromadb
%pip uninstall -y protobuf

# 2. Install specific versions known to work together
%pip install -q protobuf==3.20.3
%pip install -q chromadb==0.4.22  # Using a stable older version
%pip install -q langchain-ollama

# 3. Set the environment variable
import os
os.environ["PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION"] = "python"
188/7:
from langchain_ollama import OllamaEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
188/8:
# Split and chunk 
text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)
chunks = text_splitter.split_documents(data)
189/1:
# 1. First clean up any existing ChromaDB installations
%pip uninstall -y chromadb
%pip uninstall -y protobuf

# 2. Install specific versions known to work together
%pip install -q protobuf==3.20.3
%pip install -q chromadb==0.4.22  # Using a stable older version
%pip install -q langchain-ollama

# 3. Set the environment variable
import os
os.environ["PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION"] = "python"

# 4. Now reimport with the new versions
from langchain_ollama import OllamaEmbeddings
from langchain_community.vectorstores import Chroma

# 5. Try creating the vector database
vector_db = Chroma.from_documents(
    documents=chunks,
    embedding=OllamaEmbeddings(model="nomic-embed-text"),
    collection_name="local-rag"
)
189/2:
# LLM from Ollama
local_model = "llama3.2"
llm = ChatOllama(model=local_model)
189/3: !ollama list
189/4:
from langchain.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_ollama.chat_models import ChatOllama
from langchain_core.runnables import RunnablePassthrough
from langchain.retrievers.multi_query import MultiQueryRetriever
189/5:
# LLM from Ollama
local_model = "llama3.2"
llm = ChatOllama(model=local_model)
189/6:
# Split and chunk 
text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)
chunks = text_splitter.split_documents(data)
189/7:
from langchain_ollama import OllamaEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
189/8:
# Split and chunk 
text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)
chunks = text_splitter.split_documents(data)
189/9:
local_path = "WEF_The_Global_Cooperation_Barometer_2024.pdf"

# Local PDF file uploads
if local_path:
  loader = UnstructuredPDFLoader(file_path=local_path)
  data = loader.load()
else:
  print("Upload a PDF file")
189/10:
from langchain_community.document_loaders import UnstructuredPDFLoader
from IPython.display import display as Markdown
from tqdm.autonotebook import tqdm as notebook_tqdm
189/11:
local_path = "WEF_The_Global_Cooperation_Barometer_2024.pdf"

# Local PDF file uploads
if local_path:
  loader = UnstructuredPDFLoader(file_path=local_path)
  data = loader.load()
else:
  print("Upload a PDF file")
189/12:
# Preview first page
Markdown(data[0].page_content)
189/13:
from langchain_ollama import OllamaEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
189/14:
# 1. First clean up any existing ChromaDB installations
%pip uninstall -y chromadb
%pip uninstall -y protobuf

# 2. Install specific versions known to work together
%pip install -q protobuf==3.20.3
%pip install -q chromadb==0.4.22  # Using a stable older version
%pip install -q langchain-ollama

# 3. Set the environment variable
import os
os.environ["PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION"] = "python"
189/15:
# Split and chunk 
text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)
chunks = text_splitter.split_documents(data)
189/16:
from langchain_community.document_loaders import UnstructuredPDFLoader
from IPython.display import display as Markdown
from tqdm.autonotebook import tqdm as notebook_tqdm
189/17:
local_path = "WEF_The_Global_Cooperation_Barometer_2024.pdf"

# Local PDF file uploads
if local_path:
  loader = UnstructuredPDFLoader(file_path=local_path)
  data = loader.load()
else:
  print("Upload a PDF file")
189/18:
# Preview first page
Markdown(data[0].page_content)
189/19: !ollama list
189/20:
# # Pull nomic-embed-text model from Ollama if you don't have it
# !ollama pull nomic-embed-text
# # List models again to confirm it's available
# !ollama list
189/21:
# 1. First clean up any existing ChromaDB installations
%pip uninstall -y chromadb
%pip uninstall -y protobuf

# 2. Install specific versions known to work together
%pip install -q protobuf==3.20.3
%pip install -q chromadb==0.4.22  # Using a stable older version
%pip install -q langchain-ollama

# 3. Set the environment variable
import os
os.environ["PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION"] = "python"
189/22:
from langchain_ollama import OllamaEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
189/23:
# Split and chunk 
text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)
chunks = text_splitter.split_documents(data)
189/24:
# Split and chunk 
text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)
chunks = text_splitter.split_documents(data)
189/25:
# Split and chunk 
text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)
chunks = text_splitter.split_documents(data)
189/26:
# 1. First clean up any existing ChromaDB installations
%pip uninstall -y chromadb
%pip uninstall -y protobuf

# 2. Install specific versions known to work together
%pip install -q protobuf==3.20.3
%pip install -q chromadb==0.4.22  # Using a stable older version
%pip install -q langchain-ollama

# 3. Set the environment variable
import os
os.environ["PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION"] = "python"

# 4. Now reimport with the new versions
from langchain_ollama import OllamaEmbeddings
from langchain_community.vectorstores import Chroma

# 5. Try creating the vector database
vector_db = Chroma.from_documents(
    documents=chunks,
    embedding=OllamaEmbeddings(model="nomic-embed-text"),
    collection_name="local-rag"
)
189/27:
from langchain.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_ollama.chat_models import ChatOllama
from langchain_core.runnables import RunnablePassthrough
from langchain.retrievers.multi_query import MultiQueryRetriever
189/28: !ollama list
189/29:
QUERY_PROMPT = PromptTemplate(
    input_variables=["question"],
    template="""You are an AI language model assistant. Your task is to generate five
    different versions of the given user question to retrieve relevant documents from
    a vector database. By generating multiple perspectives on the user question, your
    goal is to help the user overcome some of the limitations of the distance-based
    similarity search. Provide these alternative questions separated by newlines.
    Original question: {question}""",
)
190/1:
%source venv/bin/activate
%pip install --q unstructured langchain langchain-community
%pip install --q "unstructured[all-docs]" ipywidgets tqdm
190/2:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows 
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
190/3:
!source venv/bin/activate
%pip install --q unstructured langchain langchain-community
%pip install --q "unstructured[all-docs]" ipywidgets tqdm
190/4:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows 
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
190/5:
from langchain_community.document_loaders import UnstructuredPDFLoader
from IPython.display import display as Markdown
from tqdm.autonotebook import tqdm as notebook_tqdm
190/6:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows 
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
190/7:
local_path = "WEF_The_Global_Cooperation_Barometer_2024.pdf"

# Local PDF file uploads
if local_path:
  loader = UnstructuredPDFLoader(file_path=local_path)
  data = loader.load()
else:
  print("Upload a PDF file")
190/8:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows 
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
190/9:
# Preview first page
Markdown(data[0].page_content)
190/10:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows 
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
191/1:
# imports

import os
import requests
from bs4 import BeautifulSoup
from typing import List
from dotenv import load_dotenv
from openai import OpenAI
import google.generativeai
import anthropic
191/2: import gradio as gr # oh yeah!
191/3:
# Load environment variables in a file called .env
# Print the key prefixes to help with any debugging

load_dotenv()
openai_api_key = os.getenv('OPENAI_API_KEY')
anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')
google_api_key = os.getenv('GOOGLE_API_KEY')

if openai_api_key:
    print(f"OpenAI API Key exists and begins {openai_api_key[:8]}")
else:
    print("OpenAI API Key not set")
    
if anthropic_api_key:
    print(f"Anthropic API Key exists and begins {anthropic_api_key[:7]}")
else:
    print("Anthropic API Key not set")

if google_api_key:
    print(f"Google API Key exists and begins {google_api_key[:8]}")
else:
    print("Google API Key not set")
191/4:
# Connect to OpenAI, Anthropic and Google; comment out the Claude or Google lines if you're not using them

openai = OpenAI()

claude = anthropic.Anthropic()

google.generativeai.configure()
191/5:
# A generic system message - no more snarky adversarial AIs!

system_message = "You are a helpful assistant"
191/6:
# Let's wrap a call to GPT-4o-mini in a simple function

def message_gpt(prompt):
    messages = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": prompt}
      ]
    completion = openai.chat.completions.create(
        model='gpt-4o-mini',
        messages=messages,
    )
    return completion.choices[0].message.content
191/7: message_gpt("What is today's date?")
191/8:
# Define this variable and then pass js=force_dark_mode when creating the Interface

force_dark_mode = """
function refresh() {
    const url = new URL(window.location);
    if (url.searchParams.get('__theme') !== 'dark') {
        url.searchParams.set('__theme', 'dark');
        window.location.href = url.href;
    }
}
"""
gr.Interface(fn=shout, inputs="textbox", outputs="textbox", flagging_mode="never", js=force_dark_mode).launch()
191/9:
# here's a simple function

def shout(text):
    print(f"Shout has been called with input {text}")
    return text.upper()
191/10: shout("hello")
191/11:
# The simplicty of gradio. This might appear in "light mode" - I'll show you how to make this in dark mode later.

gr.Interface(fn=shout, inputs="textbox", outputs="textbox").launch(share=True)
191/12:
# Inputs and Outputs

view = gr.Interface(
    fn=shout,
    inputs=[gr.Textbox(label="Your message:", lines=6)],
    outputs=[gr.Textbox(label="Response:", lines=8)],
    flagging_mode="never"
)
view.launch()
191/13:
# And now - changing the function from "shout" to "message_gpt"

view = gr.Interface(
    fn=message_gpt,
    inputs=[gr.Textbox(label="Your message:", lines=6)],
    outputs=[gr.Textbox(label="Response:", lines=8)],
    flagging_mode="never"
)
view.launch()
191/14:
# Let's create a call that streams back results
# If you'd like a refresher on Generators (the "yield" keyword),
# Please take a look at the Intermediate Python notebook in week1 folder.

def stream_gpt(prompt):
    messages = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": prompt}
      ]
    stream = openai.chat.completions.create(
        model='gpt-4o-mini',
        messages=messages,
        stream=True
    )
    result = ""
    for chunk in stream:
        result += chunk.choices[0].delta.content or ""
        yield result
192/1: requests.get("https://tryhackme.com").text
192/2:
# imports

#import os
import requests
#from dotenv import load_dotenv
from bs4 import BeautifulSoup
from IPython.display import Markdown, display


# If you get an error running this cell, then please head over to the troubleshooting notebook!
192/3: requests.get("https://tryhackme.com").text
192/4: Website("https://tryhackme.com").title
192/5:
class Website:
  def __init__(self,url):
      self.url = url
      response = requests.get(url)
      soup = BeatifulSoup(response.content, 'html-parser')
      self.title = soup.title.string if soup.title.string else "No Title Found"
192/6: Website("https://tryhackme.com").title
192/7:
# imports

#import os
import requests
#from dotenv import load_dotenv
from bs4 import BeautifulSoup
from IPython.display import Markdown, display


# If you get an error running this cell, then please head over to the troubleshooting notebook!
192/8:
class Website:
  def __init__(self,url):
      self.url = url
      response = requests.get(url)
      soup = BeatifulSoup(response.content, 'html-parser')
      self.title = soup.title.string if soup.title.string else "No Title Found"
192/9: messages = { "role" : "user", "content": "Please briefly summarize this website"}
192/10: Website("https://tryhackme.com").title
192/11:
obj = Website("https://tryhackme.com")
obj.title
192/12:
obj = Website("https://tryhackme.com")
obj.url
192/13:
class Website:
  def __init__(self,url):
      self.url = url
      response = requests.get(url)
      soup = BeatifulSoup(response.content, 'html.parser')
      self.title = soup.title.string if soup.title.string else "No Title Found"
192/14: messages = { "role" : "user", "content": "Please briefly summarize this website"}
192/15:
obj = Website("https://tryhackme.com")
obj.url
192/16:
class Website:
  def __init__(self,url):
      url:str
      self.url = url
      response = requests.get(url)
      soup = BeatifulSoup(response.content, 'html.parser')
      self.title = soup.title.string if soup.title else "No Title Found"
192/17: messages = { "role" : "user", "content": "Please briefly summarize this website"}
192/18:
obj = Website("https://tryhackme.com")
obj.url
192/19: obj = Website("https://tryhackme.com")
192/20:
# imports

#import os
import requests
#from dotenv import load_dotenv
from bs4 import BeautifulSoup
from IPython.display import Markdown, display


# If you get an error running this cell, then please head over to the troubleshooting notebook!
192/21:
class Website:
  def __init__(self,url):
      url:str
      self.url = url
      response = requests.get(url)
      soup = BeatifulSoup(response.content, 'html.parser')
      self.title = soup.title.string if soup.title else "No Title Found"
192/22: messages = { "role" : "user", "content": "Please briefly summarize this website"}
192/23: Website("https:localhost:11434")
192/24: Website("https://localhost:11434")
192/25: Website("http://localhost:11434")
192/26:
class Website:
  def __init__(self,url):
      url:str
      self.url = url
      response = requests.get(url)
      soup = BeautifulSoup(response.content, 'html.parser')
      self.title = soup.title.string if soup.title else "No Title Found"
192/27: messages = { "role" : "user", "content": "Please briefly summarize this website"}
192/28: Website("http://localhost:11434")
192/29: Website("http://localhost:11434").title
192/30: Website("https://tryhackme.com").title
192/31:
class Website:
  def __init__(self,url):
      url:str
      self.url = url
      response = requests.get(url)
      soup = BeautifulSoup(response.content, 'html.parser')
      self.title = soup.title.string if soup.title else "No Title Found"
      for irrelevant in soup.body(["script", "style","img","input"])
          irrelevant.decompose()

      self.text = soup.body.get_text(seperator = "\n", strip = True)
192/32:
class Website:
  def __init__(self,url):
      url:str
      self.url = url
      response = requests.get(url)
      soup = BeautifulSoup(response.content, 'html.parser')
      self.title = soup.title.string if soup.title else "No Title Found"
      for irrelevant in soup.body(["script", "style","img","input"]):
          irrelevant.decompose()

      self.text = soup.body.get_text(seperator = "\n", strip = True)
192/33:
Website("https://tryhackme.com").title
Website("https://tryhackme.com").text
192/34:
class Website:
  def __init__(self,url):
      url:str
      self.url = url
      response = requests.get(url)
      soup = BeautifulSoup(response.content, 'html.parser')
      self.title = soup.title.string if soup.title else "No Title Found"
      for irrelevant in soup.body(["script", "style","img","input"]):
          irrelevant.decompose()

      self.text = soup.body.get_text(separator = "\n", strip = True)
192/35: messages = { "role" : "user", "content": "Please briefly summarize this website"}
192/36:
Website("https://tryhackme.com").title
Website("https://tryhackme.com").text
192/37:

Website("https://tryhackme.com").text
192/38:
# Define our system prompt - you can experiment with this later, changing the last sentence to 'Respond in markdown in Spanish."

system_prompt = "You are an assistant that analyzes the contents of a website \
and provides a short summary, ignoring text that might be navigation related. \
Respond in markdown."
192/39:
def user_prompt_for(website):
    user_prompt = f"You are looking at a website titled {website.title}"
     user_prompt += "\nThe contents of this website is as follows; \
please provide a short summary of this website in markdown. \
If it includes news or announcements, then summarize these too.\n\n"
    user_prompt+=website.text
    return user_prompt
192/40:
def user_prompt_for(website):
    user_prompt = f"You are looking at a website titled {website.title}"
    user_prompt += "\nThe contents of this website is as follows; \
please provide a short summary of this website in markdown. \
If it includes news or announcements, then summarize these too.\n\n"
    user_prompt+=website.text
    return user_prompt
192/41:
WebObject = Website("https:tryhackme.com")
user_prompt_for(WebObject)
192/42:
WebObject = Website("https:tryhackme.com")
user_prompt_for(WebObject)
192/43:
WebObject = Website("https://tryhackme.com")
user_prompt_for(WebObject)
192/44:
WebObject = Website("https://tryhackme.com")
messages["content"] = user_prompt_for(WebObject)
192/45:
payload =  { "model": "llama3.2",
             "messages": messages,
             "stream": False
           }
192/46:
OLLAMA_API =  "http://localhost:11434/api/chat"
HEADERS = {"Content-Type": "application/json"}
MODEL = "llama3.2"
192/47:
import ollama
ollama_response = requests.get(OLLAMA_API,json=payload, headers=Headers)
print(ollama_response.json['message']['content']
192/48:
import ollama
ollama_response = requests.get(OLLAMA_API,json=payload, headers=Headers)
print(ollama_response.json()['message']['content'])
192/49:
import ollama
ollama_response = requests.get(OLLAMA_API,json=payload, headers=HEADERS)
print(ollama_response.json()['message']['content'])
192/50:
import ollama
ollama_response = requests.get(OLLAMA_API,json=payload, headers=HEADERS)
print(ollama_response.json()['message']['content'])
192/51:
import ollama
ollama_response = requests.post(OLLAMA_API,json=payload, headers=HEADERS)
print(ollama_response.json()['message']['content'])
192/52:
import ollama
ollama_response = requests.post(OLLAMA_API,json=payload, headers=HEADERS)
print(ollama_response.json()['messages']['content'])
192/53:
import ollama
ollama_response = requests.post(OLLAMA_API,json=payload, headers=HEADERS)
print(ollama_response.json()['message']['content'])
192/54:
import ollama
ollama_response = requests.post(OLLAMA_API,json=payload, headers=HEADERS)
192/55:
import ollama
ollama_response = requests.post(OLLAMA_API,json=payload, headers=HEADERS)
ollama_response.json()
192/56:
import ollama
ollama_response = requests.post(OLLAMA_API,json=payload, headers=HEADERS)
ollama_response.body
192/57:
import ollama
ollama_response = requests.post(OLLAMA_API,json=payload, headers=HEADERS)
ollama_response
192/58:
OLLAMA_API =  "http://localhost:11434"
HEADERS = {"Content-Type": "application/json"}
MODEL = "llama3.2"
192/59:
import ollama
ollama_response = requests.post(OLLAMA_API,json=payload, headers=HEADERS)
192/60:
import ollama
ollama_response = requests.post(OLLAMA_API,json=payload, headers=HEADERS)
ollama_response
192/61:
OLLAMA_API =  "http://localhost:11434/api/chat"
HEADERS = {"Content-Type": "application/json"}
MODEL = "llama3.2"
192/62:
OLLAMA_API =  "http://localhost:11434/api/chat"
HEADERS = {"Content-Type": "application/json"}
MODEL = "llama3.2"
192/63:
import ollama
ollama_response = requests.post(OLLAMA_API,json=payload, headers=HEADERS)
192/64:
# Define our system prompt - you can experiment with this later, changing the last sentence to 'Respond in markdown in Spanish."

system_prompt = "You are an assistant that analyzes the contents of a website \
and provides a short summary, ignoring text that might be navigation related. \
Respond in markdown."
192/65: messages = { "role" : "user", "content": "Please briefly summarize this website", "role":"system", "content":system_prompt}
192/66:
# Define our system prompt - you can experiment with this later, changing the last sentence to 'Respond in markdown in Spanish."

system_prompt = "You are an assistant that analyzes the contents of a website \
and provides a short summary, ignoring text that might be navigation related. \
Respond in markdown."
192/67: messages = { "role" : "user", "content": "Please briefly summarize this website", "role":"system", "content":system_prompt}
192/68:
def user_prompt_for(website):
    user_prompt = f"You are looking at a website titled {website.title}"
    user_prompt += "\nThe contents of this website is as follows; \
please provide a short summary of this website in markdown. \
If it includes news or announcements, then summarize these too.\n\n"
    user_prompt+=website.text
    return user_prompt
192/69: messages = { "role" : "user", "content": "Please briefly summarize this website"}
192/70:
def user_prompt_for(website):
    user_prompt = f"You are looking at a website titled {website.title}"
    user_prompt += "\nThe contents of this website is as follows; \
please provide a short summary of this website in markdown. \
If it includes news or announcements, then summarize these too.\n\n"
    user_prompt+=website.text
    return user_prompt
192/71:
WebObject = Website("https://tryhackme.com")
messages["content"] = user_prompt_for(WebObject)
192/72:
payload =  { "model": "llama3.2",
             "prompt": messages,
             "stream": False
           }
192/73:
OLLAMA_API =  "http://localhost:11434/api/chat"
HEADERS = {"Content-Type": "application/json"}
MODEL = "llama3.2"
192/74:
import ollama
ollama_response = requests.post(OLLAMA_API,json=payload, headers=HEADERS)
192/75:
import ollama
ollama_response = requests.post(OLLAMA_API,json=payload, headers=HEADERS)
ollama_response
192/76:
payload =  { "model": "llama3.2",
             "messages": messages,
             "stream": False
           }
192/77:
OLLAMA_API =  "http://localhost:11434/api/chat"
HEADERS = {"Content-Type": "application/json"}
MODEL = "llama3.2"
192/78:
import ollama
ollama_response = requests.post(OLLAMA_API,json=payload, headers=HEADERS)
ollama_response
192/79:
payload =  { "model": "llama3.2",
             "prompt": messages,
             "stream": False
           }
192/80:
OLLAMA_API =  "http://localhost:11434/api/chat"
HEADERS = {"Content-Type": "application/json"}
MODEL = "llama3.2"
192/81:
import ollama
ollama_response = requests.post(OLLAMA_API,json=payload, headers=HEADERS)
ollama_response
192/82:
import ollama
ollama_response = requests.post(OLLAMA_API,json=payload, headers=HEADERS)
print(ollama_response()['message']['content']
192/83:
import ollama
ollama_response = requests.post(OLLAMA_API,json=payload, headers=HEADERS)
print(ollama_response()['message']['content'])
192/84:
import ollama
ollama_response = requests.post(OLLAMA_API,json=payload, headers=HEADERS)
print(ollama_response.json()['message']['content'])
192/85:
import ollama
ollama_response = requests.post(OLLAMA_API,json=payload, headers=HEADERS)
print(ollama_response.json()['message']['content'])
192/86:
import ollama
ollama_response = requests.post(OLLAMA_API,json=payload, headers=HEADERS)
print(ollama_response.json()['message'])
192/87:
import ollama
ollama_response = requests.post(OLLAMA_API,json=payload, headers=HEADERS)
ollama_response.text
'''if ollama_response.status_code == 200:
    list_dict_words = []
    for each_word in
    '''
192/88:
import ollama
ollama_response = requests.post(OLLAMA_API,json=payload, headers=HEADERS)
ollama_response.text
'''if ollama_response.status_code == 200:
    list_dict_words = []
    for each_word in
'''
192/89:
import ollama
ollama_response = requests.post(OLLAMA_API,json=payload, headers=HEADERS)
ollama_response.text
192/90:
import ollama
ollama_response = requests.post(OLLAMA_API,json=payload, headers=HEADERS)
data = json.dumps(payload)
ollama_response.text
192/91:
# imports

#import os
import requests
#from dotenv import load_dotenv
from bs4 import BeautifulSoup
from IPython.display import Markdown, display
import json

# If you get an error running this cell, then please head over to the troubleshooting notebook!
192/92:
import ollama
ollama_response = requests.post(OLLAMA_API,json=payload, headers=HEADERS)
data = json.dumps(payload)
ollama_response.text
192/93:
WebObject = Website("https://tryhackme.com")
messages["content"] = user_prompt_for(WebObject)
192/94:
payload =  { "model": "llama3.2",
             "prompt": messages,
             "stream": False
           }
192/95:
OLLAMA_API =  "http://localhost:11434/api/chat"
HEADERS = {"Content-Type": "application/json"}
MODEL = "llama3.2"
192/96:
import ollama
ollama_response = requests.post(OLLAMA_API,json=payload, headers=HEADERS)
data = json.dumps(payload)
ollama_response.text
192/97:
import ollama
data = json.dumps(payload)
ollama_response = requests.post(OLLAMA_API,data=data, headers=HEADERS)
ollama_response.text
192/98:
import ollama
data = json.dumps(payload)
ollama_response = requests.post(OLLAMA_API,data=data, headers=HEADERS)
if ollama_response.status_code == 200:
    list_dict_words = []
    for each_word in response.text.split("\n"):
        try:
            data = json.loads(each_word) 
        except:
            pass
        list_dict_words.append(data)
        
llama_response = " ".join([word['response'] for word in list_dict_words if type(word) == type({})])
print(llama_response)
192/99:
import ollama
data = json.dumps(payload)
ollama_response = requests.post(OLLAMA_API,data=data, headers=HEADERS)
if ollama_response.status_code == 200:
    list_dict_words = []
    for each_word in ollama_response.text.split("\n"):
        try:
            data = json.loads(each_word) 
        except:
            pass
        list_dict_words.append(data)
        
llama_response = " ".join([word['response'] for word in list_dict_words if type(word) == type({})])
print(llama_response)
192/100:
import ollama
data = json.dumps(payload)
ollama_response = requests.post(OLLAMA_API,data=data, headers=HEADERS)
if ollama_response.status_code == 200:
    list_dict_words = []
    for each_word in ollama_response.text.split("\n"):
        try:
            data = json.loads(each_word) 
        except:
            pass
        list_dict_words.append(data)
        
llama_response = " ".join([word['message'] for word in list_dict_words if type(word) == type({})])
print(llama_response)
192/101:
import ollama
data = json.dumps(payload)
ollama_response = requests.post(OLLAMA_API,data=data, headers=HEADERS)
if ollama_response.status_code == 200:
    list_dict_words = []
    for each_word in ollama_response.text.split("\n"):
        try:
            data = json.loads(each_word) 
        except:
            pass
        list_dict_words.append(data)
        
llama_response = " ".join([word['message']['content'] for word in list_dict_words if type(word) == type({})])
print(llama_response)
195/1:
# imports

import requests
from bs4 import BeautifulSoup
from IPython.display import Markdown, display
195/2:
# Create a messages list using the same format that we used for OpenAI

messages = [
    {"role": "user", "content": "Describe some of the business applications of Generative AI"}
]
195/3:
payload =  { "model": "llama3.2",
             "messages": messages,
             "stream": False
           }
195/4:
OLLAMA_API =  "http://localhost:11434/api/chat"
HEADERS = {"Content-Type": "application/json"}
MODEL = "llama3.2"
195/5:
import ollama
response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
print(response.json()['message']['content'])
192/102:
import ollama
response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
print(response.json()['message']['content'])
192/103: messages =[ { "role" : "user", "content": "Please briefly summarize this website"}]
192/104:
def user_prompt_for(website):
    user_prompt = f"You are looking at a website titled {website.title}"
    user_prompt += "\nThe contents of this website is as follows; \
please provide a short summary of this website in markdown. \
If it includes news or announcements, then summarize these too.\n\n"
    user_prompt+=website.text
    return user_prompt
192/105:
WebObject = Website("https://tryhackme.com")
messages["content"] = user_prompt_for(WebObject)
192/106:
OLLAMA_API =  "http://localhost:11434/api/chat"
HEADERS = {"Content-Type": "application/json"}
MODEL = "llama3.2
192/107:
WebObject = Website("https://tryhackme.com")
messages[0]["content"] = user_prompt_for(WebObject)
192/108:
OLLAMA_API =  "http://localhost:11434/api/chat"
HEADERS = {"Content-Type": "application/json"}
MODEL = "llama3.2
192/109:
OLLAMA_API =  "http://localhost:11434/api/chat"
HEADERS = {"Content-Type": "application/json"}
MODEL = "llama3.2"
192/110:
payload =  { "model": "llama3.2",
             "prompt": messages,
             "stream": False
           }
192/111:
import ollama
response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
print(response.json()['message']['content'])
192/112:
WebObject = Website("https://tryhackme.com")
messages[0]["content"] = user_prompt_for(WebObject)
print(messages[0]
192/113:
WebObject = Website("https://tryhackme.com")
messages[0]["content"] = user_prompt_for(WebObject)
print(messages[0])
192/114:
OLLAMA_API =  "http://localhost:11434/api/chat"
HEADERS = {"Content-Type": "application/json"}
MODEL = "llama3.2"
192/115:
payload =  { "model": "llama3.2",
             "messages": messages,
             "stream": False
           }
192/116:
import ollama
response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
print(response.json()['message']['content'])
192/117:
import ollama
data = json.dumps(payload)
ollama_response.text
192/118:
import ollama
response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
print(response.json()['message']['content'])
192/119:
import ollama
response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
print(response.json()['message']['content']

def sum_of(website):
     user_prompt_for(website)
     messages =[ { "role" : "user", "content": "Please briefly summarize this website"}]
     messages[0]["content"] = user_prompt_for(website)
     OLLAMA_API =  "http://localhost:11434/api/chat"
     HEADERS = {"Content-Type": "application/json"}
     MODEL = "llama3.2"
     payload =  { "model": "llama3.2",
             "messages": messages,
             "stream": False
           }
    response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
    return response.json()['message']['content']
192/120:
import ollama
# response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
# print(response.json()['message']['content']

def sum_of(website):
     user_prompt_for(website)
     messages =[ { "role" : "user", "content": "Please briefly summarize this website"}]
     messages[0]["content"] = user_prompt_for(website)
     OLLAMA_API =  "http://localhost:11434/api/chat"
     HEADERS = {"Content-Type": "application/json"}
     MODEL = "llama3.2"
     payload =  { "model": "llama3.2",
             "messages": messages,
             "stream": False
           }
    response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
    return response.json()['message']['content']
192/122:
import ollama
# response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
# print(response.json()['message']['content']

def sum_of(website):
     user_prompt_for(website)
     messages =[ { "role" : "user", "content": "Please briefly summarize this website"}]
     messages[0]["content"] = user_prompt_for(website)
     OLLAMA_API =  "http://localhost:11434/api/chat"
     HEADERS = {"Content-Type": "application/json"}
     MODEL = "llama3.2"
     payload =  { "model": "llama3.2",
             "messages": messages,
             "stream": False
           }
     response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
     return response.json()['message']['content']
192/123: sum_of("https://tryhackme.com")
192/124: sum_of(Website("https://tryhackme.com"))
192/125:
import ollama
# response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
# print(response.json()['message']['content']

def sum_of(url):
     str(url) = Website(url)
     user_prompt_for(url)
     messages =[ { "role" : "user", "content": "Please briefly summarize this website"}]
     messages[0]["content"] = user_prompt_for(url)
     OLLAMA_API =  "http://localhost:11434/api/chat"
     HEADERS = {"Content-Type": "application/json"}
     MODEL = "llama3.2"
     payload =  { "model": "llama3.2",
             "messages": messages,
             "stream": False
           }
     response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
     return response.json()['message']['content']
192/126:
import ollama
# response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
# print(response.json()['message']['content']

def sum_of(url):
     
     url = Website(str(url))
     user_prompt_for(url)
     messages =[ { "role" : "user", "content": "Please briefly summarize this website"}]
     messages[0]["content"] = user_prompt_for(url)
     OLLAMA_API =  "http://localhost:11434/api/chat"
     HEADERS = {"Content-Type": "application/json"}
     MODEL = "llama3.2"
     payload =  { "model": "llama3.2",
             "messages": messages,
             "stream": False
           }
     response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
     return response.json()['message']['content']
192/127: sum_of(Website("https://linkedin.com"))
192/128: sum_of(Website("https://tryhackme.com"))
192/129:
import ollama
# response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
# print(response.json()['message']['content']

def sum_of(url):
     
     urlobj = Website(str(url))
     user_prompt_for(urlobj)
     messages =[ { "role" : "user", "content": "Please briefly summarize this website"}]
     messages[0]["content"] = user_prompt_for(urlobj)
     OLLAMA_API =  "http://localhost:11434/api/chat"
     HEADERS = {"Content-Type": "application/json"}
     MODEL = "llama3.2"
     payload =  { "model": "llama3.2",
             "messages": messages,
             "stream": False
           }
     response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
     return response.json()['message']['content']
192/130: sum_of(Website("https://tryhackme.com"))
192/131: sum_of("https://tryhackme.com")
192/132:
def display_summary(url):
    
    summary = sum_of(url)
    display(Markdown(summary)
192/133:
def display_summary(url):
    
    summary = sum_of(url)
    display(Markdown(summary))
192/134: display_summary(url)
192/135: display_summary("https://tryhackme.com")
192/136:
import ollama
# response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
# print(response.json()['message']['content']

def sum_of(url):
     
     urlobj = Website(str(url))
     user_prompt_for(urlobj)
     messages =[ { "role" : "user", "content": "Please briefly summarize this website"}]
     messages[0]["content"] = user_prompt_for(urlobj)
     OLLAMA_API =  "http://localhost:11434/api/chat"
     HEADERS = {"Content-Type": "application/json"}
     MODEL = "llama3.2"
     payload =  { "model": "llama3.2",
             "messages": messages,
             "stream": True
           }
     response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
     return response.json()['message']['content']
192/137: sum_of("https://tryhackme.com")
192/138: display_summary("https://tryhackme.com")
192/139:
import ollama
# response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
# print(response.json()['message']['content']

def sum_of(url):
     
     urlobj = Website(str(url))
     user_prompt_for(urlobj)
     messages =[ { "role" : "user", "content": "Please briefly summarize this website"}]
     messages[0]["content"] = user_prompt_for(urlobj)
     OLLAMA_API =  "http://localhost:11434/api/chat"
     HEADERS = {"Content-Type": "application/json"}
     MODEL = "llama3.2"
     payload =  { "model": "llama3.2",
             "messages": messages,
             "stream": True
           }
     response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
     summary = ""
    # Streaming ile gelen parÃ§alarÄ± birleÅtiriyoruz
    for chunk in response.iter_content(chunk_size=None):
        if chunk:
            summary += chunk.decode("utf-8")
192/141:
import ollama
# response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
# print(response.json()['message']['content']

def sum_of(url):
     
     urlobj = Website(str(url))
     user_prompt_for(urlobj)
     messages =[ { "role" : "user", "content": "Please briefly summarize this website"}]
     messages[0]["content"] = user_prompt_for(urlobj)
     OLLAMA_API =  "http://localhost:11434/api/chat"
     HEADERS = {"Content-Type": "application/json"}
     MODEL = "llama3.2"
     payload =  { "model": "llama3.2",
             "messages": messages,
             "stream": True
           }
     response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
     summary = ""
    # Streaming ile gelen parÃ§alarÄ± birleÅtiriyoruz
     for chunk in response.iter_content(chunk_size=None):
        if chunk:
            summary += chunk.decode("utf-8")
192/142:
def display_summary(url):
    
    summary = sum_of(url)
    display(Markdown(summary))
192/143: display_summary("https://tryhackme.com")
192/144:
import ollama
# response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
# print(response.json()['message']['content']

def sum_of(url):
     
     urlobj = Website(str(url))
     user_prompt_for(urlobj)
     messages =[ { "role" : "user", "content": "Please briefly summarize this website"}]
     messages[0]["content"] = user_prompt_for(urlobj)
     OLLAMA_API =  "http://localhost:11434/api/chat"
     HEADERS = {"Content-Type": "application/json"}
     MODEL = "llama3.2"
     payload =  { "model": "llama3.2",
             "messages": messages,
             "stream": True
           }
     response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
     summary = ""
    # Streaming ile gelen parÃ§alarÄ± birleÅtiriyoruz
     for chunk in response.iter_content(chunk_size=None):
        if chunk:
            summary += chunk.decode("utf-8")
     return summary
192/145:
def display_summary(url):
    
    summary = sum_of(url)
    display(Markdown(summary))
192/146: display_summary("https://tryhackme.com")
192/147:
import ollama
# response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
# print(response.json()['message']['content']

def sum_of(url):
     
     urlobj = Website(str(url))
     user_prompt_for(urlobj)
     messages =[ { "role" : "user", "content": "Please briefly summarize this website"}]
     messages[0]["content"] = user_prompt_for(urlobj)
     OLLAMA_API =  "http://localhost:11434/api/chat"
     HEADERS = {"Content-Type": "application/json"}
     MODEL = "llama3.2"
     payload =  { "model": "llama3.2",
             "messages": messages,
             "stream": True
           }
     response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
     return response.json()['message']['content']
192/148:
def display_summary(url):
    
    summary = sum_of(url)
    display(Markdown(summary))
192/149: display_summary("https://tryhackme.com")
192/150: sum_of("https://tryhackme.com")
192/151:
import ollama
# response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
# print(response.json()['message']['content']

def sum_of(url):
     
     urlobj = Website(str(url))
     user_prompt_for(urlobj)
     messages =[ { "role" : "user", "content": "Please briefly summarize this website"}]
     messages[0]["content"] = user_prompt_for(urlobj)
     OLLAMA_API =  "http://localhost:11434/api/chat"
     HEADERS = {"Content-Type": "application/json"}
     MODEL = "llama3.2"
     payload =  { "model": "llama3.2",
             "messages": messages,
             "stream": False
           }
     response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
     return response.json()['message']['content']
192/152: sum_of("https://tryhackme.com")
191/15:
# Let's use Markdown
# Are you wondering why it makes any difference to set system_message when it's not referred to in the code below it?
# I'm taking advantage of system_message being a global variable, used back in the message_gpt function (go take a look)
# Not a great software engineering practice, but quite sommon during Jupyter Lab R&D!

system_message = "You are a helpful assistant that responds in markdown"

view = gr.Interface(
    fn=message_gpt,
    inputs=[gr.Textbox(label="Your message:")],
    outputs=[gr.Markdown(label="Response:")],
    flagging_mode="never"
)
view.launch()
191/16:
def stream_model(prompt, model):
    if model=="GPT":
        result = stream_gpt(prompt)
    elif model=="Claude":
        result = stream_claude(prompt)
    else:
        raise ValueError("Unknown model")
    yield from result
191/17:
view = gr.Interface(
    fn=stream_model,
    inputs=[gr.Textbox(label="Your message:"), gr.Dropdown(["GPT", "Claude"], label="Select model", value="GPT")],
    outputs=[gr.Markdown(label="Response:")],
    flagging_mode="never"
)
view.launch()
191/18:
# A class to represent a Webpage

class Website:
    url: str
    title: str
    text: str

    def __init__(self, url):
        self.url = url
        response = requests.get(url)
        self.body = response.content
        soup = BeautifulSoup(self.body, 'html.parser')
        self.title = soup.title.string if soup.title else "No title found"
        for irrelevant in soup.body(["script", "style", "img", "input"]):
            irrelevant.decompose()
        self.text = soup.body.get_text(separator="\n", strip=True)

    def get_contents(self):
        return f"Webpage Title:\n{self.title}\nWebpage Contents:\n{self.text}\n\n"
191/19:
# With massive thanks to Bill G. who noticed that a prior version of this had a bug! Now fixed.

system_message = "You are an assistant that analyzes the contents of a company website landing page \
and creates a short brochure about the company for prospective customers, investors and recruits. Respond in markdown."
191/20:
def stream_brochure(company_name, url, model):
    prompt = f"Please generate a company brochure for {company_name}. Here is their landing page:\n"
    prompt += Website(url).get_contents()
    if model=="GPT":
        result = stream_gpt(prompt)
    elif model=="Claude":
        result = stream_claude(prompt)
    else:
        raise ValueError("Unknown model")
    yield from result
191/21:
view = gr.Interface(
    fn=stream_brochure,
    inputs=[
        gr.Textbox(label="Company name:"),
        gr.Textbox(label="Landing page URL including http:// or https://"),
        gr.Dropdown(["GPT", "Claude"], label="Select model")],
    outputs=[gr.Markdown(label="Brochure:")],
    flagging_mode="never"
)
view.launch()
201/1:
import os
import requests
from bs4 import BeautifulSoup
from typing import List
from dotenv import load_dotenv
from openai import OpenAI
import google.generativeai
import anthropic
201/2: import gradio as gr # oh yeah!
201/3:
# A generic system message - no more snarky adversarial AIs!

system_message = "You are a helpful assistant"
201/4:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": prompt}
      ]
    response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
    return response.json()['message']['content']
201/5: message_ollama("Hello")
201/6:
class Website:
  def __init__(self,url):
      url:str
      self.url = url
      response = requests.get(url)
      soup = BeautifulSoup(response.content, 'html.parser')
      self.title = soup.title.string if soup.title else "No Title Found"
      for irrelevant in soup.body(["script", "style","img","input"]):
          irrelevant.decompose()

      self.text = soup.body.get_text(separator = "\n", strip = True)
201/7:
    OLLAMA_API =  "http://localhost:11434/api/chat"
    HEADERS = {"Content-Type": "application/json"}
    MODEL = "llama3.2"
201/8:
payload =  { "model": "llama3.2",
             "messages": messages,
             "stream": False
           }
201/9:
payload =  { "model": "llama3.2",
             "messages": messages,
             "stream": False
           }
201/10:
# A generic system message - no more snarky adversarial AIs!

system_message = "You are a helpful assistant"
messages = ""
201/11:
class Website:
  def __init__(self,url):
      url:str
      self.url = url
      response = requests.get(url)
      soup = BeautifulSoup(response.content, 'html.parser')
      self.title = soup.title.string if soup.title else "No Title Found"
      for irrelevant in soup.body(["script", "style","img","input"]):
          irrelevant.decompose()

      self.text = soup.body.get_text(separator = "\n", strip = True)
201/12:
    OLLAMA_API =  "http://localhost:11434/api/chat"
    HEADERS = {"Content-Type": "application/json"}
    MODEL = "llama3.2"
201/13:
payload =  { "model": "llama3.2",
             "messages": messages,
             "stream": False
           }
201/14:
# A generic system message - no more snarky adversarial AIs!

system_message = "You are a helpful assistant"
message = ""
201/15:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messsage = prompt
    messages = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": prompt}
      ]
    response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
    return response.json()['message']['content']
201/16:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    message = prompt
    messages = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": prompt}
      ]
    response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
    return response.json()['message']['content']
201/17:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
   payload["messages"] = prompt
   response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
   return response.json()['message']['content']
201/18: message_ollama("Hello")
201/19:
# A generic system message - no more snarky adversarial AIs!

system_message = "You are a helpful assistant"
messages = ""
201/20:
payload =  { "model": "llama3.2",
             "messages": messages,
             "stream": False
           }
201/21:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
   payload["messages"] = prompt
   response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)
   return response.json()['message']['content']
201/22: message_ollama("Hello")
201/23:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
   payload["messages"] = prompt
   stream = ollama.chat(model=MODEL,messages=messages,stream = True)

for chunk in stream:
    print(chunk['message']['content'], end='')
201/24:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
   payload["messages"] = prompt
   stream = ollama.chat(model=MODEL,messages=messages,stream = True)

for chunk in stream:
    print(chunk['message']['content'], end='')
201/25: message_ollama("Hello")
201/26:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages["content"] = prompt
   stream = ollama.chat(model=MODEL,messages=messages,stream = True)

for chunk in stream:
    print(chunk['message']['content'], end='')
201/28:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True)

for chunk in stream:
    print(chunk['message']['content'], end='')
201/29:
# A generic system message - no more snarky adversarial AIs!

system_message = "You are a helpful assistant"
messages =[ { "role" : "user", "content": ""}]
201/30:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True)

for chunk in stream:
    print(chunk['message']['content'], end='')
201/31:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True,)

for chunk in stream:
    print(chunk['message']['content'], end='')
201/32:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True,)

    for chunk in stream:
        print(chunk['message']['content'], end='')
201/33: message_ollama("Hello")
201/34:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages[0]["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True,)

    for chunk in stream:
        print(chunk['message']['content'], end='')
201/35: message_ollama("Hello")
201/36:
# here's a simple function

def shout(text):
    print(f"Shout has been called with input {text}")
    return text.upper()
201/37: shout("hello")
201/38:
# The simplicty of gradio. This might appear in "light mode" - I'll show you how to make this in dark mode later.

gr.Interface(fn=shout, inputs="textbox", outputs="textbox").launch(share=False)
201/39:
# Adding inbrowser=True opens up a new browser window automatically

gr.Interface(fn=shout, inputs="textbox", outputs="textbox", flagging_mode="never").launch(inbrowser=True)
201/40:
# Adding inbrowser=True opens up a new browser window automatically

gr.Interface(fn=message_ollama, inputs="textbox", outputs="textbox", flagging_mode="never").launch(inbrowser=True)
201/41:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages[0]["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True,)

    for chunk in stream:
        yield (chunk['message']['content'], end='')
201/42:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages[0]["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True,)

    for chunk in stream:
        yield (chunk['message']['content']
201/43:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages[0]["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True,)

    for chunk in stream:
        yield chunk['message']['content']
201/44: message_ollama("Hello")
201/45:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages[0]["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True,)

    for chunk in stream:
        yield chunk
201/46: message_ollama("Hello")
201/47:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages[0]["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True,)

    answer = ""
    for chunk in stream:
        answer+= chunk['message']['content']
        yield answer
201/48: message_ollama("Hello")
201/49: message_ollama("Hello")
201/50:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages[0]["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True,)

    answer = ""
    for chunk in stream:
        answer+= chunk['message']['content']
        yield answer
201/51: message_ollama("Hello")
201/52:
# here's a simple function

def shout(text):
    print(f"Shout has been called with input {text}")
    return text.upper()
201/53: shout("hello")
201/54:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages[0]["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True,)
    answer = ""
    for chunk in stream:
        # Gelen her bir chunk'Ä±n doÄru formatta olup olmadÄ±ÄÄ±nÄ± kontrol ediyoruz
        if 'content' in chunk.get('message', {}):
            answer += chunk['message']['content']
            yield answer
        else:
            # Chunk formatÄ± beklendiÄi gibi deÄilse hata ayÄ±klama mesajÄ±
            print("Unexpected chunk format:", chunk)
201/55: message_ollama("Hello")
201/56:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages[0]["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True,)

  '''  for chunk in stream:
        print(chunk['message']['content'], end='')
  '''
    return stream
201/58:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages[0]["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True,)

      #  for chunk in stream:
      #  print(chunk['message']['content'], end='')
  
    return stream
201/59: message_ollama("Hello")
201/60:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages[0]["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True,)

      #  for chunk in stream:
      #  print(chunk['message']['content'], end='')
  
    return stream
201/61: message_ollama("Hello")
201/62:
# here's a simple function

def shout(text):
    print(f"Shout has been called with input {text}")
    return text.upper()
201/63: shout("hello")
201/64:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages[0]["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True,)

    for chunk in stream:
        print(chunk['message']['content'], end='')
  
    return stream
201/65: message_ollama("Hello")
201/66:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages[0]["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True,)

    answer = ""
    for chunk in stream:
        print(chunk['message']['content'])
201/67: message_ollama("Hello")
201/68:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages[0]["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True,)

    answer = ""
    for chunk in stream:
        answer += chunk['message']['content']
        yield answer
201/69: message_ollama("Hello")
201/70: message_ollama("Hello")
201/71:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages[0]["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True,)

    answer = ""
    for chunk in stream:
        answer += chunk['message']['content']
        yield answer
201/72: message_ollama("Hello")
201/73:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages[0]["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True,)

    answer = ""
    for chunk in stream:
        answer += chunk['message']['content'] or ""
        yield answer
201/74: message_ollama("Hello")
201/75:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages[0]["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True,)

    answer = ""
    for chunk in stream:
        content = chunk.get('message', {}).get('content', "")
        if content:
            answer += content
            yield answer
201/76: message_ollama("Hello")
201/77:
for partial_answer in message_ollama("Hello"):
    print(partial_answer)
201/78:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages[0]["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True,)

    answer = ""
    for chunk in stream:
        content = chunk.get('message', {}).get('content', "")
        if content:
            answer += content
        yield answer
201/79:
for partial_answer in message_ollama("Hello"):
    print(partial_answer)
201/80:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages[0]["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True,)

    answer = ""
    for chunk in stream:
        content = chunk.get('message', {}).get('content', "")
        if content:
            answer += content or ""
        yield answer
201/81:
for partial_answer in message_ollama("Hello"):
    print(partial_answer)
201/82:
for partial_answer in message_ollama("Hello"):
    print(partial_answer, end = "")
201/83:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages[0]["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True,)

    answer = ""
    for chunk in stream:
        content = chunk.get('message', {}).get('content', "")
        if content:
            answer = content
        yield answer
201/84:
for partial_answer in message_ollama("Hello"):
    print(partial_answer, end = "")
201/85:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages[0]["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True,)

    answer = ""
    for chunk in stream:
       answer += chunk['message']['content']
       yield from answer
201/86:
for partial_answer in message_ollama("Hello"):
    print(partial_answer, end = "")
201/87:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages[0]["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True,)

    answer = ""
    for chunk in stream:
       answer += chunk['message']['content'] or ''
       yield from answer
201/88:
for partial_answer in message_ollama("Hello"):
    print(partial_answer, end = "")
201/89: message_ollama("Hello"):
201/90: message_ollama("Hello")
201/91: print(message_ollama("Hello"),end="")
201/92:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages[0]["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True,)

    answer = ""
    for chunk in stream.:
       answer += chunk['message']['content'] or ''
       yield from answer
201/93: print(message_ollama("Hello"),end="")
201/94:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages[0]["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True,)

    answer = ""
    for chunk in stream:
       answer += chunk['message']['content'] or ''
       yield from answer
201/95: print(message_ollama("Hello"),end="")
201/96:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages[0]["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True,)

    answer = ""
    for chunk in stream:
       answer += chunk['message']['content'] or ''
    return answer
201/97: print(message_ollama("Hello"),end="")
201/98:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages[0]["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True,)

    answer = ""
    for chunk in stream:
       answer += chunk['message']['content'] or ''
       yield answer
201/99: print(message_ollama("Hello"),end="")
201/100:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages[0]["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True,)

    answer = ""
    for chunk in stream:
       answer += chunk['message']['content'] or ''
       yield from answer
201/101: print(message_ollama("Hello"),end="")
201/102: print(message_ollama("Hello")
201/103: print(message_ollama("Hello"))
201/104:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages[0]["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True,)

    answer = ""
    for chunk in stream:
       answer += chunk['message']['content'] or ''
       yield  answer
201/105: print(message_ollama("Hello"))
201/106:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages[0]["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True,)

    answer = ""
    for chunk in stream:
       answer += chunk['message']['content'] or ''
    yield  answer
201/107: print(message_ollama("Hello"))
201/108: print(message_ollama("Hello", end=""))
201/109: print(message_ollama("Hello"),end="")
201/110:
for partial_answer in message_ollama("Hello"):
    print(partial_answer, end = "")
201/111:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages[0]["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True,)

    answer = ""
    for chunk in stream:
       print(chunk['message']['content'] , end='', flush = True)
201/112: message_ollama("How r u")
201/113:
# Adding inbrowser=True opens up a new browser window automatically

gr.Interface(fn=message_ollama, inputs="textbox", outputs="textbox", flagging_mode="never",live=True).launch(inbrowser=True)
201/114:
# Adding inbrowser=True opens up a new browser window automatically

gr.Interface(fn=message_ollama, inputs="text", outputs="text", flagging_mode="never",live=True).launch(inbrowser=True)
201/115:
# Adding inbrowser=True opens up a new browser window automatically

gr.Interface(fn=message_ollama, inputs="textbox", outputs="textbox", flagging_mode="never",live=True).launch(inbrowser=True)
201/116:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages[0]["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True,)

    answer = ""
    for chunk in stream:
       answer += chunk['message']['content']
    return answer
201/117: message_ollama("How r u")
201/118:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages[0]["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True,)

    answer = ""
    for chunk in stream:
       answer += chunk['message']['content']
       yield answer
201/119: message_ollama("How r u")
201/120: print(message_ollama("How r u"), end="")
201/121:
# Let's wrap a call to GPT-4o-mini in a simple function

import ollama
def message_ollama(prompt):
    messages[0]["content"] = prompt
    stream = ollama.chat(model=MODEL,messages=messages,stream = True,)

    answer = ""
    for chunk in stream:
       answer += chunk['message']['content']
    return answer
201/122: message_ollama("Hello")
201/123:
view = gr.Interface(
    fn=message_ollama,
    inputs=[gr.Textbox(label="Your message:")],
    outputs=[gr.Markdown(label="Response:")],
    flagging_mode="never"
)
view.launch()
201/124:
# A class to represent a Webpage

class Website:
    url: str
    title: str
    text: str

    def __init__(self, url):
        self.url = url
        response = requests.get(url)
        self.body = response.content
        soup = BeautifulSoup(self.body, 'html.parser')
        self.title = soup.title.string if soup.title else "No title found"
        for irrelevant in soup.body(["script", "style", "img", "input"]):
            irrelevant.decompose()
        self.text = soup.body.get_text(separator="\n", strip=True)

    def get_contents(self):
        return f"Webpage Title:\n{self.title}\nWebpage Contents:\n{self.text}\n\n"
201/125:
def stream_brochure(company_name, url, model):
    prompt = f"Please generate a company brochure for {company_name}. Here is their landing page:\n"
    prompt += Website(url).get_contents()
    stream = ollama.chat(model = MODEL, messages=prompt,stream = True)
     answer = ""
    for chunk in stream:
       answer += chunk['message']['content']
    return answer
201/126:
def stream_brochure(company_name, url, model):
    prompt = f"Please generate a company brochure for {company_name}. Here is their landing page:\n"
    prompt += Website(url).get_contents()
    stream = ollama.chat(model = MODEL, messages=prompt,stream = True)
    answer = ""
    for chunk in stream:
       answer += chunk['message']['content']
    return answer
201/127:
view = gr.Interface(
    fn=stream_brochure,
    inputs=[
        gr.Textbox(label="Company name:"),
        gr.Textbox(label="Landing page URL including http:// or https://"),
        gr.Dropdown(["llama", "llama2"], label="Select model")],
    outputs=[gr.Markdown(label="Brochure:")],
    flagging_mode="never"
)
view.launch()
201/128:
def stream_brochure(company_name, url, model):
    prompt = f"Please generate a company brochure for {company_name}. Here is their landing page:\n"
    prompt += Website(url).get_contents()
    stream = ollama.chat(model = MODEL, messages=prompt,stream = True)
    answer = ""
    for chunk in stream:
       answer += chunk['message']['content']
    return answer
201/129: stream_brochure("Yasarito","https://www.netdania.com","llama")
201/130:
def stream_brochure(company_name, url, model):
    prompt = f"Please generate a company brochure for {company_name}. Here is their landing page:\n"
    prompt += Website(url).get_contents()
    messages[0]["content"] = prompt
    stream = ollama.chat(model = MODEL, messages=messages,stream = True)
    answer = ""
    for chunk in stream:
       answer += chunk['message']['content']
    return answer
201/131: stream_brochure("Yasarito","https://www.netdania.com","llama")
201/132:
view = gr.Interface(
    fn=stream_brochure,
    inputs=[
        gr.Textbox(label="Company name:"),
        gr.Textbox(label="Landing page URL including http:// or https://"),
        gr.Dropdown(["GPT", "Claude"], label="Select model")],
    outputs=[gr.Markdown(label="Brochure:")],
    flagging_mode="never"
)
view.launch()
201/133: display(Markdown(stream_brochure("Yasarito","https://tryhackme.com","llama"))
201/134: display(Markdown(stream_brochure("Yasarito","https://tryhackme.com","llama")))
201/135:
from IPython.display import Markdown, display
display(Markdown(stream_brochure("Yasarito","https://tryhackme.com","llama")))
205/1:
import os
import glob
from dotenv import load_dotenv
205/2:
# imports for langchain

from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
216/1: MODEL = Ollama(model="llama3.2")
216/2:
# imports for langchain, plotly and Chroma

from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
import numpy as np
import plotly.graph_objects as go
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import ChatPromptTemplate
from langchain_community.llms.ollama import Ollama
from langchain_community.vectorstores import Chroma
216/3: MODEL = Ollama(model="llama3.2")
216/4:
# imports for langchain, plotly and Chroma

from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
#from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
import numpy as np
import plotly.graph_objects as go
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import ChatPromptTemplate
from langchain_community.llms.ollama import Ollama
from langchain_community.vectorstores import Chroma
216/5:
# imports for langchain, plotly and Chroma

from langchain.prompts import ChatPromptTemplate
from langchain_community.llms.ollama import Ollama
from langchain_community.vectorstores import Chroma
216/6: MODEL = Ollama(model="llama3.2")
216/7: MODEL
210/1:
# imports

import os
import glob
from dotenv import load_dotenv
import gradio as gr
210/2:
# price is a factor for our company, so we're going to use a low cost model

MODEL = "gpt-4o-mini"
db_name = "vector_db"
210/3:
# imports for langchain

from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
# from langchain_chroma import Chroma
from langchain.vectorstores import FAISS
import numpy as np
from sklearn.manifold import TSNE
import plotly.graph_objects as go
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
210/4:
# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
210/5:
# imports for langchain

from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
# from langchain_chroma import Chroma
from langchain.vectorstores import FAISS
import numpy as np
from sklearn.manifold import TSNE
import plotly.graph_objects as go
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
216/8:
# imports for langchain

from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
# from langchain_chroma import Chroma
from langchain.vectorstores import FAISS
import numpy as np
from sklearn.manifold import TSNE
import plotly.graph_objects as go
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
216/9: !pip install langchain jsonlines qdrant-client gradio
216/10: text_loader_kwargs = {'encoding': 'utf-8'}
216/11: langchain_documents = []
216/12:
import jsonlines
with jsonlines.open("articles.json","r") as documents:
    print(next(iter(documents)
216/13:
import jsonlines
with jsonlines.open("articles.json","r") as documents:
    print(next(iter(documents)))
216/14:
import jsonlines
with jsonlines.open("articles.json","r") as documents:
    print(next(iter(documents)))
216/15:
import jsonlines
with jsonlines.open("articles.json","r") as documents:
    print(next(iter(documents)))
216/16:
import jsonlines
with jsonlines.open("articles.json","r") as documents:
    print(next(iter(documents)))
216/17:
import jsonlines
with jsonlines.open("articles.json","r") as documents:
    print(next(iter(documents)))
216/18: from langchain_core.documents import Document
216/19:
from langchain_core.documents import Document
langchain_documents = []
216/20:
from langchain_core.documents import Document
langchain_documents = []
216/21:
folders = glob.glob("articles/*")
folders
216/22:
import os
import glob
from dotenv import load_dotenv
216/23:
# imports for langchain

from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
# from langchain_chroma import Chroma
from langchain.vectorstores import FAISS
import numpy as np
from sklearn.manifold import TSNE
import plotly.graph_objects as go
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
216/24:
# imports for langchain, plotly and Chroma

from langchain.prompts import ChatPromptTemplate
from langchain_community.llms.ollama import Ollama
from langchain_community.vectorstores import Chroma
216/25:
MODEL = Ollama(model="llama3.2")
db_name = "vector-database
216/26:
MODEL = Ollama(model="llama3.2")
db_name = "vector-database"
216/27: folders = glob.glob("articles/*")
216/28: text_loader_kwargs = {'encoding': 'utf-8'}
216/29:
import jsonlines
with jsonlines.open("articles.json","r") as documents:
    print(next(iter(documents)))
216/30:
from langchain_core.documents import Document
langchain_documents = []
216/31:
folders = glob.glob("articles/*")
folders
216/32:
langchain_documents = []

with jsonlines.open("articles.json","r") as metadatas:
    for file,metadata in folders,metadatas:
        loader = TextLoader(file)
        file_doc = loader.load()
        file_doc.metadata.update({"url": metadata.get("url"),
                "tags": metadata.get("tags"),
                "title": metadata.get("title"),
                "authors": metadata.get("authors")
            })
        langchain_documents.append(file_doc)
216/33:
langchain_documents = []

with jsonlines.open("articles.json","r") as metadatas:
    for file,metadata in zip(folders,metadatas):
        loader = TextLoader(file)
        file_doc = loader.load()
        file_doc.metadata.update({"url": metadata.get("url"),
                "tags": metadata.get("tags"),
                "title": metadata.get("title"),
                "authors": metadata.get("authors")
            })
        langchain_documents.append(file_doc)
216/34:
from langchain_core.documents import Document
folders = glob.glob("articles/*")
216/35:
langchain_documents = []

with jsonlines.open("articles.json", "r") as metadata_file:
    for file_path, metadata in zip(folders, metadata_file):
        try:
            loader = TextLoader(file_path)
            file_doc = loader.load()

            # Metadata'yÄ± dÃ¶kÃ¼mana ekle
            file_doc.metadata.update({
                "url": metadata.get("url"),
                "tags": metadata.get("tags"),
                "title": metadata.get("title"),
                "authors": metadata.get("authors")
            })

            # DÃ¶kÃ¼manÄ± listeye ekle
            langchain_documents.append(file_doc)
        except Exception as e:
            print(f"Error loading file {file_path}: {e}")
216/36:
from langchain.document_loaders import PyPDFLoader
from langchain_core.documents import Document
folders = glob.glob("articles/*")
216/37:
langchain_documents = []

with jsonlines.open("articles.json", "r") as metadata_file:
    for file_path, metadata in zip(folders, metadata_file):
        try:
            pdf_loader = PyPDFLoader(file_path)
            pdf_documents = pdf_loader.load()

            # Metadata'yÄ± dÃ¶kÃ¼mana ekle
            pdf_doc.metadata.update({
                "url": metadata.get("url"),
                "tags": metadata.get("tags"),
                "title": metadata.get("title"),
                "authors": metadata.get("authors")
            })

            # DÃ¶kÃ¼manÄ± listeye ekle
            langchain_documents.append( Document(
                        page_content=pdf_doc.page_content,
                        metadata=pdf_doc.metadata
                    )
                ))
        except Exception as e:
            print(f"Error loading file {file_path}: {e}")
216/38:
langchain_documents = []

with jsonlines.open("articles.json", "r") as metadata_file:
    for file_path, metadata in zip(folders, metadata_file):
        try:
            pdf_loader = PyPDFLoader(file_path)
            pdf_documents = pdf_loader.load()

            # Metadata'yÄ± dÃ¶kÃ¼mana ekle
            pdf_doc.metadata.update({
                "url": metadata.get("url"),
                "tags": metadata.get("tags"),
                "title": metadata.get("title"),
                "authors": metadata.get("authors")
            })

            # DÃ¶kÃ¼manÄ± listeye ekle
            langchain_documents.append( Document(
                        page_content=pdf_doc.page_content,
                        metadata=pdf_doc.metadata
                    )
                )))
        except Exception as e:
            print(f"Error loading file {file_path}: {e}")
216/39:
langchain_documents = []

with jsonlines.open("articles.json", "r") as metadata_file:
    for file_path, metadata in zip(folders, metadata_file):
        try:
            pdf_loader = PyPDFLoader(file_path)
            pdf_documents = pdf_loader.load()

            # Metadata'yÄ± dÃ¶kÃ¼mana ekle
            pdf_doc.metadata.update({
                "url": metadata.get("url"),
                "tags": metadata.get("tags"),
                "title": metadata.get("title"),
                "authors": metadata.get("authors")
            })

            # DÃ¶kÃ¼manÄ± listeye ekle
            langchain_documents.append( Document(
                        page_content=pdf_doc.page_content,
                        metadata=pdf_doc.metadata
                    )
                )
        except Exception as e:
            print(f"Error loading file {file_path}: {e}")
216/40:
langchain_documents = []

with jsonlines.open("articles.json", "r") as metadata_file:
    for file_path, metadata in zip(folders, metadata_file):
        try:
            pdf_loader = PyPDFLoader(file_path)
            pdf_documents = pdf_loader.load()

            for pdf_doc in pdf_documents:
            # Metadata'yÄ± dÃ¶kÃ¼mana ekle
                pdf_doc.metadata.update({
                    "url": metadata.get("url"),
                    "tags": metadata.get("tags"),
                    "title": metadata.get("title"),
                    "authors": metadata.get("authors")
                })

            # DÃ¶kÃ¼manÄ± listeye ekle
                langchain_documents.append( Document(
                        page_content=pdf_doc.page_content,
                        metadata=pdf_doc.metadata
                    )
                )
        except Exception as e:
            print(f"Error loading file {file_path}: {e}")
216/41: len(langchain_documents), langchain_documents[:1]
216/42:
from langchain.vectorstores import Qdrant
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
216/43: embeddings = FastEmbedEmbeddings(model_name="BAAI/bge-base-en-v1.5")
216/44: !pip install fastembed
216/45: embeddings = FastEmbedEmbeddings(model_name="BAAI/bge-base-en-v1.5")
216/46:
store = Qdrant.from_documents(
    langchain_documents,
    embeddings,
    path="/tmp/ai_qdrant",
    collection_name="AI-Embeddings",
)
216/47:
store.similarity_search_with_score(
    query="AI and authors",
    k=1
)
216/48:
store.similarity_search_with_score(
    query="AI and authors",
    filter={"authors": "Rex Ying"},
    k=1
)
216/49:
from langchain_community.llms import Ollama
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

llm = Ollama(
    model="llama3.2",
    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),
)
216/50:
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain.prompts import PromptTemplate
216/51:
def retrieval_chain_with_filter(llm, filter={}):
    template = """You are a bot that answers user questions using only the context provided.
    If you don't know the answer, simply state that you don't know.
    {context}
    Question: {input}"""

    prompt = PromptTemplate(template=template, input_variables=["context", "input"])
    retriever = store.as_retriever(search_kwargs={'filter': filter})
    llm_with_prompt = create_stuff_documents_chain(llm, prompt)
    return create_retrieval_chain(retriever, llm_with_prompt)
216/52:
result = retrieval_chain_with_filter(llm).invoke({
    "input": "What was said about AI safety and copyright?""
})
216/53:
result = retrieval_chain_with_filter(llm).invoke({
    "input": "What was said about AI safety and copyright?"
})
216/54: !pip uninstall fastembed
216/55: y
216/56: !pip install fastembed
216/57: !pip install fastembed
216/58:
from langchain_ollama import OllamaEmbeddings

embeddings = OllamaEmbeddings(
    model="llama3.2",
)
216/59: vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
216/60:
from langchain_ollama import OllamaEmbeddings

embeddings = OllamaEmbeddings(
    model="llama3.2",
)
216/61:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(langchain_documents)
216/62:

vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
216/63: len(langchain_documents), langchain_documents[:1]
220/1:
import os
import glob
from dotenv import load_dotenv
220/2:
# imports for langchain

from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
# from langchain_chroma import Chroma
from langchain.vectorstores import FAISS
import numpy as np
from sklearn.manifold import TSNE
import plotly.graph_objects as go
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
220/3:
from langchain.document_loaders import PyPDFLoader
from langchain_core.documents import Document
folders = glob.glob("articles/*")
220/4:
from langchain.document_loaders import PyPDFLoader
from langchain_core.documents import Document
file = glob.glob("articles/2406.07053v1.pdf")
220/5:
try:
    pdf_loader = PyPDFLoader(file)
220/6:
langchain_documents = []
try:
    pdf_loader = PyPDFLoader(file)
    pdf_documents = pdf_loader(load)
    pdf_documents.metadata.update({
            "url":"https://arxiv.org/pdf/2406.07053v1"
            "title": "TelecomRAG: Taming Telecom Standards with Retrieval Augmented Generation and LLMs",
            "tags": ["Large Language Model", "Telecom", "Article", "Industry","RAG","TelecomRAG"], "authors": ["Girma M. Yilma", "Jose A. Ayala-Romero", "Andres Garcia-Saavedra", "Xavier", "Costa-Perez"]})

    langchain_documents.append(Document(page_content=pdf_documents.page_content,
                                        metadata=pdf_documents.metadata))

except Exception as e:
    print(f"Error loading file {file}: {e}")
220/7:
langchain_documents = []
try:
    pdf_loader = PyPDFLoader(file)
    pdf_documents = pdf_loader(load)
    pdf_documents.metadata.update({
            "url":"https://arxiv.org/pdf/2406.07053v1",
            "title":"TelecomRAG: Taming Telecom Standards with Retrieval Augmented Generation and LLMs"
            "tags": ["Large Language Model", "Telecom", "Article", "Industry","RAG","TelecomRAG"], "authors": ["Girma M. Yilma", "Jose A. Ayala-Romero", "Andres Garcia-Saavedra", "Xavier", "Costa-Perez"]})

    langchain_documents.append(Document(page_content=pdf_documents.page_content,
                                        metadata=pdf_documents.metadata))

except Exception as e:
    print(f"Error loading file {file}: {e}")
220/8:
langchain_documents = []
try:
    pdf_loader = PyPDFLoader(file)
    pdf_documents = pdf_loader(load)
    pdf_documents.metadata.update({
            "url":"https://arxiv.org/pdf/2406.07053v1",
            "title":"TelecomRAG: Taming Telecom Standards with Retrieval Augmented Generation and LLMs",
            "tags": ["Large Language Model", "Telecom", "Article", "Industry","RAG","TelecomRAG"], "authors": ["Girma M. Yilma", "Jose A. Ayala-Romero", "Andres Garcia-Saavedra", "Xavier", "Costa-Perez"]})

    langchain_documents.append(Document(page_content=pdf_documents.page_content,
                                        metadata=pdf_documents.metadata))

except Exception as e:
    print(f"Error loading file {file}: {e}")
220/9:
langchain_documents = []
try:
    pdf_loader = PyPDFLoader("articles/2406.07053v1.pdf")
    pdf_documents = pdf_loader(load)
    pdf_documents.metadata.update({
            "url":"https://arxiv.org/pdf/2406.07053v1",
            "title":"TelecomRAG: Taming Telecom Standards with Retrieval Augmented Generation and LLMs",
            "tags": ["Large Language Model", "Telecom", "Article", "Industry","RAG","TelecomRAG"], "authors": ["Girma M. Yilma", "Jose A. Ayala-Romero", "Andres Garcia-Saavedra", "Xavier", "Costa-Perez"]})

    langchain_documents.append(Document(page_content=pdf_documents.page_content,
                                        metadata=pdf_documents.metadata))

except Exception as e:
    print(f"Error loading file {file}: {e}")
220/10:
from langchain.document_loaders import PyPDFLoader
from langchain_core.documents import Document
220/11:
langchain_documents = []
try:
    pdf_loader = PyPDFLoader("articles/2406.07053v1.pdf")
    pdf_documents = pdf_loader(load)
    pdf_documents.metadata.update({
            "url":"https://arxiv.org/pdf/2406.07053v1",
            "title":"TelecomRAG: Taming Telecom Standards with Retrieval Augmented Generation and LLMs",
            "tags": ["Large Language Model", "Telecom", "Article", "Industry","RAG","TelecomRAG"], "authors": ["Girma M. Yilma", "Jose A. Ayala-Romero", "Andres Garcia-Saavedra", "Xavier", "Costa-Perez"]})

    langchain_documents.append(Document(page_content=pdf_documents.page_content,
                                        metadata=pdf_documents.metadata))

except Exception as e:
    print(f"Error loading file {file}: {e}")
220/12:
langchain_documents = []
try:
    pdf_loader = PyPDFLoader("articles/2406.07053v1.pdf")
    pdf_documents = pdf_loader.load()
    pdf_documents.metadata.update({
            "url":"https://arxiv.org/pdf/2406.07053v1",
            "title":"TelecomRAG: Taming Telecom Standards with Retrieval Augmented Generation and LLMs",
            "tags": ["Large Language Model", "Telecom", "Article", "Industry","RAG","TelecomRAG"], "authors": ["Girma M. Yilma", "Jose A. Ayala-Romero", "Andres Garcia-Saavedra", "Xavier", "Costa-Perez"]})

    langchain_documents.append(Document(page_content=pdf_documents.page_content,
                                        metadata=pdf_documents.metadata))

except Exception as e:
    print(f"Error loading file {file}: {e}")
220/13:
langchain_documents = []
try:
    pdf_loader = PyPDFLoader("articles/2406.07053v1.pdf")
    pdf_documents = pdf_loader.load()
    pdf_documents.metadata.update({
            "url":"https://arxiv.org/pdf/2406.07053v1",
            "title":"TelecomRAG: Taming Telecom Standards with Retrieval Augmented Generation and LLMs",
            "tags": ["Large Language Model", "Telecom", "Article", "Industry","RAG","TelecomRAG"], "authors": ["Girma M. Yilma", "Jose A. Ayala-Romero", "Andres Garcia-Saavedra", "Xavier", "Costa-Perez"]})

    langchain_documents.append(Document(page_content=pdf_documents.page_content,
                                        metadata=pdf_documents.metadata))

except Exception as e:
    print(f"Error loading file {file}: {e}")
220/14:
langchain_documents = []
try:
    pdf_loader = PyPDFLoader("articles/2406.07053v1.pdf")
    pdf_documents = pdf_loader.load()
    pdf_documents.metadata.update({
            "url":"https://arxiv.org/pdf/2406.07053v1",
            "title":"TelecomRAG: Taming Telecom Standards with Retrieval Augmented Generation and LLMs",
            "tags": ["Large Language Model", "Telecom", "Article", "Industry","RAG","TelecomRAG"], "authors": ["Girma M. Yilma", "Jose A. Ayala-Romero", "Andres Garcia-Saavedra", "Xavier", "Costa-Perez"]})

    langchain_documents.append(Document(page_content=pdf_documents.page_content,metadata=pdf_documents.metadata))

except Exception as e:
    print(f"Error loading file {file}: {e}")
220/15:
from langchain.schema import Document
from langchain.document_loaders import PyPDFLoader

langchain_documents = []
try:
    pdf_loader = PyPDFLoader("articles/2406.07053v1.pdf")
    pdf_documents = pdf_loader.load()

    # Iterate over the documents and update metadata
    for doc in pdf_documents:
        doc.metadata.update({
            "url": "https://arxiv.org/pdf/2406.07053v1",
            "title": "TelecomRAG: Taming Telecom Standards with Retrieval Augmented Generation and LLMs",
            "tags": ["Large Language Model", "Telecom", "Article", "Industry", "RAG", "TelecomRAG"],
            "authors": ["Girma M. Yilma", "Jose A. Ayala-Romero", "Andres Garcia-Saavedra", "Xavier", "Costa-Perez"]
        })
        langchain_documents.append(doc)

except Exception as e:
    print(f"Error loading file: {e}")
220/16: len(pdf_documents)
220/17: len(langchain_documents)
220/18: langchain_documents[:1]
220/19:
from langchain_ollama import OllamaEmbeddings

embeddings = OllamaEmbeddings(
    model="llama3.2",
)
220/20:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(langchain_documents)
220/21:

vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
220/22:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(langchain_documents)
220/23: len(chunks)
220/24:

vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
220/25:
for chunk in chunks:
    print(chunk.metadata['author'])
220/26:
for chunk in chunks:
    print(chunk.metadata['authors'])
220/27:
for chunk in chunks:
    print(chunk.metadata)
220/28:
for chunk in chunks:
    print(set(chunk.metadata))
220/29: query = "Which color is the sky?"
220/30:
docs = new_db.similarity_search_with_score(query)
    print('Retrieved docs:', docs)
    print('Metadata of the most relevant document:', docs[0][0].metadata)
220/31:
docs = new_db.similarity_search_with_score(query)
print('Retrieved docs:', docs)
print('Metadata of the most relevant document:', docs[0][0].metadata)
220/32: total_vectors=vectorstore.index
220/33: total_vectors
220/34: total_vectors=vectorstore.index.ntotal
220/35: total_vectors
220/36: dimensions = vectorstore.index.d
220/37: dimensions
220/38: print(f"There are {total_vectors} vectors with {dimensions:,} dimensions in the vector store")
221/1:
# Prework
vectors = []
documents = []
doc_types = []
colors = []
color_map = {'products':'blue', 'employees':'green', 'contracts':'red', 'company':'orange'}

for i in range(total_vectors):
    vectors.append(vectorstore.index.reconstruct(i))
    doc_id = vectorstore.index_to_docstore_id[i]
    document = vectorstore.docstore.search(doc_id)
    documents.append(document.page_content)
    doc_type = document.metadata['doc_type']
    doc_types.append(doc_type)
    colors.append(color_map[doc_type])
    
vectors = np.array(vectors)
220/39:
# Prework
vectors = []
documents = []
doc_types = []
colors = []
color_map = {'products':'blue', 'employees':'green', 'contracts':'red', 'company':'orange'}

for i in range(total_vectors):
    vectors.append(vectorstore.index.reconstruct(i))
    doc_id = vectorstore.index_to_docstore_id[i]
    document = vectorstore.docstore.search(doc_id)
    documents.append(document.page_content)
    doc_type = document.metadata['doc_type']
    doc_types.append(doc_type)
    colors.append(color_map[doc_type])
    
vectors = np.array(vectors)
220/40:
# Prework
vectors = []
documents = []
doc_types = []
colors = []
color_map = {'products':'blue', 'employees':'green', 'contracts':'red', 'company':'orange'}

for i in range(total_vectors):
    vectors.append(vectorstore.index.reconstruct(i))
    doc_id = vectorstore.index_to_docstore_id[i]
    document = vectorstore.docstore.search(doc_id)
    documents.append(document.page_content)
    doc_type = document.metadata['authors']
    doc_types.append(doc_type)
    colors.append(color_map[doc_type])
    
vectors = np.array(vectors)
220/41:
# Prework
vectors = []
documents = []
doc_types = []
colors = []
color_map = {'products':'blue', 'employees':'green', 'contracts':'red', 'company':'orange'}

for i in range(total_vectors):
    vectors.append(vectorstore.index.reconstruct(i))
    doc_id = vectorstore.index_to_docstore_id[i]
    document = vectorstore.docstore.search(doc_id)
    documents.append(document.page_content)
    doc_type = document.metadata['url']
    doc_types.append(doc_type)
    colors.append(color_map[doc_type])
    
vectors = np.array(vectors)
220/42:
# Prework
vectors = []
documents = []
doc_types = []
colors = []
color_map = {'authors':'blue'}

for i in range(total_vectors):
    vectors.append(vectorstore.index.reconstruct(i))
    doc_id = vectorstore.index_to_docstore_id[i]
    document = vectorstore.docstore.search(doc_id)
    documents.append(document.page_content)
    doc_type = document.metadata['authors']
    doc_types.append(doc_type)
    colors.append(color_map[doc_type])
    
vectors = np.array(vectors)
220/43:
# Prework
vectors = []
documents = []
doc_types = []
colors = []
color_map = {'authors':'blue'}

for i in range(total_vectors):
    vectors.append(vectorstore.index.reconstruct(i))
    doc_id = vectorstore.index_to_docstore_id[i]
    document = vectorstore.docstore.search(doc_id)
    documents.append(document.page_content)
    doc_types.append(doc_type)
    colors.append("blue")
    
vectors = np.array(vectors)
220/44:
# We humans find it easier to visalize things in 2D!
# Reduce the dimensionality of the vectors to 2D using t-SNE
# (t-distributed stochastic neighbor embedding)

tsne = TSNE(n_components=2, random_state=42)
reduced_vectors = tsne.fit_transform(vectors)

# Create the 2D scatter plot
fig = go.Figure(data=[go.Scatter(
    x=reduced_vectors[:, 0],
    y=reduced_vectors[:, 1],
    mode='markers',
    marker=dict(size=5, color=colors, opacity=0.8),
    text=[f"Type: {t}<br>Text: {d[:100]}..." for t, d in zip(doc_types, documents)],
    hoverinfo='text'
)])

fig.update_layout(
    title='2D FAISS Vector Store Visualization',
    scene=dict(xaxis_title='x',yaxis_title='y'),
    width=800,
    height=600,
    margin=dict(r=20, b=10, l=10, t=40)
)

fig.show()
220/45:
# We humans find it easier to visalize things in 2D!
# Reduce the dimensionality of the vectors to 2D using t-SNE
# (t-distributed stochastic neighbor embedding)

tsne = TSNE(n_components=2, random_state=42)
reduced_vectors = tsne.fit_transform(vectors)

# Create the 2D scatter plot
fig = go.Figure(data=[go.Scatter(
    x=reduced_vectors[:, 0],
    y=reduced_vectors[:, 1],
    mode='markers',
    marker=dict(size=5, color=colors, opacity=0.8),
    hoverinfo='text'
)])

fig.update_layout(
    title='2D FAISS Vector Store Visualization',
    scene=dict(xaxis_title='x',yaxis_title='y'),
    width=800,
    height=600,
    margin=dict(r=20, b=10, l=10, t=40)
)

fig.show()
220/46:
# We humans find it easier to visalize things in 2D!
# Reduce the dimensionality of the vectors to 2D using t-SNE
# (t-distributed stochastic neighbor embedding)

tsne = TSNE(n_components=2, random_state=42,perplexity=6)
reduced_vectors = tsne.fit_transform(vectors)

# Create the 2D scatter plot
fig = go.Figure(data=[go.Scatter(
    x=reduced_vectors[:, 0],
    y=reduced_vectors[:, 1],
    mode='markers',
    marker=dict(size=5, color=colors, opacity=0.8),
    hoverinfo='text'
)])

fig.update_layout(
    title='2D FAISS Vector Store Visualization',
    scene=dict(xaxis_title='x',yaxis_title='y'),
    width=800,
    height=600,
    margin=dict(r=20, b=10, l=10, t=40)
)

fig.show()
220/47: query = "What is telecomRag?"
220/48: retriever = vectorstore.as_retriever()
220/49: retrieved_documents = retriver.invoke(query)
220/50: retrieved_documents = retriever.invoke(query)
220/51: retrieved_documents[0].page_content
220/52:
from langchain_ollama.llms import OllamaLLM
model = OllamaLLM(model="llama3.2")

chain = prompt | model
220/53: prompt = "What is telecomRag?"
220/54:
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama.llms import OllamaLLM

template = """Question: {question}

Answer: Let's think step by step."""

prompt = ChatPromptTemplate.from_template(template)

model = OllamaLLM(model="llama3.2")

chain = prompt | model

chain.invoke({"question": inputl})
220/55: inputl = "What is telecomRag?"
220/56: retriever = vectorstore.as_retriever()
220/57:
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama.llms import OllamaLLM

template = """Question: {question}

Answer: Let's think step by step."""

prompt = ChatPromptTemplate.from_template(template)

model = OllamaLLM(model="llama3.2")

chain = prompt | model

chain.invoke({"question": inputl})
220/58:
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_experimental.text_splitter import SemanticChunker
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate
from langchain.chains.llm import LLMChain
from langchain.chains.combine_documents.stuff import StuffDocumentsChain
from langchain.chains import RetrievalQA
import gradio as gr
220/59:
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_exfrom langchain_community.document_loaders import PDFPlumberLoader
from langchain_experimental.text_splitter import SemanticChunker
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate
from langchain.chains.llm import LLMChain
from langchain.chains.combine_documents.stuff import StuffDocumentsChain
from langchain.chains import RetrievalQA
import gradio as grperimental.text_splitter import SemanticChunker
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate
from langchain.chains.llm import LLMChain
from langchain.chains.combine_documents.stuff import StuffDocumentsChain
from langchain.chains import RetrievalQA
import gradio as gr
220/60:
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_ex from langchain_community.document_loaders import PDFPlumberLoader
from langchain_experimental.text_splitter import SemanticChunker
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate
from langchain.chains.llm import LLMChain
from langchain.chains.combine_documents.stuff import StuffDocumentsChain
from langchain.chains import RetrievalQA
import gradio as grperimental.text_splitter import SemanticChunker
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate
from langchain.chains.llm import LLMChain
from langchain.chains.combine_documents.stuff import StuffDocumentsChain
from langchain.chains import RetrievalQA
import gradio as gr
220/61:
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_experimental.text_splitter import SemanticChunker
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate
from langchain.chains.llm import LLMChain
from langchain.chains.combine_documents.stuff import StuffDocumentsChain
from langchain.chains import RetrievalQA
import gradio as gr
220/62: retriever = vector.as_retriever(search_type="similarity", search_kwargs={"k": 3})
220/63: retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 3})
220/64: retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 3})
220/65: retriever
220/66:
prompt = """
1. Use the following pieces of context to answer the question at the end.
2. If you don't know the answer, just say that "I don't know" but don't make up an answer on your own.\n
3. Keep the answer crisp and limited to 3,4 sentences.

Context: {context}

Question: {question}

Helpful Answer:"""
220/67: QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)
220/68:
llm_chain = LLMChain(
                  llm=llm, 
                  prompt=QA_CHAIN_PROMPT, 
                  callbacks=None, 
                  verbose=True)
220/69:
llm_chain = LLMChain(
                  llm="llama3.2, 
                  prompt=QA_CHAIN_PROMPT, 
                  callbacks=None, 
                  verbose=True)
220/70:
llm_chain = LLMChain(
                  llm="llama3.2", 
                  prompt=QA_CHAIN_PROMPT, 
                  callbacks=None, 
                  verbose=True)
220/71:
llm_chain = LLMChain(
                  llm=Ollama(model="llama3.2"), 
                  prompt=QA_CHAIN_PROMPT, 
                  callbacks=None, 
                  verbose=True)
220/72: !pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM
220/73:
from langchain_ollama.llms import OllamaLLM
llm = OllamaLLM(model="llama3.2")
llm_chain = LLMChain(
                  llm=Ollama(model="llama3.2"), 
                  prompt=QA_CHAIN_PROMPT, 
                  callbacks=None, 
                  verbose=True)
220/74:
document_prompt = PromptTemplate(
    input_variables=["page_content", "source"],
    template="Context:\ncontent:{page_content}\nsource:{source}",
)
220/75:
combine_documents_chain = StuffDocumentsChain(
                  llm_chain=llm_chain,
                  document_variable_name="context",
                  document_prompt=document_prompt,
                  callbacks=None
220/76:
combine_documents_chain = StuffDocumentsChain(
                  llm_chain=llm_chain,
                  document_variable_name="context",
                  document_prompt=document_prompt,
                  callbacks=None)
220/77:
combine_documents_chain = StuffDocumentsChain(
                  llm_chain=llm_chain,
                  document_variable_name="context",
                  document_prompt=document_prompt)
220/78:
qa = RetrievalQA(
                  combine_documents_chain=combine_documents_chain,
                  verbose=True,
                  retriever=retriever,
                  return_source_documents=True)
220/79:
qa = RetrievalQA.from_llm(
                  llm,
                   retriever=retriever,prompt=prompt)
220/80:
qa = RetrievalQA.from_llm(
                  llm,
                   retriever=retriever)
220/81:
qa = RetrievalQA.from_llm(
                  llm,
                   retriever=retriever)
220/82:
def respnd(question,history):
    return qa(question["result"])
220/83:
gr.ChatInterface(
    respond,
    chatbot=gr.Chatbot(height=500),
    textbox=gr.Textbox(placeholder="Ask me question related to Plants and their diseases", container=False, scale=7),
    title="Plant's Chatbot",
    examples=["What are different kinds of plant diseases", "What is Stewartâs wilt disease"],
    cache_examples=True,
    retry_btn=None,

).launch(share = True)
220/84:
def respond(question,history):
    return qa(question["result"])
220/85:
gr.ChatInterface(
    respond,
    chatbot=gr.Chatbot(height=500),
    textbox=gr.Textbox(placeholder="Ask me question related to Plants and their diseases", container=False, scale=7),
    title="Plant's Chatbot",
    examples=["What are different kinds of plant diseases", "What is Stewartâs wilt disease"],
    cache_examples=True,
    retry_btn=None,

).launch(share = True)
220/86:
gr.ChatInterface(
    respond,
    chatbot=gr.Chatbot(height=500),
    textbox=gr.Textbox(placeholder="Ask me question related to Plants and their diseases", container=False, scale=7),
    title="Plant's Chatbot",
    examples=["What are different kinds of plant diseases", "What is Stewartâs wilt disease"],
    cache_examples=True,
    

).launch(share = True)
220/87: results = vectorstore.similarity_search(query,k=2)
220/88:
from ollama import OllamaClient
ollama_client = Oll
220/89:
from ollama import OllamaClient
ollama_client = OllamaClient(model="llama3.2")
prompt = f"Context:\n{context}\n\nSoru: {query}\n\nCevap:"
response = ollama_client.generate(prompt)
220/90:
template = """Use the following pieces of context to answer the question at the end.
    If you don't know the answer, just say that you don't know, don't try to make up an answer.
    Use three sentences maximum and keep the answer as concise as possible.
    {context}
    Question: {question}
    Helpful Answer:"""
    QA_CHAIN_PROMPT = PromptTemplate(
        input_variables=["context", "question"],
        template=template,
    )

    llm = Ollama(model="llama3.2", callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))
    qa_chain = RetrievalQA.from_chain_type(
        llm,
        retriever=vectorstore.as_retriever(),
        chain_type_kwargs={"prompt": QA_CHAIN_PROMPT},
    )

    result = qa_chain({"query": query})
220/91:
    template = """Use the following pieces of context to answer the question at the end.
    If you don't know the answer, just say that you don't know, don't try to make up an answer.
    Use three sentences maximum and keep the answer as concise as possible.
    {context}
    Question: {question}
    Helpful Answer:"""
    QA_CHAIN_PROMPT = PromptTemplate(
        input_variables=["context", "question"],
        template=template,
    )

    llm = Ollama(model="llama3.2", callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))
    qa_chain = RetrievalQA.from_chain_type(
        llm,
        retriever=vectorstore.as_retriever(),
        chain_type_kwargs={"prompt": QA_CHAIN_PROMPT},
    )

    result = qa_chain({"query": query})
220/92:
from langchain.callbacks.manager import CallbackManager
template = """Use the following pieces of context to answer the question at the end.
    If you don't know the answer, just say that you don't know, don't try to make up an answer.
    Use three sentences maximum and keep the answer as concise as possible.
    {context}
    Question: {question}
    Helpful Answer:"""
QA_CHAIN_PROMPT = PromptTemplate(
        input_variables=["context", "question"],
        template=template,
)

llm = Ollama(model="llama3.2", callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))
qa_chain = RetrievalQA.from_chain_type(
        llm,
        retriever=vectorstore.as_retriever(),
        chain_type_kwargs={"prompt": QA_CHAIN_PROMPT},
)

result = qa_chain({"query": query})
220/93:
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
template = """Use the following pieces of context to answer the question at the end.
    If you don't know the answer, just say that you don't know, don't try to make up an answer.
    Use three sentences maximum and keep the answer as concise as possible.
    {context}
    Question: {question}
    Helpful Answer:"""
QA_CHAIN_PROMPT = PromptTemplate(
        input_variables=["context", "question"],
        template=template,
)

llm = Ollama(model="llama3.2", callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))
qa_chain = RetrievalQA.from_chain_type(
        llm,
        retriever=vectorstore.as_retriever(),
        chain_type_kwargs={"prompt": QA_CHAIN_PROMPT},
)

result = qa_chain({"query": query})
220/94:
from langchain_ollama import ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

model = ChatOllama(
    model="llama3.2",
)
prompt = ChatPromptTemplate.from_template(
    "Summarize the main themes in these retrieved docs: {docs}"
)


# Convert loaded documents into strings by concatenating their content
# and ignoring metadata
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in results)


chain = {"docs": format_docs} | prompt | model | StrOutputParser()

question = "What are LLMs for specific application domains"

docs = vectorstore.similarity_search(question)

chain.invoke(docs)
220/95:
from langchain_ollama import ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

model = ChatOllama(
    model="llama3.2",
)
prompt = ChatPromptTemplate.from_template(
    "Summarize the main themes in these retrieved docs: {docs}"
)


# Convert loaded documents into strings by concatenating their content
# and ignoring metadata
def format_docs(results):
    return "\n\n".join(doc.page_content for doc in results)


chain = {"docs": format_docs} | prompt | model | StrOutputParser()

question = "What are LLMs for specific application domains"

docs = vectorstore.similarity_search(question)

chain.invoke(docs)
220/96:
from langchain_ollama import ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

model = ChatOllama(
    model="llama3.2",
)
prompt = ChatPromptTemplate.from_template(
    "Summarize the main themes in these retrieved docs: {docs}"
)


# Convert loaded documents into strings by concatenating their content
# and ignoring metadata
def format_docs(results):
    return "\n\n".join(doc.page_content for doc in results)


chain = {"docs": format_docs} | prompt | model | StrOutputParser()

question = "What are LLMs for specific application domains"

docs = vectorstore.similarity_search(question)

len(docs)
220/97:
from langchain_ollama import ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

model = ChatOllama(
    model="llama3.2",
)
prompt = ChatPromptTemplate.from_template(
    "Summarize the main themes in these retrieved docs: {docs}"
)


# Convert loaded documents into strings by concatenating their content
# and ignoring metadata
def format_docs(resu):
    return "\n\n".join(doc.page_content for doc in results)


chain = {"docs": format_docs} | prompt | model | StrOutputParser()

question = "What is the name of this article?"

docs = vectorstore.similarity_search(question)

chain.invoke(docs)
221/2:
# imports

import os
import glob
from dotenv import load_dotenv
import gradio as gr
221/3:
# price is a factor for our company, so we're going to use a low cost model

MODEL = "gpt-4o-mini"
db_name = "vector_db"
221/4:
# imports for langchain

from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
# from langchain_chroma import Chroma
from langchain.vectorstores import FAISS
import numpy as np
from sklearn.manifold import TSNE
import plotly.graph_objects as go
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
221/5:
# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
221/6:
# Read in documents using LangChain's loaders
# Take everything in all the sub-folders of our knowledgebase

folders = glob.glob("knowledge-base/*")

# With thanks to CG and Jon R, students on the course, for this fix needed for some users 
text_loader_kwargs = {'encoding': 'utf-8'}
# If that doesn't work, some Windows users might need to uncomment the next line instead
# text_loader_kwargs={'autodetect_encoding': True}

documents = []
for folder in folders:
    doc_type = os.path.basename(folder)
    loader = DirectoryLoader(folder, glob="**/*.md", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)
    folder_docs = loader.load()
    for doc in folder_docs:
        doc.metadata["doc_type"] = doc_type
        documents.append(doc)
221/7:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(documents)
221/8: len(chunks)
221/9:
doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)
print(f"Document types found: {', '.join(doc_types)}")
221/10:
# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk
# Chroma is a popular open source Vector Database based on SQLLite

embeddings = OpenAIEmbeddings()

# Create vectorstore

# BEFORE
# vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)

# AFTER
vectorstore = FAISS.from_documents(chunks, embedding=embeddings)

total_vectors = vectorstore.index.ntotal
dimensions = vectorstore.index.d

print(f"There are {total_vectors} vectors with {dimensions:,} dimensions in the vector store")
221/11:
# Prework
vectors = []
documents = []
doc_types = []
colors = []
color_map = {'products':'blue', 'employees':'green', 'contracts':'red', 'company':'orange'}

for i in range(total_vectors):
    vectors.append(vectorstore.index.reconstruct(i))
    doc_id = vectorstore.index_to_docstore_id[i]
    document = vectorstore.docstore.search(doc_id)
    documents.append(document.page_content)
    doc_type = document.metadata['doc_type']
    doc_types.append(doc_type)
    colors.append(color_map[doc_type])
    
vectors = np.array(vectors)
221/12:
# We humans find it easier to visalize things in 2D!
# Reduce the dimensionality of the vectors to 2D using t-SNE
# (t-distributed stochastic neighbor embedding)

tsne = TSNE(n_components=2, random_state=42)
reduced_vectors = tsne.fit_transform(vectors)

# Create the 2D scatter plot
fig = go.Figure(data=[go.Scatter(
    x=reduced_vectors[:, 0],
    y=reduced_vectors[:, 1],
    mode='markers',
    marker=dict(size=5, color=colors, opacity=0.8),
    text=[f"Type: {t}<br>Text: {d[:100]}..." for t, d in zip(doc_types, documents)],
    hoverinfo='text'
)])

fig.update_layout(
    title='2D FAISS Vector Store Visualization',
    scene=dict(xaxis_title='x',yaxis_title='y'),
    width=800,
    height=600,
    margin=dict(r=20, b=10, l=10, t=40)
)

fig.show()
221/13:
# Let's try 3D!

tsne = TSNE(n_components=3, random_state=42)
reduced_vectors = tsne.fit_transform(vectors)

# Create the 3D scatter plot
fig = go.Figure(data=[go.Scatter3d(
    x=reduced_vectors[:, 0],
    y=reduced_vectors[:, 1],
    z=reduced_vectors[:, 2],
    mode='markers',
    marker=dict(size=5, color=colors, opacity=0.8),
    text=[f"Type: {t}<br>Text: {d[:100]}..." for t, d in zip(doc_types, documents)],
    hoverinfo='text'
)])

fig.update_layout(
    title='3D FAISS Vector Store Visualization',
    scene=dict(xaxis_title='x', yaxis_title='y', zaxis_title='z'),
    width=900,
    height=700,
    margin=dict(r=20, b=10, l=10, t=40)
)

fig.show()
221/14:
# create a new Chat with OpenAI
llm = ChatOpenAI(temperature=0.7, model_name=MODEL)

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)
221/15:
# create a new Chat with OpenAI
llm = ChatOpenAI(temperature=0.7, model_name=MODEL)

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)
221/16:
query = "Can you describe Insurellm in a few sentences"
result = conversation_chain.invoke({"question":query})
print(result["answer"])
221/17:
# Wrapping that in a function

def chat(message, history):
    result = conversation_chain.invoke({"question": message})
    return result["answer"]
221/18:
# And in Gradio:

view = gr.ChatInterface(chat).launch()
224/1:
# imports

import os
import glob
from dotenv import load_dotenv
import gradio as gr
224/2:
# price is a factor for our company, so we're going to use a low cost model

MODEL = "gpt-4o-mini"
db_name = "vector_db"
224/3:
# imports for langchain

from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
# from langchain_chroma import Chroma
from langchain.vectorstores import FAISS
import numpy as np
from sklearn.manifold import TSNE
import plotly.graph_objects as go
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
224/4:
# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
224/5:
# Read in documents using LangChain's loaders
# Take everything in all the sub-folders of our knowledgebase

folders = glob.glob("knowledge-base/*")

# With thanks to CG and Jon R, students on the course, for this fix needed for some users 
text_loader_kwargs = {'encoding': 'utf-8'}
# If that doesn't work, some Windows users might need to uncomment the next line instead
# text_loader_kwargs={'autodetect_encoding': True}

documents = []
for folder in folders:
    doc_type = os.path.basename(folder)
    loader = DirectoryLoader(folder, glob="**/*.md", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)
    folder_docs = loader.load()
    for doc in folder_docs:
        doc.metadata["doc_type"] = doc_type
        documents.append(doc)
224/6:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(documents)
224/7:
# Read in documents using LangChain's loaders
# Take everything in all the sub-folders of our knowledgebase

folders = glob.glob("knowledge-base/*")

# With thanks to CG and Jon R, students on the course, for this fix needed for some users 
text_loader_kwargs = {'encoding': 'utf-8'}
# If that doesn't work, some Windows users might need to uncomment the next line instead
# text_loader_kwargs={'autodetect_encoding': True}

documents = []
for folder in folders:
    doc_type = os.path.basename(folder)
    loader = DirectoryLoader(folder, glob="**/*.md", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)
    folder_docs = loader.load()
    for doc in folder_docs:
        doc.metadata["doc_type"] = doc_type
        documents.append(doc)
224/8:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(documents)
224/9: len(chunks)
224/10:
doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)
print(f"Document types found: {', '.join(doc_types)}")
224/11:
# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk
# Chroma is a popular open source Vector Database based on SQLLite

embeddings = OpenAIEmbeddings()

# Create vectorstore

# BEFORE
# vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)

# AFTER
vectorstore = FAISS.from_documents(chunks, embedding=embeddings)

total_vectors = vectorstore.index.ntotal
dimensions = vectorstore.index.d

print(f"There are {total_vectors} vectors with {dimensions:,} dimensions in the vector store")
224/12:
# create a new Chat with OpenAI
llm = ChatOpenAI(temperature=0.7, model_name=MODEL)

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)
224/13:
# create a new Chat with OpenAI
llm = ChatOpenAI(temperature=0.7, model_name=MODEL)

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)
224/14:
# create a new Chat with OpenAI
llm = ChatOpenAI(temperature=0.7, model_name=MODEL)

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)
224/15:
query = "Can you describe Insurellm in a few sentences"
result = conversation_chain.invoke({"question":query})
print(result["answer"])
224/16:
import numpy as np
from trulens.core import Feedback
from trulens.providers.openai import OpenAI

# Initialize provider class
provider = OpenAI()

# select context to be used in feedback. the location of context is app specific.
context = TruChain.select_context(rag_chain)

# Define a groundedness feedback function
f_groundedness = (
    Feedback(
        provider.groundedness_measure_with_cot_reasons, name="Groundedness"
    )
    .on(context.collect())  # collect context chunks into a list
    .on_output()
)

# Question/answer relevance between overall question and answer.
f_answer_relevance = Feedback(
    provider.relevance_with_cot_reasons, name="Answer Relevance"
).on_input_output()
# Context relevance between question and each context chunk.
f_context_relevance = (
    Feedback(
        provider.context_relevance_with_cot_reasons, name="Context Relevance"
    )
    .on_input()
    .on(context)
    .aggregate(np.mean)
)
224/17:  !pip install trulens trulens-apps-langchain trulens-providers-openai openai langchain langchainhub langchain-openai langchain_community faiss-cpu bs4 tiktoken
224/18:
import numpy as np
from trulens.core import Feedback
from trulens.providers.openai import OpenAI

# Initialize provider class
provider = OpenAI()

# select context to be used in feedback. the location of context is app specific.
context = TruChain.select_context(rag_chain)

# Define a groundedness feedback function
f_groundedness = (
    Feedback(
        provider.groundedness_measure_with_cot_reasons, name="Groundedness"
    )
    .on(context.collect())  # collect context chunks into a list
    .on_output()
)

# Question/answer relevance between overall question and answer.
f_answer_relevance = Feedback(
    provider.relevance_with_cot_reasons, name="Answer Relevance"
).on_input_output()
# Context relevance between question and each context chunk.
f_context_relevance = (
    Feedback(
        provider.context_relevance_with_cot_reasons, name="Context Relevance"
    )
    .on_input()
    .on(context)
    .aggregate(np.mean)
)
224/19:
# Imports main tools:
from trulens.apps.langchain import TruChain
from trulens.core import TruSession

session = TruSession()
session.reset_database()
225/1:
# Imports main tools:
from trulens.apps.langchain import TruChain
from trulens.core import TruSession

session = TruSession()
session.reset_database()
225/2:
# Imports from LangChain to build app
import bs4
from langchain import hub
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import WebBaseLoader
from langchain.schema import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
225/3:
import numpy as np
from trulens.core import Feedback
from trulens.providers.openai import OpenAI

# Initialize provider class
provider = OpenAI()

# select context to be used in feedback. the location of context is app specific.
context = TruChain.select_context(rag_chain)

# Define a groundedness feedback function
f_groundedness = (
    Feedback(
        provider.groundedness_measure_with_cot_reasons, name="Groundedness"
    )
    .on(context.collect())  # collect context chunks into a list
    .on_output()
)

# Question/answer relevance between overall question and answer.
f_answer_relevance = Feedback(
    provider.relevance_with_cot_reasons, name="Answer Relevance"
).on_input_output()
# Context relevance between question and each context chunk.
f_context_relevance = (
    Feedback(
        provider.context_relevance_with_cot_reasons, name="Context Relevance"
    )
    .on_input()
    .on(context)
    .aggregate(np.mean)
)
225/4:
# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
225/5:
# imports

import os
import glob
from dotenv import load_dotenv
import gradio as gr
225/6:
# price is a factor for our company, so we're going to use a low cost model

MODEL = "gpt-4o-mini"
db_name = "vector_db"
225/7:
# Imports main tools:
from trulens.apps.langchain import TruChain
from trulens.core import TruSession

session = TruSession()
session.reset_database()
225/8:
# Imports from LangChain to build app
import bs4
from langchain import hub
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import WebBaseLoader
from langchain.schema import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
225/9:
# imports for langchain

from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
# from langchain_chroma import Chroma
from langchain.vectorstores import FAISS
import numpy as np
from sklearn.manifold import TSNE
import plotly.graph_objects as go
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
225/10:
# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
225/11:
# Read in documents using LangChain's loaders
# Take everything in all the sub-folders of our knowledgebase

folders = glob.glob("knowledge-base/*")

# With thanks to CG and Jon R, students on the course, for this fix needed for some users 
text_loader_kwargs = {'encoding': 'utf-8'}
# If that doesn't work, some Windows users might need to uncomment the next line instead
# text_loader_kwargs={'autodetect_encoding': True}

documents = []
for folder in folders:
    doc_type = os.path.basename(folder)
    loader = DirectoryLoader(folder, glob="**/*.md", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)
    folder_docs = loader.load()
    for doc in folder_docs:
        doc.metadata["doc_type"] = doc_type
        documents.append(doc)
225/12:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(documents)
225/13: len(chunks)
225/14:
doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)
print(f"Document types found: {', '.join(doc_types)}")
225/15:
# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk
# Chroma is a popular open source Vector Database based on SQLLite

embeddings = OpenAIEmbeddings()

# Create vectorstore

# BEFORE
# vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)

# AFTER
vectorstore = FAISS.from_documents(chunks, embedding=embeddings)

total_vectors = vectorstore.index.ntotal
dimensions = vectorstore.index.d

print(f"There are {total_vectors} vectors with {dimensions:,} dimensions in the vector store")
225/16:
# create a new Chat with OpenAI
llm = ChatOpenAI(temperature=0.7, model_name=MODEL)

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)
225/17:
query = "Can you describe Insurellm in a few sentences"
result = conversation_chain.invoke({"question":query})
print(result["answer"])
225/18:
import numpy as np
from trulens.core import Feedback
from trulens.providers.openai import OpenAI

# Initialize provider class
provider = OpenAI()

# select context to be used in feedback. the location of context is app specific.
context = TruChain.select_context(rag_chain)

# Define a groundedness feedback function
f_groundedness = (
    Feedback(
        provider.groundedness_measure_with_cot_reasons, name="Groundedness"
    )
    .on(context.collect())  # collect context chunks into a list
    .on_output()
)

# Question/answer relevance between overall question and answer.
f_answer_relevance = Feedback(
    provider.relevance_with_cot_reasons, name="Answer Relevance"
).on_input_output()
# Context relevance between question and each context chunk.
f_context_relevance = (
    Feedback(
        provider.context_relevance_with_cot_reasons, name="Context Relevance"
    )
    .on_input()
    .on(context)
    .aggregate(np.mean)
)
226/1:
# imports

import os
import glob
from dotenv import load_dotenv
import gradio as gr
226/2:
# price is a factor for our company, so we're going to use a low cost model

MODEL = "gpt-4o-mini"
db_name = "vector_db"
226/3:
# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
226/4:
# Imports main tools:
from trulens.apps.langchain import TruChain
from trulens.core import TruSession

session = TruSession()
session.reset_database()
226/5:
# Imports from LangChain to build app
import bs4
from langchain import hub
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import WebBaseLoader
from langchain.schema import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
226/6:
loader = WebBaseLoader(
    web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/",),
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(
            class_=("post-content", "post-title", "post-header")
        )
    ),
)
docs = loader.load()
226/7:
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

embeddings = OpenAIEmbeddings()


text_splitter = RecursiveCharacterTextSplitter()
documents = text_splitter.split_documents(docs)
vectorstore = FAISS.from_documents(documents, embeddings)
226/8:
retriever = vectorstore.as_retriever()

prompt = hub.pull("rlm/rag-prompt")
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)


def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)
226/9: rag_chain.invoke("What is Task Decomposition?")
226/10:
import numpy as np
from trulens.core import Feedback
from trulens.providers.openai import OpenAI

# Initialize provider class
provider = OpenAI()

# select context to be used in feedback. the location of context is app specific.
context = TruChain.select_context(rag_chain)

# Define a groundedness feedback function
f_groundedness = (
    Feedback(
        provider.groundedness_measure_with_cot_reasons, name="Groundedness"
    )
    .on(context.collect())  # collect context chunks into a list
    .on_output()
)

# Question/answer relevance between overall question and answer.
f_answer_relevance = Feedback(
    provider.relevance_with_cot_reasons, name="Answer Relevance"
).on_input_output()
# Context relevance between question and each context chunk.
f_context_relevance = (
    Feedback(
        provider.context_relevance_with_cot_reasons, name="Context Relevance"
    )
    .on_input()
    .on(context)
    .aggregate(np.mean)
)
226/11:
tru_recorder = TruChain(
    rag_chain,
    app_name="ChatApplication",
    app_version="Chain1",
    feedbacks=[f_answer_relevance, f_context_relevance, f_groundedness],
)
226/12:
with tru_recorder as recording:
    llm_response = rag_chain.invoke("What is Task Decomposition?")

display(llm_response)
226/13: session.get_leaderboard()
226/14:
from trulens.dashboard.display import get_feedback_result

last_record = recording.records[-1]
get_feedback_result(last_record, "Context Relevance")
222/1:
from langchain_community.llms import Ollama
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

llm = Ollama(
    model="llama3.2",
    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),
)
226/15:
from langchain_community.llms.ollama import Ollama
retriever = vectorstore.as_retriever()

prompt = hub.pull("rlm/rag-prompt")
llm = Oll


def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)
226/16:
from langchain_community.llms.ollama import Ollama
retriever = vectorstore.as_retriever()

prompt = hub.pull("rlm/rag-prompt")
llm = Ollama(model="llama3.2")


def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)
226/17: rag_chain.invoke("What is Task Decomposition?")
226/18:
import numpy as np
from trulens.core import Feedback
from trulens.providers.openai import OpenAI

# Initialize provider class
provider = OpenAI()

# select context to be used in feedback. the location of context is app specific.
context = TruChain.select_context(rag_chain)

# Define a groundedness feedback function
f_groundedness = (
    Feedback(
        provider.groundedness_measure_with_cot_reasons, name="Groundedness"
    )
    .on(context.collect())  # collect context chunks into a list
    .on_output()
)

# Question/answer relevance between overall question and answer.
f_answer_relevance = Feedback(
    provider.relevance_with_cot_reasons, name="Answer Relevance"
).on_input_output()
# Context relevance between question and each context chunk.
f_context_relevance = (
    Feedback(
        provider.context_relevance_with_cot_reasons, name="Context Relevance"
    )
    .on_input()
    .on(context)
    .aggregate(np.mean)
)
226/19:
tru_recorder = TruChain(
    rag_chain,
    app_name="ChatApplication",
    app_version="Chain1",
    feedbacks=[f_answer_relevance, f_context_relevance, f_groundedness],
)
226/20:
with tru_recorder as recording:
    llm_response = rag_chain.invoke("What is Task Decomposition?")

display(llm_response)
226/21: session.get_leaderboard()
226/22:
from trulens.dashboard.display import get_feedback_result

last_record = recording.records[-1]
get_feedback_result(last_record, "Context Relevance")
226/23:
from trulens.dashboard.display import get_feedback_result

last_record = recording.records[-1]
get_feedback_result(last_record, "Context Relevance")
226/24:
import numpy as np
from trulens.core import Feedback
from trulens.providers.openai import OpenAI

# Initialize provider class
provider = Ollama()

# select context to be used in feedback. the location of context is app specific.
context = TruChain.select_context(rag_chain)

# Define a groundedness feedback function
f_groundedness = (
    Feedback(
        provider.groundedness_measure_with_cot_reasons, name="Groundedness"
    )
    .on(context.collect())  # collect context chunks into a list
    .on_output()
)

# Question/answer relevance between overall question and answer.
f_answer_relevance = Feedback(
    provider.relevance_with_cot_reasons, name="Answer Relevance"
).on_input_output()
# Context relevance between question and each context chunk.
f_context_relevance = (
    Feedback(
        provider.context_relevance_with_cot_reasons, name="Context Relevance"
    )
    .on_input()
    .on(context)
    .aggregate(np.mean)
)
225/19:
# imports

import os
import glob
from dotenv import load_dotenv
import gradio as gr
225/20:
# price is a factor for our company, so we're going to use a low cost model

MODEL = "gpt-4o-mini"
db_name = "vector_db"
225/21:
# Imports main tools:
from trulens.apps.langchain import TruChain
from trulens.core import TruSession

session = TruSession()
session.reset_database()
225/22:
# Imports from LangChain to build app
import bs4
from langchain import hub
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import WebBaseLoader
from langchain.schema import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
225/23:
# imports for langchain

from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
# from langchain_chroma import Chroma
from langchain.vectorstores import FAISS
import numpy as np
from sklearn.manifold import TSNE
import plotly.graph_objects as go
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
225/24:
# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
225/25:
# Read in documents using LangChain's loaders
# Take everything in all the sub-folders of our knowledgebase

folders = glob.glob("knowledge-base/*")

# With thanks to CG and Jon R, students on the course, for this fix needed for some users 
text_loader_kwargs = {'encoding': 'utf-8'}
# If that doesn't work, some Windows users might need to uncomment the next line instead
# text_loader_kwargs={'autodetect_encoding': True}

documents = []
for folder in folders:
    doc_type = os.path.basename(folder)
    loader = DirectoryLoader(folder, glob="**/*.md", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)
    folder_docs = loader.load()
    for doc in folder_docs:
        doc.metadata["doc_type"] = doc_type
        documents.append(doc)
225/26:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(documents)
225/27: len(chunks)
225/28:
doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)
print(f"Document types found: {', '.join(doc_types)}")
225/29:
# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk
# Chroma is a popular open source Vector Database based on SQLLite

from langchain_ollama import OllamaEmbeddings

embeddings = OllamaEmbeddings(
    model="llama3.2",
)

# Create vectorstore

# BEFORE
# vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)

# AFTER
vectorstore = FAISS.from_documents(chunks, embedding=embeddings)

total_vectors = vectorstore.index.ntotal
dimensions = vectorstore.index.d

print(f"There are {total_vectors} vectors with {dimensions:,} dimensions in the vector store")
225/30:
# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk
# Chroma is a popular open source Vector Database based on SQLLite

from langchain_ollama import OllamaEmbeddings

embeddings = OllamaEmbeddings(
    model="llama3.2",
)

# Create vectorstore

# BEFORE
# vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)

# AFTER
vectorstore = FAISS.from_documents(chunks, embedding=embeddings)

total_vectors = vectorstore.index.ntotal
dimensions = vectorstore.index.d

print(f"There are {total_vectors} vectors with {dimensions:,} dimensions in the vector store")
225/31:
# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk
# Chroma is a popular open source Vector Database based on SQLLite

from langchain_ollama import OllamaEmbeddings

embeddings = OllamaEmbeddings(
    model="llama3.2"
)

# Create vectorstore

# BEFORE
# vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)

# AFTER
vectorstore = FAISS.from_documents(chunks, embedding=embeddings)

total_vectors = vectorstore.index.ntotal
dimensions = vectorstore.index.d

print(f"There are {total_vectors} vectors with {dimensions:,} dimensions in the vector store")
225/32:
# imports

import os
import glob
from dotenv import load_dotenv
import gradio as gr
225/33:
# price is a factor for our company, so we're going to use a low cost model

MODEL = "gpt-4o-mini"
db_name = "vector_db"
225/34:
# Imports main tools:
from trulens.apps.langchain import TruChain
from trulens.core import TruSession

session = TruSession()
session.reset_database()
225/35:
# Imports from LangChain to build app
import bs4
from langchain import hub
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import WebBaseLoader
from langchain.schema import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
225/36:
# imports for langchain

from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
# from langchain_chroma import Chroma
from langchain.vectorstores import FAISS
import numpy as np
from sklearn.manifold import TSNE
import plotly.graph_objects as go
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
225/37:
# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
225/38:
# Read in documents using LangChain's loaders
# Take everything in all the sub-folders of our knowledgebase

folders = glob.glob("knowledge-base/*")

# With thanks to CG and Jon R, students on the course, for this fix needed for some users 
text_loader_kwargs = {'encoding': 'utf-8'}
# If that doesn't work, some Windows users might need to uncomment the next line instead
# text_loader_kwargs={'autodetect_encoding': True}

documents = []
for folder in folders:
    doc_type = os.path.basename(folder)
    loader = DirectoryLoader(folder, glob="**/*.md", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)
    folder_docs = loader.load()
    for doc in folder_docs:
        doc.metadata["doc_type"] = doc_type
        documents.append(doc)
225/39:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(documents)
225/40: len(chunks)
225/41:
doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)
print(f"Document types found: {', '.join(doc_types)}")
225/42:
# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk
# Chroma is a popular open source Vector Database based on SQLLite

from langchain_ollama import OllamaEmbeddings

embeddings = OllamaEmbeddings(
    model="llama3.2"
)

# Create vectorstore

# BEFORE
# vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)

# AFTER
vectorstore = FAISS.from_documents(chunks, embedding=embeddings)

total_vectors = vectorstore.index.ntotal
dimensions = vectorstore.index.d

print(f"There are {total_vectors} vectors with {dimensions:,} dimensions in the vector store")
225/43:
# imports

import os
import glob
from dotenv import load_dotenv
import gradio as gr
225/44:
# price is a factor for our company, so we're going to use a low cost model

MODEL = "gpt-4o-mini"
db_name = "vector_db"
225/45:
# Imports main tools:
from trulens.apps.langchain import TruChain
from trulens.core import TruSession

session = TruSession()
session.reset_database()
225/46:
# Imports from LangChain to build app
import bs4
from langchain import hub
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import WebBaseLoader
from langchain.schema import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
225/47:
# imports for langchain

from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
# from langchain_chroma import Chroma
from langchain.vectorstores import FAISS
import numpy as np
from sklearn.manifold import TSNE
import plotly.graph_objects as go
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
225/48:
# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
225/49:
# Read in documents using LangChain's loaders
# Take everything in all the sub-folders of our knowledgebase

folders = glob.glob("knowledge-base/*")

# With thanks to CG and Jon R, students on the course, for this fix needed for some users 
text_loader_kwargs = {'encoding': 'utf-8'}
# If that doesn't work, some Windows users might need to uncomment the next line instead
# text_loader_kwargs={'autodetect_encoding': True}

documents = []
for folder in folders:
    doc_type = os.path.basename(folder)
    loader = DirectoryLoader(folder, glob="**/*.md", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)
    folder_docs = loader.load()
    for doc in folder_docs:
        doc.metadata["doc_type"] = doc_type
        documents.append(doc)
225/50:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(documents)
225/51: len(chunks)
225/52:
doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)
print(f"Document types found: {', '.join(doc_types)}")
225/53:
# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk
# Chroma is a popular open source Vector Database based on SQLLite

from langchain_ollama import OllamaEmbeddings

embeddings = OllamaEmbeddings(
    model="llama3.2"
)

# Create vectorstore

# BEFORE
# vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)

# AFTER
vectorstore = FAISS.from_documents(chunks, embedding=embeddings)

total_vectors = vectorstore.index.ntotal
dimensions = vectorstore.index.d

print(f"There are {total_vectors} vectors with {dimensions:,} dimensions in the vector store")
230/1: !pip install ragas
230/2:
from ragas.dataset_schema import SingleTurnSample
from ragas.metrics import LLMContextRecall

sample = SingleTurnSample(
    user_input="Where is the Eiffel Tower located?",
    response="The Eiffel Tower is located in Paris.",
    reference="The Eiffel Tower is located in Paris.",
    retrieved_contexts=["Paris is the capital of France."], 
)

context_recall = LLMContextRecall()
await context_recall.single_turn_ascore(sample)
230/3:
from ragas.dataset_schema import SingleTurnSample
from ragas.metrics import NonLLMContextRecall

sample = SingleTurnSample(
    retrieved_contexts=["Paris is the capital of France."], 
    reference_contexts=["Paris is the capital of France.", "The Eiffel Tower is one of the most famous landmarks in Paris."]
)

context_recall = NonLLMContextRecall()
await context_recall.single_turn_ascore(sample)
230/4: !pip install rapidfuzz
230/5:
from ragas.dataset_schema import SingleTurnSample
from ragas.metrics import NonLLMContextRecall

sample = SingleTurnSample(
    retrieved_contexts=["Paris is the capital of France."], 
    reference_contexts=["Paris is the capital of France.", "The Eiffel Tower is one of the most famous landmarks in Paris."]
)

context_recall = NonLLMContextRecall()
await context_recall.single_turn_ascore(sample)
240/1:
from dotenv import load_dotenv
import os
240/2: os.environ['KMP_DUPLICATE_LTB_OK'] = 'True
240/3: os.environ['KMP_DUPLICATE_LTB_OK'] = 'True'
240/4: load_dotenv()
240/5: from langchain_ollama import ChatOllama
240/6:
from langchain_ollama import ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.tools import tool
240/7:
from langchain_ollama import ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.tools import tool
240/8:
llm =  ChatOllama(model="llama3.2",base_url='https://localhost:11434')
llm.invoke('hi')
240/9:
llm =  ChatOllama(model="llama3.2",base_url='http://localhost:11434')
llm.invoke('hi')
240/10: !pip install pymysql
240/11:
from langchain_community.utilities import SQLDatabase

db = SQLDatabase.from_uri("mysql+pymsql://root:root@localhost")
242/1:
import os
import glob
from dotenv import load_dotenv
242/2:
# imports for langchain

from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
# from langchain_chroma import Chroma
from langchain.vectorstores import FAISS
import numpy as np
from sklearn.manifold import TSNE
import plotly.graph_objects as go
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
242/3:
# imports for langchain, plotly and Chroma

from langchain.prompts import ChatPromptTemplate
from langchain_community.llms.ollama import Ollama
from langchain_community.vectorstores import Chroma
242/4:
MODEL = Ollama(model="llama3.2")
db_name = "vector-database"
242/5: folders = glob.glob("articles/*")
242/6:
import jsonlines
with jsonlines.open("articles.json","r") as documents:
    print(next(iter(documents)))
242/7: folders = glob.glob("articles/*")
242/8:
from langchain.document_loaders import PyPDFLoader
from langchain_core.documents import Document
folders = glob.glob("articles/*")
242/9:
langchain_documents = []

with jsonlines.open("articles.json", "r") as metadata_file:
    for file_path, metadata in zip(folders, metadata_file):
        try:
            pdf_loader = PyPDFLoader(file_path)
            pdf_documents = pdf_loader.load()

            for pdf_doc in pdf_documents:
            # Metadata'yÄ± dÃ¶kÃ¼mana ekle
                pdf_doc.metadata.update({
                    "url": metadata.get("url"),
                    "tags": metadata.get("tags"),
                    "title": metadata.get("title"),
                    "authors": metadata.get("authors")
                })

            # DÃ¶kÃ¼manÄ± listeye ekle
                langchain_documents.append( Document(
                        page_content=pdf_doc.page_content,
                        metadata=pdf_doc.metadata
                    )
                )
        except Exception as e:
            print(f"Error loading file {file_path}: {e}")
242/10: len(langchain_documents), langchain_documents[:1]
242/11:
from langchain.vectorstores import Qdrant
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
242/12:
from langchain_ollama import OllamaEmbeddings

embeddings = OllamaEmbeddings(
    model="llama3.2",
)
242/13:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(langchain_documents)
242/14:

vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
248/1:
# imports

import os
import glob
from dotenv import load_dotenv
import gradio as gr
248/2:
# price is a factor for our company, so we're going to use a low cost model

MODEL = "gpt-4o-mini"
db_name = "vector_db"
248/3:
# imports for langchain

from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
# from langchain_chroma import Chroma
from langchain.vectorstores import FAISS
import numpy as np
from sklearn.manifold import TSNE
import plotly.graph_objects as go
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
248/4:
# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
248/5:
# Read in documents using LangChain's loaders
# Take everything in all the sub-folders of our knowledgebase

folders = glob.glob("knowledge-base/*")

# With thanks to CG and Jon R, students on the course, for this fix needed for some users 
text_loader_kwargs = {'encoding': 'utf-8'}
# If that doesn't work, some Windows users might need to uncomment the next line instead
# text_loader_kwargs={'autodetect_encoding': True}

documents = []
for folder in folders:
    doc_type = os.path.basename(folder)
    loader = DirectoryLoader(folder, glob="**/*.md", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)
    folder_docs = loader.load()
    for doc in folder_docs:
        doc.metadata["doc_type"] = doc_type
        documents.append(doc)
248/6:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(documents)
248/7:
doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)
print(f"Document types found: {', '.join(doc_types)}")
248/8:
# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk
# Chroma is a popular open source Vector Database based on SQLLite

embeddings = OpenAIEmbeddings()

# Create vectorstore

# BEFORE
# vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)

# AFTER
vectorstore = FAISS.from_documents(chunks, embedding=embeddings)

total_vectors = vectorstore.index.ntotal
dimensions = vectorstore.index.d

print(f"There are {total_vectors} vectors with {dimensions:,} dimensions in the vector store")
248/9:
# Prework
vectors = []
documents = []
doc_types = []
colors = []
color_map = {'products':'blue', 'employees':'green', 'contracts':'red', 'company':'orange'}

for i in range(total_vectors):
    vectors.append(vectorstore.index.reconstruct(i))
    doc_id = vectorstore.index_to_docstore_id[i]
    document = vectorstore.docstore.search(doc_id)
    documents.append(document.page_content)
    doc_type = document.metadata['doc_type']
    doc_types.append(doc_type)
    colors.append(color_map[doc_type])
    
vectors = np.array(vectors)
248/10:
# We humans find it easier to visalize things in 2D!
# Reduce the dimensionality of the vectors to 2D using t-SNE
# (t-distributed stochastic neighbor embedding)

tsne = TSNE(n_components=2, random_state=42)
reduced_vectors = tsne.fit_transform(vectors)

# Create the 2D scatter plot
fig = go.Figure(data=[go.Scatter(
    x=reduced_vectors[:, 0],
    y=reduced_vectors[:, 1],
    mode='markers',
    marker=dict(size=5, color=colors, opacity=0.8),
    text=[f"Type: {t}<br>Text: {d[:100]}..." for t, d in zip(doc_types, documents)],
    hoverinfo='text'
)])

fig.update_layout(
    title='2D FAISS Vector Store Visualization',
    scene=dict(xaxis_title='x',yaxis_title='y'),
    width=800,
    height=600,
    margin=dict(r=20, b=10, l=10, t=40)
)

fig.show()
248/11:
# Let's try 3D!

tsne = TSNE(n_components=3, random_state=42)
reduced_vectors = tsne.fit_transform(vectors)

# Create the 3D scatter plot
fig = go.Figure(data=[go.Scatter3d(
    x=reduced_vectors[:, 0],
    y=reduced_vectors[:, 1],
    z=reduced_vectors[:, 2],
    mode='markers',
    marker=dict(size=5, color=colors, opacity=0.8),
    text=[f"Type: {t}<br>Text: {d[:100]}..." for t, d in zip(doc_types, documents)],
    hoverinfo='text'
)])

fig.update_layout(
    title='3D FAISS Vector Store Visualization',
    scene=dict(xaxis_title='x', yaxis_title='y', zaxis_title='z'),
    width=900,
    height=700,
    margin=dict(r=20, b=10, l=10, t=40)
)

fig.show()
248/12:
# Let's try 3D!

tsne = TSNE(n_components=3, random_state=42)
reduced_vectors = tsne.fit_transform(vectors)

# Create the 3D scatter plot
fig = go.Figure(data=[go.Scatter3d(
    x=reduced_vectors[:, 0],
    y=reduced_vectors[:, 1],
    z=reduced_vectors[:, 2],
    mode='markers',
    marker=dict(size=5, color=colors, opacity=0.8),
    text=[f"Type: {t}<br>Text: {d[:100]}..." for t, d in zip(doc_types, documents)],
    hoverinfo='text'
)])

fig.update_layout(
    title='3D FAISS Vector Store Visualization',
    scene=dict(xaxis_title='x', yaxis_title='y', zaxis_title='z'),
    width=900,
    height=700,
    margin=dict(r=20, b=10, l=10, t=40)
)

fig.show()
248/13:
# create a new Chat with OpenAI
llm = ChatOpenAI(temperature=0.7, model_name=MODEL)

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)
248/14:
query = "Who created thee insureLL"
result = conversation_chain.invoke({"question":query})
print(result["answer"])
248/15:
query = "Who created thee insureLLm"
result = conversation_chain.invoke({"question":query})
print(result["answer"])
248/16:
import numpy as np
from trulens.core import Feedback
from trulens.providers.openai import OpenAI

# Initialize provider class
provider = OpenAI()

# select context to be used in feedback. the location of context is app specific.
context = TruChain.select_context(rag_chain)

# Define a groundedness feedback function
f_groundedness = (
    Feedback(
        provider.groundedness_measure_with_cot_reasons, name="Groundedness"
    )
    .on(context.collect())  # collect context chunks into a list
    .on_output()
)

# Question/answer relevance between overall question and answer.
f_answer_relevance = Feedback(
    provider.relevance_with_cot_reasons, name="Answer Relevance"
).on_input_output()
# Context relevance between question and each context chunk.
f_context_relevance = (
    Feedback(
        provider.context_relevance_with_cot_reasons, name="Context Relevance"
    )
    .on_input()
    .on(context)
    .aggregate(np.mean)
)
248/17:
# Wrapping that in a function

def chat(message, history):
    result = conversation_chain.invoke({"question": message})
    return result["answer"]
248/18:
# And in Gradio:

view = gr.ChatInterface(chat).launch()
244/1:
import numpy as np
from trulens.core import Feedback
from trulens.providers.openai import OpenAI

# Initialize provider class
provider = OpenAI()

# select context to be used in feedback. the location of context is app specific.
context = TruChain.select_context(rag_chain)

# Define a groundedness feedback function
f_groundedness = (
    Feedback(
        provider.groundedness_measure_with_cot_reasons, name="Groundedness"
    )
    .on(context.collect())  # collect context chunks into a list
    .on_output()
)

# Question/answer relevance between overall question and answer.
f_answer_relevance = Feedback(
    provider.relevance_with_cot_reasons, name="Answer Relevance"
).on_input_output()
# Context relevance between question and each context chunk.
f_context_relevance = (
    Feedback(
        provider.context_relevance_with_cot_reasons, name="Context Relevance"
    )
    .on_input()
    .on(context)
    .aggregate(np.mean)
)
244/2:
# imports

import os
import glob
from dotenv import load_dotenv
import gradio as gr
244/3:
# price is a factor for our company, so we're going to use a low cost model

MODEL = "gpt-4o-mini"
db_name = "vector_db"
244/4:
# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
244/5:
# Imports main tools:
from trulens.apps.langchain import TruChain
from trulens.core import TruSession

session = TruSession()
session.reset_database()
244/6:
# Imports from LangChain to build app
import bs4
from langchain import hub
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import WebBaseLoader
from langchain.schema import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
244/7:
loader = WebBaseLoader(
    web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/",),
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(
            class_=("post-content", "post-title", "post-header")
        )
    ),
)
docs = loader.load()
244/8:
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

embeddings = OpenAIEmbeddings()


text_splitter = RecursiveCharacterTextSplitter()
documents = text_splitter.split_documents(docs)
vectorstore = FAISS.from_documents(documents, embeddings)
244/9:
from langchain_community.llms.ollama import Ollama
retriever = vectorstore.as_retriever()

prompt = hub.pull("rlm/rag-prompt")
llm = Ollama(model="llama3.2")


def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)
244/10:
# imports

import os
import glob
from dotenv import load_dotenv
import gradio as gr
244/11:
# price is a factor for our company, so we're going to use a low cost model

MODEL = "gpt-4o-mini"
db_name = "vector_db"
244/12:
# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
244/13:
# Imports main tools:
from trulens.apps.langchain import TruChain
from trulens.core import TruSession

session = TruSession()
session.reset_database()
244/14:
# Imports from LangChain to build app
import bs4
from langchain import hub
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import WebBaseLoader
from langchain.schema import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
244/15:
loader = WebBaseLoader(
    web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/",),
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(
            class_=("post-content", "post-title", "post-header")
        )
    ),
)
docs = loader.load()
244/16:
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

embeddings = OpenAIEmbeddings()


text_splitter = RecursiveCharacterTextSplitter()
documents = text_splitter.split_documents(docs)
vectorstore = FAISS.from_documents(documents, embeddings)
244/17:
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

embeddings = OpenAIEmbeddings()


text_splitter = RecursiveCharacterTextSplitter()
documents = text_splitter.split_documents(docs)
vectorstore = FAISS.from_documents(documents, embeddings)
244/18:
from langchain_community.llms.ollama import Ollama
retriever = vectorstore.as_retriever()

prompt = hub.pull("rlm/rag-prompt")
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)


def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)
244/19: rag_chain.invoke("What is Task Decomposition?")
244/20:
# imports

import os
import glob
from dotenv import load_dotenv
import gradio as gr
244/21:
# price is a factor for our company, so we're going to use a low cost model

MODEL = "gpt-4o-mini"
db_name = "vector_db"
244/22:
# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
244/23:
# Imports main tools:
from trulens.apps.langchain import TruChain
from trulens.core import TruSession

session = TruSession()
session.reset_database()
244/24:
# Imports from LangChain to build app
import bs4
from langchain import hub
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import WebBaseLoader
from langchain.schema import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
244/25:
loader = WebBaseLoader(
    web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/",),
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(
            class_=("post-content", "post-title", "post-header")
        )
    ),
)
docs = loader.load()
244/26:
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

embeddings = OpenAIEmbeddings()


text_splitter = RecursiveCharacterTextSplitter()
documents = text_splitter.split_documents(docs)
vectorstore = FAISS.from_documents(documents, embeddings)
244/27:
from langchain_community.llms.ollama import Ollama
retriever = vectorstore.as_retriever()

prompt = hub.pull("rlm/rag-prompt")
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)


def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)
244/28: rag_chain.invoke("What is Task Decomposition?")
244/29:
import numpy as np
from trulens.core import Feedback
from trulens.providers.openai import OpenAI

# Initialize provider class
provider = OpenAI()

# select context to be used in feedback. the location of context is app specific.
context = TruChain.select_context(rag_chain)

# Define a groundedness feedback function
f_groundedness = (
    Feedback(
        provider.groundedness_measure_with_cot_reasons, name="Groundedness"
    )
    .on(context.collect())  # collect context chunks into a list
    .on_output()
)

# Question/answer relevance between overall question and answer.
f_answer_relevance = Feedback(
    provider.relevance_with_cot_reasons, name="Answer Relevance"
).on_input_output()
# Context relevance between question and each context chunk.
f_context_relevance = (
    Feedback(
        provider.context_relevance_with_cot_reasons, name="Context Relevance"
    )
    .on_input()
    .on(context)
    .aggregate(np.mean)
)
259/1:
llm =  ChatOllama(model="llama3.2",base_url='http://localhost:11434')
llm.invoke('hi')[0]
259/2:
from dotenv import load_dotenv
import os
259/3: os.environ['KMP_DUPLICATE_LTB_OK'] = 'True'
259/4: load_dotenv()
259/5:
from langchain_ollama import ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.tools import tool
259/6:
llm =  ChatOllama(model="llama3.2",base_url='http://localhost:11434')
llm.invoke('hi')[0]
259/7:
llm =  ChatOllama(model="llama3.2",base_url='http://localhost:11434')
type(llm.invoke('hi'))
259/8:
llm =  ChatOllama(model="llama3.2",base_url='http://localhost:11434')
llm.invoke('hi').parse_raw
259/9:
llm =  ChatOllama(model="llama3.2",base_url='http://localhost:11434')
llm.invoke('hi').parse_raw[0]
262/1:
# imports

import os
from dotenv import load_dotenv
from openai import OpenAI
import gradio as gr
262/2:
# Load environment variables in a file called .env
# Print the key prefixes to help with any debugging

load_dotenv()
openai_api_key = os.getenv('OPENAI_API_KEY')
anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')
google_api_key = os.getenv('GOOGLE_API_KEY')

if openai_api_key:
    print(f"OpenAI API Key exists and begins {openai_api_key[:8]}")
else:
    print("OpenAI API Key not set")
    
if anthropic_api_key:
    print(f"Anthropic API Key exists and begins {anthropic_api_key[:7]}")
else:
    print("Anthropic API Key not set")

if google_api_key:
    print(f"Google API Key exists and begins {google_api_key[:8]}")
else:
    print("Google API Key not set")
262/3:
# Initialize

openai = OpenAI()
MODEL = 'gpt-4o-mini'
262/4: system_message = "You are a helpful assistant"
262/5:
# Simpler than in my video - we can easily create this function that calls OpenAI
# It's now just 1 line of code to prepare the input to OpenAI!

def chat(message, history):
    messages = [{"role": "system", "content": system_message}] + history + [{"role": "user", "content": message}]

    print("History is:")
    print(history)
    print("And messages is:")
    print(messages)

    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)

    response = ""
    for chunk in stream:
        response += chunk.choices[0].delta.content or ''
        yield response
262/6:
# Simpler than in my video - we can easily create this function that calls OpenAI
# It's now just 1 line of code to prepare the input to OpenAI!

def chat(message, history):
    messages = [{"role": "system", "content": system_message}] + history + [{"role": "user", "content": message}]

    print("History is:")
    print(history)
    print("And messages is:")
    print(messages)

    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)

    response = ""
    for chunk in stream:
        response += chunk.choices[0].delta.content or ''
        yield response
262/7: gr.ChatInterface(fn=chat, type="messages").launch()
262/8:
system_message = "You are a helpful assistant in a clothes store. You should try to gently encourage \
the customer to try items that are on sale. Hats are 60% off, and most other items are 50% off. \
For example, if the customer says 'I'm looking to buy a hat', \
you could reply something like, 'Wonderful - we have lots of hats - including several that are part of our sales event.'\
Encourage the customer to buy hats if they are unsure what to get."
262/9:
def chat(message, history):
    messages = [{"role": "system", "content": system_message}] + history + [{"role": "user", "content": message}]

    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)

    response = ""
    for chunk in stream:
        response += chunk.choices[0].delta.content or ''
        yield response
262/10: gr.ChatInterface(fn=chat, type="messages").launch()
262/11:
system_message += "\nIf the customer asks for shoes, you should respond that shoes are not on sale today, \
but remind the customer to look at hats!"
262/12: gr.ChatInterface(fn=chat, type="messages").launch()
262/13:
def chat(message, history):
    messages = [{"role": "system", "content": system_message}] + history + [{"role": "user", "content": message}]

    if 'belt' in message:
        messages.append({"role": "system", "content": "For added context, the store does not sell belts, \
but be sure to point out other items on sale"})
    
    messages.append({"role": "user", "content": message})

    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)

    response = ""
    for chunk in stream:
        response += chunk.choices[0].delta.content or ''
        yield response
262/14: gr.ChatInterface(fn=chat, type="messages").launch()
263/1:
# Let's start by making a useful function

ticket_prices = {"london": "$799", "paris": "$899", "tokyo": "$1400", "berlin": "$499"}

def get_ticket_price(destination_city):
    print(f"Tool get_ticket_price called for {destination_city}")
    city = destination_city.lower()
    return ticket_prices.get(city, "Unknown")
263/2: get_ticket_price("Berlin")
263/3:
# imports

import os
import json
from dotenv import load_dotenv
from openai import OpenAI
import gradio as gr
263/4:
# Initialization

load_dotenv()

openai_api_key = os.getenv('OPENAI_API_KEY')
if openai_api_key:
    print(f"OpenAI API Key exists and begins {openai_api_key[:8]}")
else:
    print("OpenAI API Key not set")
    
MODEL = "gpt-4o-mini"
openai = OpenAI()
263/5:
system_message = "You are a helpful assistant for an Airline called FlightAI. "
system_message += "Give short, courteous answers, no more than 1 sentence. "
system_message += "Always be accurate. If you don't know the answer, say so."
263/6:
# This function looks rather simpler than the one from my video, because we're taking advantage of the latest Gradio updates

def chat(message, history):
    messages = [{"role": "system", "content": system_message}] + history + [{"role": "user", "content": message}]
    response = openai.chat.completions.create(model=MODEL, messages=messages)
    return response.choices[0].message.content

gr.ChatInterface(fn=chat, type="messages").launch()
263/7:
# Let's start by making a useful function

ticket_prices = {"london": "$799", "paris": "$899", "tokyo": "$1400", "berlin": "$499"}

def get_ticket_price(destination_city):
    print(f"Tool get_ticket_price called for {destination_city}")
    city = destination_city.lower()
    return ticket_prices.get(city, "Unknown")
263/8: get_ticket_price("Berlin")
263/9:
# There's a particular dictionary structure that's required to describe our function:

price_function = {
    "name": "get_ticket_price",
    "description": "Get the price of a return ticket to the destination city. Call this whenever you need to know the ticket price, for example when a customer asks 'How much is a ticket to this city'",
    "parameters": {
        "type": "object",
        "properties": {
            "destination_city": {
                "type": "string",
                "description": "The city that the customer wants to travel to",
            },
        },
        "required": ["destination_city"],
        "additionalProperties": False
    }
}
263/10:
# And this is included in a list of tools:

tools = [{"type": "function", "function": price_function}]
263/11:
def chat(message, history):
    messages = [{"role": "system", "content": system_message}] + history + [{"role": "user", "content": message}]
    response = openai.chat.completions.create(model=MODEL, messages=messages, tools=tools)

    if response.choices[0].finish_reason=="tool_calls":
        message = response.choices[0].message
        response, city = handle_tool_call(message)
        messages.append(message)
        messages.append(response)
        response = openai.chat.completions.create(model=MODEL, messages=messages)
    
    return response.choices[0].message.content
263/12:
# We have to write that function handle_tool_call:

def handle_tool_call(message):
    tool_call = message.tool_calls[0]
    arguments = json.loads(tool_call.function.arguments)
    city = arguments.get('destination_city')
    price = get_ticket_price(city)
    response = {
        "role": "tool",
        "content": json.dumps({"destination_city": city,"price": price}),
        "tool_call_id": message.tool_calls[0].id
    }
    return response, city
263/13:
# We have to write that function handle_tool_call:

def handle_tool_call(message):
    tool_call = message.tool_calls[0]
    arguments = json.loads(tool_call.function.arguments)
    city = arguments.get('destination_city')
    price = get_ticket_price(city)
    response = {
        "role": "tool",
        "content": json.dumps({"destination_city": city,"price": price}),
        "tool_call_id": message.tool_calls[0].id
    }
    return response, city
263/14: gr.ChatInterface(fn=chat, type="messages").launch()
264/1:
# imports

import os
import json
from dotenv import load_dotenv
from openai import OpenAI
import gradio as gr
264/2:
# Initialization

load_dotenv()

openai_api_key = os.getenv('OPENAI_API_KEY')
if openai_api_key:
    print(f"OpenAI API Key exists and begins {openai_api_key[:8]}")
else:
    print("OpenAI API Key not set")
    
MODEL = "gpt-4o-mini"
openai = OpenAI()
264/3:
system_message = "You are a helpful assistant for an Airline called FlightAI. "
system_message += "Give short, courteous answers, no more than 1 sentence. "
system_message += "Always be accurate. If you don't know the answer, say so."
264/4:
# This function looks rather simpler than the one from my video, because we're taking advantage of the latest Gradio updates

def chat(message, history):
    messages = [{"role": "system", "content": system_message}] + history + [{"role": "user", "content": message}]
    response = openai.chat.completions.create(model=MODEL, messages=messages)
    return response.choices[0].message.content

gr.ChatInterface(fn=chat, type="messages").launch()
264/5:
# Let's start by making a useful function

ticket_prices = {"london": "$799", "paris": "$899", "tokyo": "$1400", "berlin": "$499"}

def get_ticket_price(destination_city):
    print(f"Tool get_ticket_price called for {destination_city}")
    city = destination_city.lower()
    return ticket_prices.get(city, "Unknown")
264/6: get_ticket_price("London")
264/7:
# There's a particular dictionary structure that's required to describe our function:

price_function = {
    "name": "get_ticket_price",
    "description": "Get the price of a return ticket to the destination city. Call this whenever you need to know the ticket price, for example when a customer asks 'How much is a ticket to this city'",
    "parameters": {
        "type": "object",
        "properties": {
            "destination_city": {
                "type": "string",
                "description": "The city that the customer wants to travel to",
            },
        },
        "required": ["destination_city"],
        "additionalProperties": False
    }
}
264/8:
# And this is included in a list of tools:

tools = [{"type": "function", "function": price_function}]
264/9:
def chat(message, history):
    messages = [{"role": "system", "content": system_message}] + history + [{"role": "user", "content": message}]
    response = openai.chat.completions.create(model=MODEL, messages=messages, tools=tools)

    if response.choices[0].finish_reason=="tool_calls":
        message = response.choices[0].message
        response, city = handle_tool_call(message)
        messages.append(message)
        messages.append(response)
        response = openai.chat.completions.create(model=MODEL, messages=messages)
    
    return response.choices[0].message.content
264/10:
# We have to write that function handle_tool_call:

def handle_tool_call(message):
    tool_call = message.tool_calls[0]
    arguments = json.loads(tool_call.function.arguments)
    city = arguments.get('destination_city')
    price = get_ticket_price(city)
    response = {
        "role": "tool",
        "content": json.dumps({"destination_city": city,"price": price}),
        "tool_call_id": message.tool_calls[0].id
    }
    return response, city
264/11: gr.ChatInterface(fn=chat, type="messages").launch()
264/12:
# Some imports for handling images

import base64
from io import BytesIO
from PIL import Image
264/13:
def artist(city):
    image_response = openai.images.generate(
            model="dall-e-3",
            prompt=f"An image representing a vacation in {city}, showing tourist spots and everything unique about {city}, in a vibrant pop-art style",
            size="1024x1024",
            n=1,
            response_format="b64_json",
        )
    image_base64 = image_response.data[0].b64_json
    image_data = base64.b64decode(image_base64)
    return Image.open(BytesIO(image_data))
264/14:
image = artist("New York City")
display(image)
264/15: !ffmpeg -version
264/16:
from pydub import AudioSegment
from pydub.playback import play

def talker(message):
    response = openai.audio.speech.create(
      model="tts-1",
      voice="onyx",    # Also, try replacing onyx with alloy
      input=message
    )
    
    audio_stream = BytesIO(response.content)
    audio = AudioSegment.from_file(audio_stream, format="mp3")
    play(audio)
264/17: talker("Well, hi there")
264/18: talker("How are you")
264/19: talker("The person you have called can not reach at the moment, please try again later")
264/20:
from pydub import AudioSegment
from pydub.playback import play

def talker(message):
    response = openai.audio.speech.create(
      model="tts-1",
      voice="alloy",    # Also, try replacing onyx with alloy
      input=message
    )
    
    audio_stream = BytesIO(response.content)
    audio = AudioSegment.from_file(audio_stream, format="mp3")
    play(audi)
264/21: talker("The person you have called can not reach at the moment, please try again later")
264/22:
from pydub import AudioSegment
from pydub.playback import play

def talker(message):
    response = openai.audio.speech.create(
      model="tts-1",
      voice="alloy",    # Also, try replacing onyx with alloy
      input=message
    )
    
    audio_stream = BytesIO(response.content)
    audio = AudioSegment.from_file(audio_stream, format="mp3")
    play(audio)
264/23: talker("The person you have called can not reach at the moment, please try again later")
264/24:
def chat(history):
    messages = [{"role": "system", "content": system_message}] + history
    response = openai.chat.completions.create(model=MODEL, messages=messages, tools=tools)
    image = None
    
    if response.choices[0].finish_reason=="tool_calls":
        message = response.choices[0].message
        response, city = handle_tool_call(message)
        messages.append(message)
        messages.append(response)
        image = artist(city)
        response = openai.chat.completions.create(model=MODEL, messages=messages)
        
    reply = response.choices[0].message.content
    history += [{"role":"assistant", "content":reply}]

    # Comment out or delete the next line if you'd rather skip Audio for now..
    talker(reply)
    
    return history, image
264/25:
# More involved Gradio code as we're not using the preset Chat interface!
# Passing in inbrowser=True in the last line will cause a Gradio window to pop up immediately.

with gr.Blocks() as ui:
    with gr.Row():
        chatbot = gr.Chatbot(height=500, type="messages")
        image_output = gr.Image(height=500)
    with gr.Row():
        entry = gr.Textbox(label="Chat with our AI Assistant:")
    with gr.Row():
        clear = gr.Button("Clear")

    def do_entry(message, history):
        history += [{"role":"user", "content":message}]
        return "", history

    entry.submit(do_entry, inputs=[entry, chatbot], outputs=[entry, chatbot]).then(
        chat, inputs=chatbot, outputs=[chatbot, image_output]
    )
    clear.click(lambda: None, inputs=None, outputs=chatbot, queue=False)

ui.launch(inbrowser=True)
267/1:
# imports

from dotenv import load_dotenv
from IPython.display import Markdown, display, update_display
from openai import OpenAI
import ollama
267/2:
# constants

MODEL_GPT = 'gpt-4o-mini'
MODEL_LLAMA = 'llama3.2'
267/3:
# set up environment

load_dotenv()
openai = OpenAI()
267/4:
# here is the question; type over this to ask something new

question = """
Please explain what this code does and why:
yield from {book.get("author") for book in books if book.get("author")}
"""
267/5:
# prompts

system_prompt = "You are a helpful technical tutor who answers questions about python code, software engineering, data science and LLMs"
user_prompt = "Please give a detailed explanation to the following question: " + question
267/6:
# messages

messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": user_prompt}
]
267/7:
# Get gpt-4o-mini to answer, with streaming

stream = openai.chat.completions.create(model=MODEL_GPT, messages=messages,stream=True)
    
response = ""
display_handle = display(Markdown(""), display_id=True)
for chunk in stream:
    response += chunk.choices[0].delta.content or ''
    response = response.replace("```","").replace("markdown", "")
    update_display(Markdown(response), display_id=display_handle.display_id)
267/8:
# Get Llama 3.2 to answer

response = ollama.chat(model=MODEL_LLAMA, messages=messages)
reply = response['message']['content']
display(Markdown(reply))
263/15:
# We have to write that function handle_tool_call:

def handle_tool_call(message):
    tool_call = message.tool_calls[0]
    arguments = json.loads(tool_call.function.arguments)
    city = arguments.get('destination_city')
    price = get_ticket_price(city)
    response = {
        "role": "tool",
        "content": json.dumps({"destination_city": city,"price": price}),
        "tool_call_id": message.tool_calls[0].id
    }
    print(response["tool_call_id"]
    return response, city
263/16:
# We have to write that function handle_tool_call:

def handle_tool_call(message):
    tool_call = message.tool_calls[0]
    arguments = json.loads(tool_call.function.arguments)
    city = arguments.get('destination_city')
    price = get_ticket_price(city)
    response = {
        "role": "tool",
        "content": json.dumps({"destination_city": city,"price": price}),
        "tool_call_id": message.tool_calls[0].id
    }
    print(response["tool_call_id"])
    return response, city
263/17: gr.ChatInterface(fn=chat, type="messages").launch()
263/18:

chat("Hi")
#gr.ChatInterface(fn=chat, type="messages").launch()
263/19:

chat("Hi",[])
#gr.ChatInterface(fn=chat, type="messages").launch()
263/20:

chat("Hi",[]).tool_call
#gr.ChatInterface(fn=chat, type="messages").launch()
263/21:

handle_tool_call("Hello")
#gr.ChatInterface(fn=chat, type="messages").launch()
263/22:
# We have to write that function handle_tool_call:

def handle_tool_call(message):
    tool_call = message.tool_calls[0]
    arguments = json.loads(tool_call.function.arguments)
    city = arguments.get('destination_city')
    price = get_ticket_price(city)
    response = {
        "role": "tool",
        "content": json.dumps({"destination_city": city,"price": price}),
        "tool_call_id": message.tool_calls[0].id
    }
    return response, city
263/23:

handle_tool_call("Hello")
#gr.ChatInterface(fn=chat, type="messages").launch()
263/24:
# We have to write that function handle_tool_call:

def handle_tool_call(message):
    tool_call = message.tool_calls[0]
    arguments = json.loads(tool_call.function.arguments)
    city = arguments.get('destination_city')
    price = get_ticket_price(city)
    response = {
        "role": "tool",
        "content": json.dumps({"destination_city": city,"price": price}),
        "tool_call_id": message.tool_calls[0].id
    }
    return response, city
263/25:

handle_tool_call("Hello")
#gr.ChatInterface(fn=chat, type="messages").launch()
263/26:
# We have to write that function handle_tool_call:

def handle_tool_call(message):
    tool_call = message.tool_calls[0]
    arguments = json.loads(tool_call.function.arguments)
    city = arguments.get('destination_city')
    price = get_ticket_price(city)
    response = {
        "role": "tool",
        "content": json.dumps({"destination_city": city,"price": price}),
        "tool_call_id": message.tool_calls[0].id
    }
    price_function(message.tool_calls[0].id
263/27:
# We have to write that function handle_tool_call:

def handle_tool_call(message):
    tool_call = message.tool_calls[0]
    arguments = json.loads(tool_call.function.arguments)
    city = arguments.get('destination_city')
    price = get_ticket_price(city)
    response = {
        "role": "tool",
        "content": json.dumps({"destination_city": city,"price": price}),
        "tool_call_id": message.tool_calls[0].id
    }
   print(price_function(message.tool_calls[0].id)
263/29:
# We have to write that function handle_tool_call:

def handle_tool_call(message):
    tool_call = message.tool_calls[0]
    arguments = json.loads(tool_call.function.arguments)
    city = arguments.get('destination_city')
    price = get_ticket_price(city)
    response = {
        "role": "tool",
        "content": json.dumps({"destination_city": city,"price": price}),
        "tool_call_id": message.tool_calls[0].id
    }
    print(price_function(message.tool_calls[0].id)
263/30:
# We have to write that function handle_tool_call:

def handle_tool_call(message):
    tool_call = message.tool_calls[0]
    arguments = json.loads(tool_call.function.arguments)
    city = arguments.get('destination_city')
    price = get_ticket_price(city)
    response = {
        "role": "tool",
        "content": json.dumps({"destination_city": city,"price": price}),
        "tool_call_id": message.tool_calls[0].id
    }
    print(price_function(message.tool_calls[0].id))
263/31:
# We have to write that function handle_tool_call:

def handle_tool_call(message):
    tool_call = message.tool_calls[0]
    arguments = json.loads(tool_call.function.arguments)
    city = arguments.get('destination_city')
    price = get_ticket_price(city)
    response = {
        "role": "tool",
        "content": json.dumps({"destination_city": city,"price": price}),
        "tool_call_id": message.tool_calls[0].id
    }
    print(message.tool_calls[0].id)
263/32: handle_tool_call("Hello")
263/33:
# We have to write that function handle_tool_call:

def handle_tool_call(message):
    tool_call = message.tool_calls[0]
    arguments = json.loads(tool_call.function.arguments)
    city = arguments.get('destination_city')
    price = get_ticket_price(city)
    response = {
        "role": "tool",
        "content": json.dumps({"destination_city": city,"price": price}),
        "tool_call_id": message.tool_calls[0].id
    }
    return response,city
263/34:

gr.ChatInterface(fn=chat, type="messages").launch()
263/35:

Agencies = {"maras hotel":"4.9/5.0", "izmir hotel ":"4.5/5.0", "ankara hotel": "4.6/5.0"}
def get_hotel_like_rate(hotel_to_stay):
    print(f"Tool get_hotel_like_rate called for {hotel_to_stay}")
    hotel = hotel_to_stay.lower()
    return Agencies.get(hotel,"Unknown")
263/36: get_hotel_like_rate("maras hotel")
263/37:
def chat(message, history):
    messages = [{"role": "system", "content": system_message}] + history + [{"role": "user", "content": message}]
    response = openai.chat.completions.create(model=MODEL, messages=messages, tools=tools)

    if response.choices[0].finish_reason=="tool_calls":
        message = response.choices[0].message
        response, city = handle_tool_call(message)
        messages.append(message)
        messages.append(response)
        response = openai.chat.completions.create(model=MODEL, messages=messages)
    
    return response.choices[0].message.content
263/38:
# And this is included in a list of tools:

tools = [{"type": "function", "function": price_function},{"type":"function", "function": get_hotel_like_rate}]
263/39:
def chat(message, history):
    messages = [{"role": "system", "content": system_message}] + history + [{"role": "user", "content": message}]
    response = openai.chat.completions.create(model=MODEL, messages=messages, tools=tools)

    if response.choices[0].finish_reason=="tool_calls":
        message = response.choices[0].message
        response, city = handle_tool_call(message)
        messages.append(message)
        messages.append(response)
        response = openai.chat.completions.create(model=MODEL, messages=messages)
    
    return response.choices[0].message.content
263/40:
# There's a particular dictionary structure that's required to describe our function:

price_function = {
    "name": "get_ticket_price",
    "description": "Get the price of a return ticket to the destination city. Call this whenever you need to know the ticket price, for example when a customer asks 'How much is a ticket to this city'",
    "parameters": {
        "type": "object",
        "properties": {
            "destination_city": {
                "type": "string",
                "description": "The city that the customer wants to travel to",
            },
        },
        "required": ["destination_city"],
        "additionalProperties": False
    }
}
263/41:
hotel_function = {
    "name":get_hotel_like_rate,
    "description":"Get the rate of a return hotel to destination city. Assume that all cities have same hotels. Call this whenevere you need to know the hotel like rate, for example when a customer asks ' What is the rates of ' ",
    "parameters": {
        "type": "object",
        "properties": {
        "hotel name": {
            "type":"string",
            "description": "The hotel to customer wants to stay",
        },
    },
    "required": ["hotel_name"],
    "additionalProperties": False
    }
}
263/42: get_hotel_like_rate("maras hotel")
263/43:
# And this is included in a list of tools:

tools = [{"type": "function", "function": price_function},{"type":"function", "function": get_hotel_like_rate}]
263/44:
# We have to write that function handle_tool_call:

def handle_tool_call(message):
    tool_call = message.tool_calls[0]
    arguments = json.loads(tool_call.function.arguments)
    city = arguments.get('destination_city')
    price = get_ticket_price(city)
    hotel = arguments.get('hotel_to_stay')
    rate = get_hotel_like_rate(hotel)
    response = {
        "role": "tool",
        "content": json.dumps({"destination_city": city,"price": price}),
        "tool_call_id": message.tool_calls[0].id
    }
    return response,city
278/1:
# imports

import os
import glob
from dotenv import load_dotenv
import gradio as gr
278/2:
# imports for langchain

from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
import numpy as np
from sklearn.manifold import TSNE
import plotly.graph_objects as go
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
278/3:
# price is a factor for our company, so we're going to use a low cost model

MODEL = "gpt-4o-mini"
db_name = "vector_db"
278/4:
# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
278/5:
# Read in documents using LangChain's loaders
# Take everything in all the sub-folders of our knowledgebase

folders = glob.glob("knowledge-base/*")

# With thanks to CG and Jon R, students on the course, for this fix needed for some users 
text_loader_kwargs = {'encoding': 'utf-8'}
# If that doesn't work, some Windows users might need to uncomment the next line instead
# text_loader_kwargs={'autodetect_encoding': True}

documents = []
for folder in folders:
    doc_type = os.path.basename(folder)
    loader = DirectoryLoader(folder, glob="**/*.md", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)
    folder_docs = loader.load()
    for doc in folder_docs:
        doc.metadata["doc_type"] = doc_type
        documents.append(doc)
278/6:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(documents)
276/1:
import os
import glob
from dotenv import load_dotenv
276/2:
# imports for langchain

from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
# from langchain_chroma import Chroma
from langchain.vectorstores import FAISS
import numpy as np
from sklearn.manifold import TSNE
import plotly.graph_objects as go
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
276/3:
# imports for langchain, plotly and Chroma

from langchain.prompts import ChatPromptTemplate
from langchain_community.llms.ollama import Ollama
from langchain_community.vectorstores import Chroma
276/4:
MODEL = Ollama(model="llama3.2")
db_name = "vector-database"
276/5: folders = glob.glob("articles/*")
276/6: text_loader_kwargs = {'encoding': 'utf-8'}
276/7:
import jsonlines
with jsonlines.open("articles.json","r") as documents:
    print(next(iter(documents)))
276/8:
from langchain.document_loaders import PyPDFLoader
from langchain_core.documents import Document
folders = glob.glob("articles/*")
276/9:
langchain_documents = []

with jsonlines.open("articles.json", "r") as metadata_file:
    for file_path, metadata in zip(folders, metadata_file):
        try:
            pdf_loader = PyPDFLoader(file_path)
            pdf_documents = pdf_loader.load()

            for pdf_doc in pdf_documents:
            # Metadata'yÄ± dÃ¶kÃ¼mana ekle
                pdf_doc.metadata.update({
                    "url": metadata.get("url"),
                    "tags": metadata.get("tags"),
                    "title": metadata.get("title"),
                    "authors": metadata.get("authors")
                })

            # DÃ¶kÃ¼manÄ± listeye ekle
                langchain_documents.append( Document(
                        page_content=pdf_doc.page_content,
                        metadata=pdf_doc.metadata
                    )
                )
        except Exception as e:
            print(f"Error loading file {file_path}: {e}")
276/10: len(langchain_documents), langchain_documents[:1]
276/11:
from langchain.vectorstores import Qdrant
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
276/12:
from langchain_ollama import OllamaEmbeddings

embeddings = OllamaEmbeddings(
    model="llama3.2",
)
276/13:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(langchain_documents)
276/14:
if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)

#vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
276/15:
MODEL = Ollama(model="llama3.2")
db_name = "vector-database"
276/16: folders = glob.glob("example/*")
276/17: text_loader_kwargs = {'encoding': 'utf-8'}
276/18:
import jsonlines
with jsonlines.open("articles.json","r") as documents:
    print(next(iter(documents)))
276/19:
langchain_documents = []

with jsonlines.open("articles.json", "r") as metadata_file:
    for file_path, metadata in zip(folders, metadata_file):
        try:
            pdf_loader = PyPDFLoader(file_path)
            pdf_documents = pdf_loader.load()

            for pdf_doc in pdf_documents:
            # Metadata'yÄ± dÃ¶kÃ¼mana ekle
                pdf_doc.metadata.update({
                    "url": metadata.get("url"),
                    "tags": metadata.get("tags"),
                    "title": metadata.get("title"),
                    "authors": metadata.get("authors")
                })

            # DÃ¶kÃ¼manÄ± listeye ekle
                langchain_documents.append( Document(
                        page_content=pdf_doc.page_content,
                        metadata=pdf_doc.metadata
                    )
                )
        except Exception as e:
            print(f"Error loading file {file_path}: {e}")
276/20: len(langchain_documents), langchain_documents[:1]
276/21:
from langchain.vectorstores import Qdrant
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
276/22:
from langchain_ollama import OllamaEmbeddings

embeddings = OllamaEmbeddings(
    model="llama3.2",
)
276/23:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(langchain_documents)
276/24:
if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)

#vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
276/25:
#MODEL = Ollama(model="llama3.2")
MODEL = OpenAI()
db_name = "vector-database"
276/26:
import os
import glob
from dotenv import load_dotenv
276/27:
# imports for langchain

from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
# from langchain_chroma import Chroma
from langchain.vectorstores import FAISS
import numpy as np
from sklearn.manifold import TSNE
import plotly.graph_objects as go
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
276/28:
# imports for langchain, plotly and Chroma

from langchain.prompts import ChatPromptTemplate
from langchain_community.llms.ollama import Ollama
from langchain_community.vectorstores import Chroma
276/29:
#MODEL = Ollama(model="llama3.2")
MODEL = "gpt-4o-mini"
db_name = "vector-database"
276/30: folders = glob.glob("example/*")
276/31: text_loader_kwargs = {'encoding': 'utf-8'}
276/32:
import jsonlines
with jsonlines.open("articles.json","r") as documents:
    print(next(iter(documents)))
276/33:
from langchain.document_loaders import PyPDFLoader
from langchain_core.documents import Document
folders = glob.glob("articles/*")
276/34:
langchain_documents = []

with jsonlines.open("articles.json", "r") as metadata_file:
    for file_path, metadata in zip(folders, metadata_file):
        try:
            pdf_loader = PyPDFLoader(file_path)
            pdf_documents = pdf_loader.load()

            for pdf_doc in pdf_documents:
            # Metadata'yÄ± dÃ¶kÃ¼mana ekle
                pdf_doc.metadata.update({
                    "url": metadata.get("url"),
                    "tags": metadata.get("tags"),
                    "title": metadata.get("title"),
                    "authors": metadata.get("authors")
                })

            # DÃ¶kÃ¼manÄ± listeye ekle
                langchain_documents.append( Document(
                        page_content=pdf_doc.page_content,
                        metadata=pdf_doc.metadata
                    )
                )
        except Exception as e:
            print(f"Error loading file {file_path}: {e}")
276/35: len(langchain_documents), langchain_documents[:1]
276/36:
from langchain.vectorstores import Qdrant
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
276/37:
from langchain_ollama import OllamaEmbeddings

embeddings = OpenAIEmbeddings()
276/38:
from langchain_ollama import OllamaEmbeddings

# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
embeddings = OpenAIEmbeddings()
276/39:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(langchain_documents)
276/40:
if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)

#vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
276/41:
if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)

#vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
276/42:
langchain_documents = []

with jsonlines.open("articles.json", "r") as metadata_file:
    for file_path, metadata in zip(folders, metadata_file):
        try:
            pdf_loader = PyPDFLoader(file_path)
            pdf_documents = pdf_loader.load()

            for pdf_doc in pdf_documents:
            # Metadata'yÄ± dÃ¶kÃ¼mana ekle
                pdf_doc.metadata.update({
                    "url": metadata.get("url"),
                    "tags": metadata.get("tags"),
                    "title": metadata.get("title"),
                    "authors": metadata.get("authors")
                })

            # DÃ¶kÃ¼manÄ± listeye ekle
                langchain_documents.append( Document(
                        page_content=pdf_doc.page_content,
                        metadata=pdf_doc.metadata
                    )
                )
        except Exception as e:
            print(f"Error loading file {file_path}: {e}")
276/43: len(langchain_documents), langchain_documents[:1]
276/44:
from langchain.vectorstores import Qdrant
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
276/45:
from langchain_ollama import OllamaEmbeddings

# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
embeddings = OpenAIEmbeddings()
276/46:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(langchain_documents)
276/47:
if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)

#vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
276/48:
if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)

#vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
276/49:
langchain_documents = []

with jsonlines.open("articles.json", "r") as metadata_file:
    for file_path, metadata in zip(folders, metadata_file):
        try:
            pdf_loader = PyPDFLoader(file_path)
            pdf_documents = pdf_loader.load()

            for pdf_doc in pdf_documents:
            # Metadata'yÄ± dÃ¶kÃ¼mana ekle
                pdf_doc.metadata.update({
                    "url": metadata.get("url"),
                    "tags": metadata.get("tags"),
                    "title": metadata.get("title"),
                    "authors": metadata.get("authors")
                })

            # DÃ¶kÃ¼manÄ± listeye ekle
                langchain_documents.append( Document(
                        page_content=pdf_doc.page_content,
                        metadata=pdf_doc.metadata
                    )
                )
        except Exception as e:
            print(f"Error loading file {file_path}: {e}")
276/50: len(langchain_documents), langchain_documents[:1]
276/51:
from langchain.vectorstores import Qdrant
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
276/52:
from langchain_ollama import OllamaEmbeddings

# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
embeddings = OpenAIEmbeddings()
276/53:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(langchain_documents)
276/54:
if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)

#vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
276/55:
import jsonlines
with jsonlines.open("articles-Copy1.json.json","r") as documents:
    print(next(iter(documents)))
276/56:
import jsonlines
with jsonlines.open("articles-Copy1.json","r") as documents:
    print(next(iter(documents)))
276/57:
from langchain.document_loaders import PyPDFLoader
from langchain_core.documents import Document
folders = glob.glob("articles/*")
276/58:
langchain_documents = []

with jsonlines.open("articles.json", "r") as metadata_file:
    for file_path, metadata in zip(folders, metadata_file):
        try:
            pdf_loader = PyPDFLoader(file_path)
            pdf_documents = pdf_loader.load()

            for pdf_doc in pdf_documents:
            # Metadata'yÄ± dÃ¶kÃ¼mana ekle
                pdf_doc.metadata.update({
                    "url": metadata.get("url"),
                    "tags": metadata.get("tags"),
                    "title": metadata.get("title"),
                    "authors": metadata.get("authors")
                })

            # DÃ¶kÃ¼manÄ± listeye ekle
                langchain_documents.append( Document(
                        page_content=pdf_doc.page_content,
                        metadata=pdf_doc.metadata
                    )
                )
        except Exception as e:
            print(f"Error loading file {file_path}: {e}")
276/59:
import jsonlines
with jsonlines.open("articles-Copy1.json","r") as documents:
    print(next(iter(documents)))
276/60:
from langchain.document_loaders import PyPDFLoader
from langchain_core.documents import Document
folders = glob.glob("articles/*")
276/61:
langchain_documents = []

with jsonlines.open("articles.json", "r") as metadata_file:
    for file_path, metadata in zip(folders, metadata_file):
        try:
            pdf_loader = PyPDFLoader(file_path)
            pdf_documents = pdf_loader.load()

            for pdf_doc in pdf_documents:
            # Metadata'yÄ± dÃ¶kÃ¼mana ekle
                pdf_doc.metadata.update({
                    "url": metadata.get("url"),
                    "tags": metadata.get("tags"),
                    "title": metadata.get("title"),
                    "authors": metadata.get("authors")
                })

            # DÃ¶kÃ¼manÄ± listeye ekle
                langchain_documents.append( Document(
                        page_content=pdf_doc.page_content,
                        metadata=pdf_doc.metadata
                    )
                )
        except Exception as e:
            print(f"Error loading file {file_path}: {e}")
276/62: len(langchain_documents), langchain_documents[:1]
276/63:
from langchain.vectorstores import Qdrant
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
276/64:
from langchain_ollama import OllamaEmbeddings

# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
embeddings = OpenAIEmbeddings()
276/65:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(langchain_documents)
276/66:
if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)

#vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
276/67:
from langchain.document_loaders import PyPDFLoader
from langchain_core.documents import Document
folders = glob.glob("example/*")
276/68:
langchain_documents = []

with jsonlines.open("articles.json", "r") as metadata_file:
    for file_path, metadata in zip(folders, metadata_file):
        try:
            pdf_loader = PyPDFLoader(file_path)
            pdf_documents = pdf_loader.load()

            for pdf_doc in pdf_documents:
            # Metadata'yÄ± dÃ¶kÃ¼mana ekle
                pdf_doc.metadata.update({
                    "url": metadata.get("url"),
                    "tags": metadata.get("tags"),
                    "title": metadata.get("title"),
                    "authors": metadata.get("authors")
                })

            # DÃ¶kÃ¼manÄ± listeye ekle
                langchain_documents.append( Document(
                        page_content=pdf_doc.page_content,
                        metadata=pdf_doc.metadata
                    )
                )
        except Exception as e:
            print(f"Error loading file {file_path}: {e}")
276/69: len(langchain_documents), langchain_documents[:1]
276/70:
from langchain.vectorstores import Qdrant
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
276/71:
from langchain_ollama import OllamaEmbeddings

# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
embeddings = OpenAIEmbeddings()
276/72:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(langchain_documents)
276/73:
if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)

#vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
276/74:
langchain_documents = []

with jsonlines.open("articles.json", "r") as metadata_file:
    for file_path, metadata in zip(folders, metadata_file):
        try:
            pdf_loader = PyPDFLoader(file_path)
            pdf_documents = pdf_loader.load()

            for pdf_doc in pdf_documents:
            # Metadata'yÄ± dÃ¶kÃ¼mana ekle
                pdf_doc.metadata.update({
                    "url": metadata.get("url"),
                    "tags":", ".join(metadata.get("tags"),
                    "title": metadata.get("title"),
                    "authors":", ".join(metadata.get("authors"))
                })

            # DÃ¶kÃ¼manÄ± listeye ekle
                langchain_documents.append( Document(
                        page_content=pdf_doc.page_content,
                        metadata=pdf_doc.metadata
                    )
                )
        except Exception as e:
            print(f"Error loading file {file_path}: {e}")

            metadata["tags"] = ", ".join(['Large Language Model', 'Telecom', 'Article', '5G'])
276/75:
langchain_documents = []

with jsonlines.open("articles.json", "r") as metadata_file:
    for file_path, metadata in zip(folders, metadata_file):
        try:
            pdf_loader = PyPDFLoader(file_path)
            pdf_documents = pdf_loader.load()

            for pdf_doc in pdf_documents:
            # Metadata'yÄ± dÃ¶kÃ¼mana ekle
                pdf_doc.metadata.update({
                    "url": metadata.get("url"),
                    "tags":", ".join(metadata.get("tags"),
                    "title": ", ".join(metadata.get("title")),
                    "authors":", ".join(metadata.get("authors"))
                }
                )

            # DÃ¶kÃ¼manÄ± listeye ekle
                langchain_documents.append( Document(
                        page_content=pdf_doc.page_content,
                        metadata=pdf_doc.metadata
                    )
                )
        except Exception as e:
            print(f"Error loading file {file_path}: {e}")

            metadata["tags"] = ", ".join(['Large Language Model', 'Telecom', 'Article', '5G'])
276/76:
langchain_documents = []

with jsonlines.open("articles.json", "r") as metadata_file:
    for file_path, metadata in zip(folders, metadata_file):
        try:
            pdf_loader = PyPDFLoader(file_path)
            pdf_documents = pdf_loader.load()

            for pdf_doc in pdf_documents:
            # Metadata'yÄ± dÃ¶kÃ¼mana ekle
                pdf_doc.metadata.update({
                    "url": metadata.get("url"),
                    "tags":", ".join(metadata.get("tags")),
                    "title": ", ".join(metadata.get("title")),
                    "authors":", ".join(metadata.get("authors"))
                }
                )

            # DÃ¶kÃ¼manÄ± listeye ekle
                langchain_documents.append( Document(
                        page_content=pdf_doc.page_content,
                        metadata=pdf_doc.metadata
                    )
                )
        except Exception as e:
            print(f"Error loading file {file_path}: {e}")

            metadata["tags"] = ", ".join(['Large Language Model', 'Telecom', 'Article', '5G'])
276/77: len(langchain_documents), langchain_documents[:1]
276/78:
langchain_documents = []

with jsonlines.open("articles.json", "r") as metadata_file:
    for file_path, metadata in zip(folders, metadata_file):
        try:
            pdf_loader = PyPDFLoader(file_path)
            pdf_documents = pdf_loader.load()

            for pdf_doc in pdf_documents:
            # Metadata'yÄ± dÃ¶kÃ¼mana ekle
                pdf_doc.metadata.update({
                    "url": metadata.get("url"),
                    "tags":", ".join(metadata.get("tags")),
                    "title": metadata.get("title"),
                    "authors":", ".join(metadata.get("authors"))
                }
                )

            # DÃ¶kÃ¼manÄ± listeye ekle
                langchain_documents.append( Document(
                        page_content=pdf_doc.page_content,
                        metadata=pdf_doc.metadata
                    )
                )
        except Exception as e:
            print(f"Error loading file {file_path}: {e}")

            metadata["tags"] = ", ".join(['Large Language Model', 'Telecom', 'Article', '5G'])
276/79: len(langchain_documents), langchain_documents[:1]
276/80:
from langchain.vectorstores import Qdrant
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
276/81:
from langchain_ollama import OllamaEmbeddings

# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
embeddings = OpenAIEmbeddings()
276/82:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(langchain_documents)
276/83:
if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)

#vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
276/84:
vectorstore.similarity_search_with_score(
    query="AI and authors",
    k=1
)
276/85:
store.similarity_search_with_score(
    query="AI and authors",
    filter={"authors": "Rex Yinssg"},
    k=1
)
276/86:
vectorstore.similarity_search_with_score(
    query="AI and authors",
    filter={"authors": "Rex Yinssg"},
    k=1
)
276/87:
vectorstore.similarity_search_with_score(
    query="AI and authors",
    filter={"authors": "Rex Ying"},
    k=1
)
276/88:
vectorstore.similarity_search_with_score(
    query="AI and authors",
    filter={"authors": "Yili Jin"},
    k=1
)
276/89:
vectorstore.similarity_search_with_score(
    filter={"authors": "Yili Jin"},
    k=1
)
276/90:
vectorstore.similarity_search_with_score(
    query="authors",
    filter={"authors": "Yili Jin"},
    k=1
)
276/91:
vectorstore.similarity_search_with_score(
    query="AI and authors",
    k=1
)
276/92: retriever = vectorstore.as_retriever
276/93:
retriever = vectorstore.as_retriever
retriever.invoke("What is groundedness")
276/94:
retriever = vectorstore.as_retriever
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever)
276/95:
retriever = vectorstore.as_retriever
conversation_chain = ConversationalRetrievalChain.from_llm(llm=MODEL, retriever=retriever)
276/96:
llm = ChatOpenAI(temperature=0.7, model_name=MODEL)
retriever = vectorstore.as_retriever
conversation_chain = ConversationalRetrievalChain.from_llm(llm=MODEL, retriever=retriever)
276/97:
llm = ChatOpenAI(temperature=0.7, model_name=MODEL)
retriever = vectorstore.as_retriever
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever)
280/1:
if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)
#vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
# create a new Chat with OpenAI
llm = ChatOpenAI(temperature=0.7, model_name=MODEL)

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

query = "What is groundedness score"
result = conversation_chain.invoke({"question":query})
print(result["answer"])
280/2:
import os
import glob
from dotenv import load_dotenv
280/3:
# imports for langchain

from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
# from langchain_chroma import Chroma
from langchain.vectorstores import FAISS
import numpy as np
from sklearn.manifold import TSNE
import plotly.graph_objects as go
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
280/4:
# imports for langchain, plotly and Chroma

from langchain.prompts import ChatPromptTemplate
from langchain_community.llms.ollama import Ollama
from langchain_community.vectorstores import Chroma
280/5:
#MODEL = Ollama(model="llama3.2")
MODEL = "gpt-4o-mini"
db_name = "vector-database"
280/6: folders = glob.glob("example/*")
280/7: text_loader_kwargs = {'encoding': 'utf-8'}
280/8:
import jsonlines
with jsonlines.open("articles.json","r") as documents:
    print(next(iter(documents)))
280/9:
from langchain.document_loaders import PyPDFLoader
from langchain_core.documents import Document
folders = glob.glob("articles/*")
280/10:
langchain_documents = []

with jsonlines.open("articles.json", "r") as metadata_file:
    for file_path, metadata in zip(folders, metadata_file):
        try:
            pdf_loader = PyPDFLoader(file_path)
            pdf_documents = pdf_loader.load()

            for pdf_doc in pdf_documents:
            # Metadata'yÄ± dÃ¶kÃ¼mana ekle
                pdf_doc.metadata.update({
                    "url": metadata.get("url"),
                    "tags": metadata.get("tags"),
                    "title": metadata.get("title"),
                    "authors": metadata.get("authors")
                })

            # DÃ¶kÃ¼manÄ± listeye ekle
                langchain_documents.append( Document(
                        page_content=pdf_doc.page_content,
                        metadata=pdf_doc.metadata
                    )
                )
        except Exception as e:
            print(f"Error loading file {file_path}: {e}")
280/11: len(langchain_documents), langchain_documents[:1]
280/12:
from langchain.vectorstores import Qdrant
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
280/13:
from langchain_ollama import OllamaEmbeddings

# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
embeddings = OpenAIEmbeddings()
280/14:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(langchain_documents)
280/15:
if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)
#vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
# create a new Chat with OpenAI
llm = ChatOpenAI(temperature=0.7, model_name=MODEL)

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

query = "What is groundedness score"
result = conversation_chain.invoke({"question":query})
print(result["answer"])
276/98:
langchain_documents = []

with jsonlines.open("articles.json", "r") as metadata_file:
    for file_path, metadata in zip(folders, metadata_file):
        try:
            pdf_loader = PyPDFLoader(file_path)
            pdf_documents = pdf_loader.load()

            for pdf_doc in pdf_documents:
            # Metadata'yÄ± dÃ¶kÃ¼mana ekle
                pdf_doc.metadata.update({
                    "url": metadata.get("url"),
                    "tags":", ".join(metadata.get("tags")),
                    "title": metadata.get("title"),
                    "authors":", ".join(metadata.get("authors"))
                }
                )

            # DÃ¶kÃ¼manÄ± listeye ekle
                langchain_documents.append( Document(
                        page_content=pdf_doc.page_content,
                        metadata=pdf_doc.metadata
                    )
                )
        except Exception as e:
            print(f"Error loading file {file_path}: {e}")

            metadata["tags"] = ", ".join(['Large Language Model', 'Telecom', 'Article', '5G'])
276/99: len(langchain_documents), langchain_documents[:1]
276/100:
from langchain.vectorstores import Qdrant
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
276/101:
from langchain_ollama import OllamaEmbeddings

# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
embeddings = OpenAIEmbeddings()
276/102:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(langchain_documents)
276/103:
if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)

#vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
276/104:
llm = ChatOpenAI(temperature=0.7, model_name=MODEL)
retriever = vectorstore.as_retriever
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever)
co
280/16:
import jsonlines
import json
with jsonlines.open("articles.json","r") as documents:
    print(next(iter(documents)))
280/17:
from langchain.document_loaders import PyPDFLoader
from langchain_core.documents import Document
folders = glob.glob("articles/*")
280/18:
langchain_documents = []

with jsonlines.open("articles.json", "r") as metadata_file:
    for file_path, metadata in zip(folders, metadata_file):
        try:
            pdf_loader = PyPDFLoader(file_path)
            pdf_documents = pdf_loader.load()

            for pdf_doc in pdf_documents:
            # Metadata'yÄ± dÃ¶kÃ¼mana ekle
                pdf_doc.metadata.update({
                    "url": json.dumps(metadata.get("url")),
                    "tags": json.dumps(metadata.get("tags")),
                    "title": metadata.get("title"),
                    "authors": json.dumps(metadata.get("authors"))
                })

            # DÃ¶kÃ¼manÄ± listeye ekle
                langchain_documents.append( Document(
                        page_content=pdf_doc.page_content,
                        metadata=pdf_doc.metadata
                    )
                )
        except Exception as e:
            print(f"Error loading file {file_path}: {e}")
280/19: len(langchain_documents), langchain_documents[:1]
280/20:
from langchain.vectorstores import Qdrant
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
280/21:
from langchain_ollama import OllamaEmbeddings

# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
embeddings = OpenAIEmbeddings()
280/22:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(langchain_documents)
280/23:
if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)
#vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
# create a new Chat with OpenAI
llm = ChatOpenAI(temperature=0.7, model_name=MODEL)

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

query = "What is groundedness score"
result = conversation_chain.invoke({"question":query})
print(result["answer"])
280/24:
from langchain.document_loaders import PyPDFLoader
from langchain_core.documents import Document
folders = glob.glob("example/*")
280/25:
langchain_documents = []

with jsonlines.open("articles.json", "r") as metadata_file:
    for file_path, metadata in zip(folders, metadata_file):
        try:
            pdf_loader = PyPDFLoader(file_path)
            pdf_documents = pdf_loader.load()

            for pdf_doc in pdf_documents:
            # Metadata'yÄ± dÃ¶kÃ¼mana ekle
                pdf_doc.metadata.update({
                    "url": json.dumps(metadata.get("url")),
                    "tags": json.dumps(metadata.get("tags")),
                    "title": metadata.get("title"),
                    "authors": json.dumps(metadata.get("authors"))
                })

            # DÃ¶kÃ¼manÄ± listeye ekle
                langchain_documents.append( Document(
                        page_content=pdf_doc.page_content,
                        metadata=pdf_doc.metadata
                    )
                )
        except Exception as e:
            print(f"Error loading file {file_path}: {e}")
280/26: len(langchain_documents), langchain_documents[:1]
280/27:
from langchain.vectorstores import Qdrant
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
280/28:
from langchain_ollama import OllamaEmbeddings

# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
embeddings = OpenAIEmbeddings()
280/29:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(langchain_documents)
280/30:
if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)
#vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
# create a new Chat with OpenAI
llm = ChatOpenAI(temperature=0.7, model_name=MODEL)

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

query = "What is groundedness score"
result = conversation_chain.invoke({"question":query})
print(result["answer"])
280/31:
if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)
#vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
# create a new Chat with OpenAI
llm = ChatOpenAI(temperature=0.7, model_name=MODEL)

from langchain_community.llms import Ollama
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

llm = Ollama(
    model="llama3.2",
    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),
) 

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

query = "What is groundedness score"
result = conversation_chain.invoke({"question":query})
print(result["answer"])
280/32:
if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)
#vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
# create a new Chat with OpenAI
llm = ChatOpenAI(temperature=0.7, model_name=MODEL)

from langchain_community.llms import Ollama
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

llm = OllamaChat(
    model="llama3.2",
    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),
) 

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

query = "What is groundedness score"
result = conversation_chain.invoke({"question":query})
print(result["answer"])
280/33:
if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)
#vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
# create a new Chat with OpenAI
llm = ChatOpenAI(temperature=0.7, model_name=MODEL)

from langchain_community.llms import Ollama
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain_ollama import ChatOllama
llm = ChatOllama(
    model="llama3.2",
    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),
) 

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

query = "What is groundedness score"
result = conversation_chain.invoke({"question":query})
print(result["answer"])
280/34:
vectorstore.similarity_search_with_score(
    query="AI and authors",
    k=1
)
280/35:
vectorstore.similarity_search_with_score(
    query="AI and authors",
    filter={"authors": "Rex Ying"},
    k=1
)
280/36:
vectorstore.similarity_search_with_score(
    query="AI and authors",
    filter={"authors": "Yili Jin"},
    k=1
)
280/37:
vectorstore.similarity_search_with_score(
    query="AI and authors",
    filter={"author"},
    k=1
)
280/38:
vectorstore.similarity_search_with_score(
    query="AI and authors",
    filter={"title": 'Large Language Model (LLM) for Telecommunications: A Comprehensive Survey on Principles, Key Techniques, and Opportunities' },
    k=1
)
280/39: len(langchain_documents), langchain_documents
280/40:
chunk_metadata = []

for i, chunk in enumerate(chunks):
    # 1. BaÅlÄ±k yakalama
    title_match = re.search(r"^(.*?):", chunk)
    title = title_match.group(1).strip() if title_match else "Unknown"

    # 2. Yazar bilgisi
    authors_match = re.findall(r"Authors?:\s*([a-zA-Z ,]+)", chunk)
    authors = authors_match[0] if authors_match else "Unknown"

    # 3. Tarih bilgisi
    date_match = re.search(r"Date:\s*([\d-]+)", chunk)
    date = date_match.group(1) if date_match else "Unknown"

    # 4. Anahtar kelimeler
    words = re.findall(r'\w+', chunk.lower())
    common_words = Counter(words).most_common(5)
    keywords = [word for word, _ in common_words]

    # 5. Metadata oluÅturma
    metadata = {
        "chunk_index": i,
        "title": title,
        "authors": authors,
        "date": date,
        "keywords": keywords
    }
    chunk_metadata.append(metadata)

# SonuÃ§larÄ± yazdÄ±rma
for i, metadata in enumerate(chunk_metadata):
    print(f"Chunk {i}: {chunks[i]}")
    print(f"Metadata: {metadata}")
    print("--------")
280/41:
import re

chunk_metadata = []

for i, chunk in enumerate(chunks):
    # 1. BaÅlÄ±k yakalama
    title_match = re.search(r"^(.*?):", chunk)
    title = title_match.group(1).strip() if title_match else "Unknown"

    # 2. Yazar bilgisi
    authors_match = re.findall(r"Authors?:\s*([a-zA-Z ,]+)", chunk)
    authors = authors_match[0] if authors_match else "Unknown"

    # 3. Tarih bilgisi
    date_match = re.search(r"Date:\s*([\d-]+)", chunk)
    date = date_match.group(1) if date_match else "Unknown"

    # 4. Anahtar kelimeler
    words = re.findall(r'\w+', chunk.lower())
    common_words = Counter(words).most_common(5)
    keywords = [word for word, _ in common_words]

    # 5. Metadata oluÅturma
    metadata = {
        "chunk_index": i,
        "title": title,
        "authors": authors,
        "date": date,
        "keywords": keywords
    }
    chunk_metadata.append(metadata)

# SonuÃ§larÄ± yazdÄ±rma
for i, metadata in enumerate(chunk_metadata):
    print(f"Chunk {i}: {chunks[i]}")
    print(f"Metadata: {metadata}")
    print("--------")
280/42:
import re

chunk_metadata = []

for i, chunk in enumerate(chunks):
    if isinstance(chunk, Document):
        chunk_text = chunk.page_content
    else:
        chunk_text = chunk  # EÄer zaten string ise direkt kullan

    # 1. BaÅlÄ±k yakalama
    title_match = re.search(r"^(.*?):", chunk)
    title = title_match.group(1).strip() if title_match else "Unknown"

    # 2. Yazar bilgisi
    authors_match = re.findall(r"Authors?:\s*([a-zA-Z ,]+)", chunk)
    authors = authors_match[0] if authors_match else "Unknown"

    # 3. Tarih bilgisi
    date_match = re.search(r"Date:\s*([\d-]+)", chunk)
    date = date_match.group(1) if date_match else "Unknown"

    # 4. Anahtar kelimeler
    words = re.findall(r'\w+', chunk.lower())
    common_words = Counter(words).most_common(5)
    keywords = [word for word, _ in common_words]

    # 5. Metadata oluÅturma
    metadata = {
        "chunk_index": i,
        "title": title,
        "authors": authors,
        "date": date,
        "keywords": keywords
    }
    chunk_metadata.append(metadata)

# SonuÃ§larÄ± yazdÄ±rma
for i, metadata in enumerate(chunk_metadata):
    print(f"Chunk {i}: {chunks[i]}")
    print(f"Metadata: {metadata}")
    print("--------")
280/43:
import re

chunk_metadata = []

for i, chunk in enumerate(chunks):
    if isinstance(chunk, Document):
        chunk_text = chunk.page_content
    else:
        chunk_text = chunk  # EÄer zaten string ise direkt kullan

    # 1. BaÅlÄ±k yakalama
    title_match = re.search(r"^(.*?):", chunk_text)
    title = title_match.group(1).strip() if title_match else "Unknown"

    # 2. Yazar bilgisi
    authors_match = re.findall(r"Authors?:\s*([a-zA-Z ,]+)", chunk_text)
    authors = authors_match[0] if authors_match else "Unknown"

    # 3. Tarih bilgisi
    date_match = re.search(r"Date:\s*([\d-]+)", chunk_text)
    date = date_match.group(1) if date_match else "Unknown"

    # 4. Anahtar kelimeler
    words = re.findall(r'\w+', chunk_text.lower())
    common_words = Counter(words).most_common(5)
    keywords = [word for word, _ in common_words]

    # 5. Metadata oluÅturma
    metadata = {
        "chunk_index": i,
        "title": title,
        "authors": authors,
        "date": date,
        "keywords": keywords
    }
    chunk_metadata.append(metadata)

# SonuÃ§larÄ± yazdÄ±rma
for i, metadata in enumerate(chunk_metadata):
    print(f"Chunk {i}: {chunks[i]}")
    print(f"Metadata: {metadata}")
    print("--------")
280/44:
import re
from collections import Counter
chunk_metadata = []

for i, chunk in enumerate(chunks):
    if isinstance(chunk, Document):
        chunk_text = chunk.page_content
    else:
        chunk_text = chunk  # EÄer zaten string ise direkt kullan

    # 1. BaÅlÄ±k yakalama
    title_match = re.search(r"^(.*?):", chunk_text)
    title = title_match.group(1).strip() if title_match else "Unknown"

    # 2. Yazar bilgisi
    authors_match = re.findall(r"Authors?:\s*([a-zA-Z ,]+)", chunk_text)
    authors = authors_match[0] if authors_match else "Unknown"

    # 3. Tarih bilgisi
    date_match = re.search(r"Date:\s*([\d-]+)", chunk_text)
    date = date_match.group(1) if date_match else "Unknown"

    # 4. Anahtar kelimeler
    words = re.findall(r'\w+', chunk_text.lower())
    common_words = Counter(words).most_common(5)
    keywords = [word for word, _ in common_words]

    # 5. Metadata oluÅturma
    metadata = {
        "chunk_index": i,
        "title": title,
        "authors": authors,
        "date": date,
        "keywords": keywords
    }
    chunk_metadata.append(metadata)

# SonuÃ§larÄ± yazdÄ±rma
for i, metadata in enumerate(chunk_metadata):
    print(f"Chunk {i}: {chunks[i]}")
    print(f"Metadata: {metadata}")
    print("--------")
280/45:
import re

def document_based_chunking(text):
    paragraphs = re.split(r'\n\s*\n', text)
    chunks = []
    
    for paragraph in paragraphs:
        sentences = nltk.sent_tokenize(paragraph)
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk) + len(sentence) <= 100:
                current_chunk += sentence + " "
            else:
                chunks.append(current_chunk.strip())
                current_chunk = sentence + " "
        
        if current_chunk:
            chunks.append(current_chunk.strip())
    
    return chunks

text = """This is the first paragraph of the document.
It contains multiple sentences.

This is the second paragraph.
It also has multiple sentences for demonstration."""

chunks = document_based_chunking(text)
print(chunks)
280/46:
import re
!pip install nltk
import nltk
def document_based_chunking(text):
    paragraphs = re.split(r'\n\s*\n', text)
    chunks = []
    
    for paragraph in paragraphs:
        sentences = nltk.sent_tokenize(paragraph)
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk) + len(sentence) <= 100:
                current_chunk += sentence + " "
            else:
                chunks.append(current_chunk.strip())
                current_chunk = sentence + " "
        
        if current_chunk:
            chunks.append(current_chunk.strip())
    
    return chunks

text = """This is the first paragraph of the document.
It contains multiple sentences.

This is the second paragraph.
It also has multiple sentences for demonstration."""

chunks = document_based_chunking(text)
print(chunks)
280/47:
import re
!pip install nltk
import nltk
def document_based_chunking(text):
    paragraphs = re.split(r'\n\s*\n', text)
    chunks = chunks.page_content
    
    for paragraph in paragraphs:
        sentences = nltk.sent_tokenize(paragraph)
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk) + len(sentence) <= 100:
                current_chunk += sentence + " "
            else:
                chunks.append(current_chunk.strip())
                current_chunk = sentence + " "
        
        if current_chunk:
            chunks.append(current_chunk.strip())
    
    return chunks

text = """This is the first paragraph of the document.
It contains multiple sentences.

This is the second paragraph.
It also has multiple sentences for demonstration."""

chunks = document_based_chunking(text)
print(chunks)
280/48:
import re
!pip install nltk
import nltk
def document_based_chunking(text):
    paragraphs = re.split(r'\n\s*\n', text)
    chunks =
    
    for paragraph in paragraphs:
        sentences = nltk.sent_tokenize(paragraph)
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk) + len(sentence) <= 100:
                current_chunk += sentence + " "
            else:
                chunks.append(current_chunk.strip())
                current_chunk = sentence + " "
        
        if current_chunk:
            chunks.append(current_chunk.strip())
    
    return chunks

text = langchain_documents

chunks = document_based_chunking(text)
print(chunks)
280/49:
import re
!pip install nltk
import nltk
def document_based_chunking(text):
    paragraphs = re.split(r'\n\s*\n', text)
    chunks = []
    
    for paragraph in paragraphs:
        sentences = nltk.sent_tokenize(paragraph)
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk) + len(sentence) <= 100:
                current_chunk += sentence + " "
            else:
                chunks.append(current_chunk.strip())
                current_chunk = sentence + " "
        
        if current_chunk:
            chunks.append(current_chunk.strip())
    
    return chunks

text = langchain_documents

chunks = document_based_chunking(text)
print(chunks)
280/50:
for pdf_doc in pdf_documents:
    print(pdf_doc.page_content)
280/51:
for pdf_doc in pdf_documents:
    print(Display(Markdown(pdf_doc.page_content)))
280/52:
from IPython.display import display, Markdown
for pdf_doc in pdf_documents:
    print(display(Markdown(pdf_doc.page_content)))
280/53:
from pdfminer.high_level import extract_pages
from pdfminer.layout import LTTextContainer, LTChar,LTLine,LAParams
import os
path=r'example/Rapor-RAGEvaluation.pdf'

Extract_Data=[]

for page_layout in extract_pages(path):
    for element in page_layout:
        if isinstance(element, LTTextContainer):
            for text_line in element:
                for character in text_line:
                    if isinstance(character, LTChar):
                        Font_size=character.size
            Extract_Data.append([Font_size,(element.get_text())])
280/54:
from pdfminer.high_level import extract_pages
from pdfminer.layout import LTTextContainer, LTChar,LTLine,LAParams
import os
!pip install pdfminer
path=r'example/Rapor-RAGEvaluation.pdf'

Extract_Data=[]

for page_layout in extract_pages(path):
    for element in page_layout:
        if isinstance(element, LTTextContainer):
            for text_line in element:
                for character in text_line:
                    if isinstance(character, LTChar):
                        Font_size=character.size
            Extract_Data.append([Font_size,(element.get_text())])
280/55:
!pip install pdfminer
from pdfminer.high_level import extract_pages
from pdfminer.layout import LTTextContainer, LTChar,LTLine,LAParams
import os

path=r'example/Rapor-RAGEvaluation.pdf'

Extract_Data=[]

for page_layout in extract_pages(path):
    for element in page_layout:
        if isinstance(element, LTTextContainer):
            for text_line in element:
                for character in text_line:
                    if isinstance(character, LTChar):
                        Font_size=character.size
            Extract_Data.append([Font_size,(element.get_text())])
280/56:
!pip install pdfminer.six
from pdfminer.high_level import extract_pages
from pdfminer.layout import LTTextContainer, LTChar,LTLine,LAParams
import os

path=r'example/Rapor-RAGEvaluation.pdf'

Extract_Data=[]

for page_layout in extract_pages(path):
    for element in page_layout:
        if isinstance(element, LTTextContainer):
            for text_line in element:
                for character in text_line:
                    if isinstance(character, LTChar):
                        Font_size=character.size
            Extract_Data.append([Font_size,(element.get_text())])
280/57: print(Extract_Data)
280/58:
font_dict = {}
for element in Extract_Data:
    font_dict.update({element[0] : element[1]})
font_dict.keys()
280/59:
font_dict = {}
for element in Extract_Data:
    font_dict.update({element[0] : element[1]})
font_dict.keys().sort()
280/60:
font_dict = {}
for element in Extract_Data:
    font_dict.update({element[0] : element[1]})
type(font_dict.keys())
280/61:
font_dict = {}
for element in Extract_Data:
    font_dict.update({element[0] : element[1]})
type(list(font_dict.keys()))
280/62:
font_dict = {}
for element in Extract_Data:
    font_dict.update({element[0] : element[1]})
list(font_dict.keys())
280/63:
font_dict = {}
for element in Extract_Data:
    font_dict.update({element[0] : element[1]})
list(font_dict.keys()).sort()
280/64:
font_dict = {}
for element in Extract_Data:
    font_dict.update({element[0] : element[1]})
list(font_dict.keys()).sort()
280/65:
font_dict = {}
for element in Extract_Data:
    font_dict.update({element[0] : element[1]})
print(list(font_dict.keys()).sort())
280/66:
font_dict = {}
for element in Extract_Data:
    font_dict.update({element[0] : element[1]})
sort(list(font_dict.keys()))
280/67:
font_dict = {}
for element in Extract_Data:
    font_dict.update({element[0] : element[1]})
list(font_dict.keys).sort()
280/68:
font_dict = {}
for element in Extract_Data:
    font_dict.update({element[0] : element[1]})
(list(font_dict.keys)).sort()
280/69:
font_dict = {}
for element in Extract_Data:
    font_dict.update({element[0] : element[1]})
(list(font_dict.keys()).sort()
280/70:
font_dict = {}
for element in Extract_Data:
    font_dict.update({element[0] : element[1]})
list(font_dict.keys()).sort()
280/71:
font_dict = {}
for element in Extract_Data:
    font_dict.update({element[0] : element[1]})
print(list(font_dict.keys()).sort()=
280/72:
font_dict = {}
for element in Extract_Data:
    font_dict.update({element[0] : element[1]})
print(list(font_dict.keys()).sort())
280/73:
font_dict = {}
for element in Extract_Data:
    font_dict.update({element[0] : element[1]})
list_of_fonts = list(font_dict.keys())
list_of_fonts.sort()
280/74:
font_dict = {}
for element in Extract_Data:
    font_dict.update({element[0] : element[1]})
list_of_fonts = list(font_dict.keys())
print(list_of_fonts)
280/75:
font_dict = {}
for element in Extract_Data:
    font_dict.update({element[0] : element[1]})
list_of_fonts = list(font_dict.keys())
print(list_of_fonts.sort())
280/76:
font_dict = {}
for element in Extract_Data:
    font_dict.update({element[0] : element[1]})
list_of_fonts = list(font_dict.keys())
list_of_fonts.sort()
280/77:
font_dict = {}
for element in Extract_Data:
    font_dict.update({element[0] : element[1]})
list_of_fonts = list(font_dict.keys())
list_of_fonts.sort()
list_of_fonts
280/78:
font_dict = {}
for element in Extract_Data:
    font_dict.update({element[0] : element[1]})
list_of_fonts = list(font_dict.keys())
list_of_fonts.sort(reverse=True)
list_of_fonts
280/79:
font_dict = {}
for element in Extract_Data:
    font_dict.update({element[0] : element[1]})
list_of_fonts = list(font_dict.keys())
list_of_fonts.sort(reverse=True)
list_of_fonts
print(f"Title is {font_dict[list_of_fonts[0]]}")
280/80:
font_dict = {}
for element in Extract_Data:
    font_dict.update({element[0] : element[1]})
list_of_fonts = list(font_dict.keys())
list_of_fonts.sort(reverse=True)
list_of_fonts
print(f"Title is {font_dict[list_of_fonts[1]}")
280/81:
font_dict = {}
for element in Extract_Data:
    font_dict.update({element[0] : element[1]})
list_of_fonts = list(font_dict.keys())
list_of_fonts.sort(reverse=True)
list_of_fonts
print(f"Title is {font_dict[list_of_fonts[1]]}")
280/82:
font_dict = {}
for element in Extract_Data:
    font_dict.update({element[0] : element[1]})
list_of_fonts = list(font_dict.keys())
list_of_fonts.sort(reverse=True)
list_of_fonts
print(f"Title is {font_dict[list_of_fonts[0]]}")
280/83: font_dict
276/105:
%pip install transformers --quiet
%pip install intel-extension-for-transformers
276/106:
from intel_extension_for_transformers.transformers import WeightOnlyQuantConfig
from langchain_community.llms.weight_only_quantization import WeightOnlyQuantPipeline

conf = WeightOnlyQuantConfig(weight_dtype="nf4")
hf = WeightOnlyQuantPipeline.from_model_id(
    model_id="google/flan-t5-large",
    task="text2text-generation",
    quantization_config=conf,
    pipeline_kwargs={"max_new_tokens": 10},
)
276/107:
from intel_extension_for_transformers.transformers import WeightOnlyQuantConfig
from langchain_community.llms.weight_only_quantization import WeightOnlyQuantPipeline
!pip install neural_compressor.conf
conf = WeightOnlyQuantConfig(weight_dtype="nf4")
hf = WeightOnlyQuantPipeline.from_model_id(
    model_id="google/flan-t5-large",
    task="text2text-generation",
    quantization_config=conf,
    pipeline_kwargs={"max_new_tokens": 10},
)
276/108:
from intel_extension_for_transformers.transformers import WeightOnlyQuantConfig
from langchain_community.llms.weight_only_quantization import WeightOnlyQuantPipeline

conf = WeightOnlyQuantConfig(weight_dtype="nf4")
hf = WeightOnlyQuantPipeline.from_model_id(
    model_id="google/flan-t5-large",
    task="text2text-generation",
    quantization_config=conf,
    pipeline_kwargs={"max_new_tokens": 10}
)
276/109:
from intel_extension_for_transformers.transformers import AutoModelForSeq2SeqLM
from transformers import AutoTokenizer, pipeline

model_id = "google/flan-t5-large"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForSeq2SeqLM.from_pretrained(model_id)
pipe = pipeline(
    "text2text-generation", model=model, tokenizer=tokenizer, max_new_tokens=10
)
hf = WeightOnlyQuantPipeline(pipeline=pipe)
276/110:
from intel_extension_for_transformers.transformers import AutoModelForSeq2SeqLM
from transformers import AutoTokenizer, pipeline
!pip install neural-compressor

model_id = "google/flan-t5-large"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForSeq2SeqLM.from_pretrained(model_id)
pipe = pipeline(
    "text2text-generation", model=model, tokenizer=tokenizer, max_new_tokens=10
)
hf = WeightOnlyQuantPipeline(pipeline=pipe)
276/111:
from intel_extension_for_transformers.transformers import WeightOnlyQuantConfig
from langchain_community.llms.weight_only_quantization import WeightOnlyQuantPipeline
!pip install neural-compressor
conf = WeightOnlyQuantConfig(weight_dtype="nf4")
hf = WeightOnlyQuantPipeline.from_model_id(
    model_id="google/flan-t5-large",
    task="text2text-generation",
    quantization_config=conf,
    pipeline_kwargs={"max_new_tokens": 10}
)
276/112:
!pip install neural-compressor
from intel_extension_for_transformers.transformers import WeightOnlyQuantConfig
from langchain_community.llms.weight_only_quantization import WeightOnlyQuantPipeline

conf = WeightOnlyQuantConfig(weight_dtype="nf4")
hf = WeightOnlyQuantPipeline.from_model_id(
    model_id="google/flan-t5-large",
    task="text2text-generation",
    quantization_config=conf,
    pipeline_kwargs={"max_new_tokens": 10}
)
276/113:
!pip install neural-compressor
from intel_extension_for_transformers.transformers import WeightOnlyQuantConfig
from langchain_community.llms.weight_only_quantization import WeightOnlyQuantPipeline

conf = WeightOnlyQuantConfig(weight_dtype="nf4")
hf = WeightOnlyQuantPipeline.from_model_id(
    model_id="google/flan-t5-large",
    task="text2text-generation",
    quantization_config=conf,
    pipeline_kwargs={"max_new_tokens": 10}
)
276/114:
from intel_extension_for_transformers.transformers import AutoModelForSeq2SeqLM
from transformers import AutoTokenizer, pipeline


model_id = "google/flan-t5-large"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForSeq2SeqLM.from_pretrained(model_id)
pipe = pipeline(
    "text2text-generation", model=model, tokenizer=tokenizer, max_new_tokens=10
)
hf = WeightOnlyQuantPipeline(pipeline=pipe)
276/115:
!pip install neural-compressor
from intel_extension_for_transformers.transformers import WeightOnlyQuantConfig
from langchain_community.llms.weight_only_quantization import WeightOnlyQuantPipeline

conf = WeightOnlyQuantConfig(weight_dtype="nf4")
hf = WeightOnlyQuantPipeline.from_model_id(
    model_id="google/flan-t5-large",
    task="text2text-generation",
    quantization_config=conf,
    pipeline_kwargs={"max_new_tokens": 10}
)
276/116:
from intel_extension_for_transformers.transformers import WeightOnlyQuantConfig
from langchain_community.llms.weight_only_quantization import WeightOnlyQuantPipeline

conf = WeightOnlyQuantConfig(weight_dtype="nf4")
hf = WeightOnlyQuantPipeline.from_model_id(
    model_id="google/flan-t5-large",
    task="text2text-generation",
    quantization_config=conf,
    pipeline_kwargs={"max_new_tokens": 10},
)
276/117:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(langchain_documents)
chunks
276/118:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(langchain_documents)
len(chunks)
280/84:
import re
!pip install nltk
import nltk
def document_based_chunking(text):
    paragraphs = re.split(r'\n\s*\n', text)
    chunks = []
    
    for paragraph in paragraphs:
        sentences = nltk.sent_tokenize(paragraph)
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk) + len(sentence) <= 100:
                current_chunk += sentence + " "
            else:
                chunks.append(current_chunk.strip())
                current_chunk = sentence + " "
        
        if current_chunk:
            chunks.append(current_chunk.strip())
    
    return chunks

text = langchain_documents

chunks = document_based_chunking(text)
print(chunks)
280/85:
import re
!pip install nltk
import nltk
def document_based_chunking(text):
    paragraphs = re.split(r'\n\s*\n', text)
    chunks = []
    
    for paragraph in paragraphs:
        sentences = nltk.sent_tokenize(paragraph)
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk) + len(sentence) <= 100:
                current_chunk += sentence + " "
            else:
                chunks.append(current_chunk.strip())
                current_chunk = sentence + " "
        
        if current_chunk:
            chunks.append(current_chunk.strip())
    
    return chunks

text = langchain_documents

for text in langchain_documents:
    document_based_chunking(text)
280/86:
import re
!pip install nltk
import nltk
def document_based_chunking(text):
    paragraphs = re.split(r'\n\s*\n', text)
    chunks = []
    
    for paragraph in paragraphs:
        sentences = nltk.sent_tokenize(paragraph)
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk) + len(sentence) <= 100:
                current_chunk += sentence + " "
            else:
                chunks.append(current_chunk.strip())
                current_chunk = sentence + " "
        
        if current_chunk:
            chunks.append(current_chunk.strip())
    
    return chunks

text = langchain_documents

for text in langchain_documents:
    document_based_chunking(text)
280/87:
import re
!pip install nltk
import nltk
def document_based_chunking(text):
    paragraphs = re.split(r'\n\s*\n', text)
    chunks = []
    
    for paragraph in paragraphs:
        sentences = nltk.sent_tokenize(paragraph)
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk) + len(sentence) <= 100:
                current_chunk += sentence + " "
            else:
                chunks.append(current_chunk.strip())
                current_chunk = sentence + " "
        
        if current_chunk:
            chunks.append(current_chunk.strip())
    
    return chunks



for text.page_content in langchain_documents:
    document_based_chunking(text)
280/88:
import re
!pip install nltk
import nltk
def document_based_chunking(text):
    paragraphs = re.split(r'\n\s*\n', text)
    chunks = []
    
    for paragraph in paragraphs:
        sentences = nltk.sent_tokenize(paragraph)
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk) + len(sentence) <= 100:
                current_chunk += sentence + " "
            else:
                chunks.append(current_chunk.strip())
                current_chunk = sentence + " "
        
        if current_chunk:
            chunks.append(current_chunk.strip())
    
    return chunks



for text in langchain_documents:
    document_based_chunking(text.page_content)
280/89:
import re
!pip install nltk
import nltk
def document_based_chunking(text):
    paragraphs = re.split(r'\n\s*\n', text)
    chunks = []
    
    for paragraph in paragraphs:
        sentences = nltk.sent_tokenize(paragraph)
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk) + len(sentence) <= 100:
                current_chunk += sentence + " "
            else:
                chunks.append(current_chunk.strip())
                current_chunk = sentence + " "
        
        if current_chunk:
            chunks.append(current_chunk.strip())
    
    return chunks



for text in langchain_documents:
    text = document.page_content
    document_based_chunking(text.page_content)
280/90:
import re
!pip install nltk
import nltk
def document_based_chunking(text):
    paragraphs = re.split(r'\n\s*\n', text)
    chunks = []
    
    for paragraph in paragraphs:
        sentences = nltk.sent_tokenize(paragraph)
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk) + len(sentence) <= 100:
                current_chunk += sentence + " "
            else:
                chunks.append(current_chunk.strip())
                current_chunk = sentence + " "
        
        if current_chunk:
            chunks.append(current_chunk.strip())
    
    return chunks



for text in langchain_documents:
    documents = text.page_content
    document_based_chunking(documents)
280/91:
import re
!pip install nltk
import nltk
def document_based_chunking(text):
    paragraphs = re.split(r'\n\s*\n', text)
    chunks = []
    
    for paragraph in paragraphs:
        sentences = nltk.sent_tokenize(paragraph)
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk) + len(sentence) <= 100:
                current_chunk += sentence + " "
            else:
                chunks.append(current_chunk.strip())
                current_chunk = sentence + " "
        
        if current_chunk:
            chunks.append(current_chunk.strip())
    
    return chunks



for document in langchain_documents:
    text = document.page_content
    document_based_chunking(text)
280/92:
import re
import nltk

def document_based_chunking(text):
    paragraphs = re.split(r'\n\s*\n', text)
    chunks = []
    
    for paragraph in paragraphs:
        sentences = nltk.sent_tokenize(paragraph)
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk) + len(sentence) <= 100:
                current_chunk += sentence + " "
            else:
                chunks.append(current_chunk.strip())
                current_chunk = sentence + " "
        
        if current_chunk:
            chunks.append(current_chunk.strip())
    
    return chunks

# Store chunks for each document
all_chunks = []

for document in langchain_documents:
    text = document.page_content
    chunks = document_based_chunking(text)
    all_chunks.extend(chunks)  # Add the chunks to the all_chunks list

# Now all_chunks contains all the chunks from all documents
280/93:
import re
import nltk

def document_based_chunking(text):
    paragraphs = re.split(r'\n\s*\n', text)
    chunks = []
    
    for paragraph in paragraphs:
        sentences = nltk.sent_tokenize(paragraph)
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk) + len(sentence) <= 100:
                current_chunk += sentence + " "
            else:
                chunks.append(current_chunk.strip())
                current_chunk = sentence + " "
        
        if current_chunk:
            chunks.append(current_chunk.strip())
    
    return chunks

# Store chunks for each document
all_chunks = []

for document in langchain_documents:
    text = document.page_content  # This might not be the correct way to access the content
    if hasattr(document, 'text'):
        text = document.text  # Ensure you're using the correct attribute to access the text
    
    chunks = document_based_chunking(text)
    all_chunks.extend(chunks)  # Add the chunks to the all_chunks list

# Now all_chunks contains all the chunks from all documents
280/94:
    

if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)
#vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
# create a new Chat with OpenAI
#llm = ChatOpenAI(temperature=0.7, model_name=MODEL)

from langchain_community.llms import Ollama
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain_ollama import ChatOllama

llm = ChatOllama(
    model="llama3.2",
    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),
) 

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

query = "What is groundedness score"
result = conversation_chain.invoke({"question":query})
print(result["answer"])


from langchain_core.messages import AIMessage

messages = [
    (
        "system",
        "You are a helpful assistant that translates English to French. Translate the user sentence.",
    ),
    ("human", "I love programming."),
]
ai_msg = llm.invoke(messages)
ai_msg
280/95:
    

if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)
#vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
# create a new Chat with OpenAI
#llm = ChatOpenAI(temperature=0.7, model_name=MODEL)

from langchain_community.llms import Ollama
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain_ollama import ChatOllama

llm = ChatOllama(
    model="llama3.2",
    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),
) 

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

query = "What is groundedness score"
result = conversation_chain.invoke({"question":query})
print(result["answer"])


from langchain_core.messages import AIMessage

messages = [
    (
        "system",
        "You are a helpful assistant that translates English to French. Translate the user sentence.",
    ),
    ("human", "I love programming."),
]
ai_msg = llm.invoke(messages)
ai_msg
280/96:
"""import re
import nltk

def document_based_chunking(text):
    paragraphs = re.split(r'\n\s*\n', text)
    chunks = []
    
    for paragraph in paragraphs:
        sentences = nltk.sent_tokenize(paragraph)
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk) + len(sentence) <= 100:
                current_chunk += sentence + " "
            else:
                chunks.append(current_chunk.strip())
                current_chunk = sentence + " "
        
        if current_chunk:
            chunks.append(current_chunk.strip())
    
    return chunks

# Store chunks for each document
all_chunks = []

for document in langchain_documents:
    text = document.page_content  # This might not be the correct way to access the content
    if hasattr(document, 'text'):
        text = document.text  # Ensure you're using the correct attribute to access the text
    
    chunks = document_based_chunking(text)
    all_chunks.extend(chunks)  # Add the chunks to the all_chunks list

# Now all_chunks contains all the chunks from all documents
"""
280/97:
    

if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)
#vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
# create a new Chat with OpenAI
#llm = ChatOpenAI(temperature=0.7, model_name=MODEL)

from langchain_community.llms import Ollama
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain_ollama import ChatOllama

llm = ChatOllama(
    model="llama3.2",
    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),
) 

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

query = "What is groundedness score"
result = conversation_chain.invoke({"question":query})
print(result["answer"])


from langchain_core.messages import AIMessage

messages = [
    (
        "system",
        "You are a helpful assistant that translates English to French. Translate the user sentence.",
    ),
    ("human", "I love programming."),
]
ai_msg = llm.invoke(messages)
ai_msg
280/98:
    

if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)
#vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
# create a new Chat with OpenAI
#llm = ChatOpenAI(temperature=0.7, model_name=MODEL)

from langchain_community.llms import Ollama
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain_ollama import ChatOllama

llm = ChatOllama(
    model="llama3.2",
    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),
) 

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

query = "What is groundedness score"
result = conversation_chain.invoke({"question":query})
print(result["answer"])


from langchain_core.messages import AIMessage

messages = [
    (
        "system",
        "You are a helpful assistant that translates English to French. Translate the user sentence.",
    ),
    ("human", "I love programming."),
]
ai_msg = llm.invoke(messages)
ai_msg
280/99:
langchain_documents = []

with jsonlines.open("articles.json", "r") as metadata_file:
    for file_path, metadata in zip(folders, metadata_file):
        try:
            pdf_loader = PyPDFLoader(file_path)
            pdf_documents = pdf_loader.load()

            for pdf_doc in pdf_documents:
            # Metadata'yÄ± dÃ¶kÃ¼mana ekle
                pdf_doc.metadata.update({
                    "url": json.dumps(metadata.get("url")),
                    "tags": json.dumps(metadata.get("tags")),
                    "title": metadata.get("title"),
                    "authors": json.dumps(metadata.get("authors"))
                })

            # DÃ¶kÃ¼manÄ± listeye ekle
                langchain_documents.append( Document(
                        page_content=pdf_doc.page_content,
                        metadata=pdf_doc.metadata
                    )
                )
        except Exception as e:
            print(f"Error loading file {file_path}: {e}")
280/100:
'''
!pip install pdfminer.six
from pdfminer.high_level import extract_pages
from pdfminer.layout import LTTextContainer, LTChar,LTLine,LAParams
import os

path=r'example/Rapor-RAGEvaluation.pdf'

Extract_Data=[]

for page_layout in extract_pages(path):
    for element in page_layout:
        if isinstance(element, LTTextContainer):
            for text_line in element:
                for character in text_line:
                    if isinstance(character, LTChar):
                        Font_size=character.size
            Extract_Data.append([Font_size,(element.get_text())])
'''
280/101:
'''font_dict = {}
for element in Extract_Data:
    font_dict.update({element[0] : element[1]})
list_of_fonts = list(font_dict.keys())
list_of_fonts.sort(reverse=True)
list_of_fonts
print(f"Title is {font_dict[list_of_fonts[0]]}")
'''
280/102: font_dict
280/103: len(langchain_documents), langchain_documents
280/104:
#from langchain.vectorstores import Qdrant
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
280/105:
from langchain_ollama import OllamaEmbeddings

# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
embeddings = OpenAIEmbeddings()
280/106:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(langchain_documents)
280/107:
"""import re
import nltk

def document_based_chunking(text):
    paragraphs = re.split(r'\n\s*\n', text)
    chunks = []
    
    for paragraph in paragraphs:
        sentences = nltk.sent_tokenize(paragraph)
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk) + len(sentence) <= 100:
                current_chunk += sentence + " "
            else:
                chunks.append(current_chunk.strip())
                current_chunk = sentence + " "
        
        if current_chunk:
            chunks.append(current_chunk.strip())
    
    return chunks

# Store chunks for each document
all_chunks = []

for document in langchain_documents:
    text = document.page_content  # This might not be the correct way to access the content
    if hasattr(document, 'text'):
        text = document.text  # Ensure you're using the correct attribute to access the text
    
    chunks = document_based_chunking(text)
    all_chunks.extend(chunks)  # Add the chunks to the all_chunks list

# Now all_chunks contains all the chunks from all documents
"""
280/108:
    

if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)
#vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
# create a new Chat with OpenAI
#llm = ChatOpenAI(temperature=0.7, model_name=MODEL)

from langchain_community.llms import Ollama
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain_ollama import ChatOllama

llm = ChatOllama(
    model="llama3.2",
    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),
) 

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

query = "What is groundedness score"
result = conversation_chain.invoke({"question":query})
print(result["answer"])


from langchain_core.messages import AIMessage

messages = [
    (
        "system",
        "You are a helpful assistant that translates English to French. Translate the user sentence.",
    ),
    ("human", "I love programming."),
]
ai_msg = llm.invoke(messages)
ai_msg
280/109:
    

if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)
#vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
# create a new Chat with OpenAI
#llm = ChatOpenAI(temperature=0.7, model_name=MODEL)

from langchain_community.llms import Ollama
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain_ollama import ChatOllama

llm = ChatOllama(
    model="llama3.2",
    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),
) 

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

query = "What is groundedness score"
result = conversation_chain.invoke({"question":query})
print(result["answer"])


'''
messages = [
    (
        "system",
        "You are a helpful assistant that translates English to French. Translate the user sentence.",
    ),
    ("human", "I love programming."),
]
ai_msg = llm.invoke(messages)
ai_msg
'''
276/119:
if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)
docs = vectorstore.similarity_search("AI")
#vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
276/120:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(langchain_documents)
276/121:
if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)
docs = vectorstore.similarity_search("AI")
#vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
docs
276/122:
if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)
docs = vectorstore.similarity_search("AI")
#vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
# Convert loaded documents into strings by concatenating their content
# and ignoring metadata

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

chain = {"docs": format_docs} | prompt | model | StrOutputParser()

question = "Summarize this document"

docs = vectorstore.similarity_search(question)

chain.invoke(docs)
287/1:
import os
import glob
from dotenv import load_dotenv
287/2:
# imports for langchain

from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
# from langchain_chroma import Chroma
from langchain.vectorstores import FAISS
import numpy as np
from sklearn.manifold import TSNE
import plotly.graph_objects as go
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
287/3:
# imports for langchain, plotly and Chroma

from langchain.prompts import ChatPromptTemplate
from langchain_community.llms.ollama import Ollama
from langchain_community.vectorstores import Chroma
287/4:
#MODEL = Ollama(model="llama3.2")
MODEL = "gpt-4o-mini"
db_name = "vector-database"
287/5: folders = glob.glob("example/*")
287/6: text_loader_kwargs = {'encoding': 'utf-8'}
287/7:
import jsonlines
with jsonlines.open("articles-Copy1.json","r") as documents:
    print(next(iter(documents)))
287/8:
from langchain.document_loaders import PyPDFLoader
from langchain_core.documents import Document
folders = glob.glob("example/*")
287/9:
langchain_documents = []

with jsonlines.open("articles.json", "r") as metadata_file:
    for file_path, metadata in zip(folders, metadata_file):
        try:
            pdf_loader = PyPDFLoader(file_path)
            pdf_documents = pdf_loader.load()

            for pdf_doc in pdf_documents:
            # Metadata'yÄ± dÃ¶kÃ¼mana ekle
                pdf_doc.metadata.update({
                    "url": metadata.get("url"),
                    "tags":", ".join(metadata.get("tags")),
                    "title": metadata.get("title"),
                    "authors":", ".join(metadata.get("authors"))
                }
                )

            # DÃ¶kÃ¼manÄ± listeye ekle
                langchain_documents.append( Document(
                        page_content=pdf_doc.page_content,
                        metadata=pdf_doc.metadata
                    )
                )
        except Exception as e:
            print(f"Error loading file {file_path}: {e}")

            metadata["tags"] = ", ".join(['Large Language Model', 'Telecom', 'Article', '5G'])
287/10: len(langchain_documents), langchain_documents[:1]
287/11:
from langchain.vectorstores import Qdrant
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
287/12:
'''from langchain_ollama import OllamaEmbeddings

# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
embeddings = OpenAIEmbeddings()
'''
287/13:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(langchain_documents)
287/14:
if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)
docs = vectorstore.similarity_search("AI")
#vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
# Convert loaded documents into strings by concatenating their content
# and ignoring metadata

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

chain = {"docs": format_docs} | prompt | model | StrOutputParser()

question = "Summarize this document"

docs = vectorstore.similarity_search(question)

chain.invoke(docs)
287/15:
"""llm = ChatOpenAI(temperature=0.7, model_name=MODEL)
retriever = vectorstore.as_retriever
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever)
"""
from langchain_ollama import ChatOllama


# create a new Chat 
llm = ChatOllama(temperature=0.7, model_name=MODEL,temperature=0.7)

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)
287/16:
"""llm = ChatOpenAI(temperature=0.7, model_name=MODEL)
retriever = vectorstore.as_retriever
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever)
"""
from langchain_ollama import ChatOllama


# create a new Chat 
llm = ChatOllama(temperature=0.7, model_name=MODEL)

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)
287/17:
"""llm = ChatOpenAI(temperature=0.7, model_name=MODEL)
retriever = vectorstore.as_retriever
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever)
"""
from langchain_ollama import ChatOllama

model = "ollama-3.2"
# create a new Chat 
llm = ChatOllama(temperature=0.7, model_name=MODEL)

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)
287/18:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(langchain_documents)
len(chunks)
287/19:
from langchain_ollama import OllamaEmbeddings

embeddings = OllamaEmbeddings(model="llama3.2")

if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)

print(f"Vectorstore created with {vectorstore._collection.count()} documents")
#vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
# Convert loaded documents into strings by concatenating their content
# and ignoring metadata

"""def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

chain = {"docs": format_docs} | prompt | model | StrOutputParser()

question = "Summarize this document"

docs = vectorstore.similarity_search(question)

chain.invoke(docs)
"""
287/20:
# Get one vector and find how many dimensions it has

collection = vectorstore._collection
sample_embedding = collection.get(limit=1, include=["embeddings"])["embeddings"][0]
dimensions = len(sample_embedding)
print(f"The vectors have {dimensions:,} dimensions")
vectorstore.
287/21:
# Get one vector and find how many dimensions it has

collection = vectorstore._collection
sample_embedding = collection.get(limit=1, include=["embeddings"])["embeddings"][0]
dimensions = len(sample_embedding)
print(f"The vectors have {dimensions:,} dimensions")
287/22:
"""llm = ChatOpenAI(temperature=0.7, model_name=MODEL)
retriever = vectorstore.as_retriever
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever)
"""
from langchain_ollama import OllamaLLM

PROMPT_TEMPLATE = """
Answer the question based only on the following context:

{context}

---

Answer the question based on the above context: {question}
"""
query_text = "Summarize this document"
results = vectorstore.similarity_search_with_score(query_text,k = 5)

context_text = "\n\n\n\n".join(doc.page_content for doc, _score in results])
prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
prompt = prompt_template.format(context = context_text,question=query_text)

model = OllamaLLM(model="llama3.2")
response = model.invoke(prompt)


"""
model = "ollama-3.2"
# create a new Chat 
llm = ChatOllama(temperature=0.7, model_name=MODEL)

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)
"""
287/23:
"""llm = ChatOpenAI(temperature=0.7, model_name=MODEL)
retriever = vectorstore.as_retriever
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever)
"""
from langchain_ollama import OllamaLLM

PROMPT_TEMPLATE = """
Answer the question based only on the following context:

{context}

---

Answer the question based on the above context: {question}
"""
query_text = "Summarize this document"
results = vectorstore.similarity_search_with_score(query_text,k = 5)

context_text = "\n\n\n\n".join([doc.page_content for doc, _score in results])
prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
prompt = prompt_template.format(context = context_text,question=query_text)

model = OllamaLLM(model="llama3.2")
response = model.invoke(prompt)


"""
model = "ollama-3.2"
# create a new Chat 
llm = ChatOllama(temperature=0.7, model_name=MODEL)

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)
"""
287/24:
"""llm = ChatOpenAI(temperature=0.7, model_name=MODEL)
retriever = vectorstore.as_retriever
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever)
"""
from langchain_ollama import OllamaLLM

PROMPT_TEMPLATE = """
Answer the question based only on the following context:

{context}

---

Answer the question based on the above context: {question}
"""
query_text = "Summarize this document"
results = vectorstore.similarity_search_with_score(query_text,k = 5)

context_text = "\n\n\n\n".join([doc.page_content for doc, _score in results])
prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
prompt = prompt_template.format(context = context_text,question=query_text)

model = OllamaLLM(model="llama3.2")
response = model.invoke(prompt)


"""
model = "ollama-3.2"
# create a new Chat 
llm = ChatOllama(temperature=0.7, model_name=MODEL)

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)
"""
response
287/25:
vectorstore.similarity_search_with_score(
    query="AI and authors",
    k=1
)
287/26:
"""llm = ChatOpenAI(temperature=0.7, model_name=MODEL)
retriever = vectorstore.as_retriever
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever)
"""
from langchain_ollama import OllamaLLM

PROMPT_TEMPLATE = """
Answer the question based only on the following context:

{context}

---

Answer the question based on the above context: {question}
"""
query_text = "Give examples about RAG evaluation tools"
results = vectorstore.similarity_search_with_score(query_text,k = 5) # The most important part

context_text = "\n\n\n\n".join([doc.page_content for doc, _score in results])
prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
prompt = prompt_template.format(context = context_text,question=query_text)

model = OllamaLLM(model="llama3.2")
response = model.invoke(prompt)


"""
model = "ollama-3.2"
# create a new Chat 
llm = ChatOllama(temperature=0.7, model_name=MODEL)

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)
"""
response
287/27: from IPython.display import Markdown, display, update_display
287/28:
from IPython.display import Markdown, display, update_display
display(Markdown(response))
287/29:
"""llm = ChatOpenAI(temperature=0.7, model_name=MODEL)
retriever = vectorstore.as_retriever
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever)
"""
from langchain_ollama import OllamaLLM

PROMPT_TEMPLATE = """
Answer the question based only on the following context:

{context}

---

Answer the question based on the above context: {question}
"""
query_text = "Give examples about RAG evaluation tools"
results = vectorstore.similarity_search_with_score(query_text,k = 9) # The most important part

context_text = "\n\n\n\n".join([doc.page_content for doc, _score in results])
prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
prompt = prompt_template.format(context = context_text,question=query_text)

model = OllamaLLM(model="llama3.2")
response = model.invoke(prompt)


"""
model = "ollama-3.2"
# create a new Chat 
llm = ChatOllama(temperature=0.7, model_name=MODEL)

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)
"""
287/30:
from IPython.display import Markdown, display, update_display
display(Markdown(response))
287/31:
# imports for langchain

from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
# from langchain_chroma import Chroma
from langchain.vectorstores import FAISS
import numpy as np
from sklearn.manifold import TSNE
import plotly.graph_objects as go
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
287/32:
#text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
text_splitter = RecursiveCharacterTextSplitter(chunk_size = 200, chunk_overlap = 20, seperator='')
chunks = text_splitter.split_documents(langchain_documents)
len(chunks)
287/33:
#text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
text_splitter = RecursiveCharacterTextSplitter(chunk_size = 200, chunk_overlap = 20, seperators=['','\n'])
chunks = text_splitter.split_documents(langchain_documents)
len(chunks)
287/34:
#text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
text_splitter = RecursiveCharacterTextSplitter(chunk_size = 200, chunk_overlap = 20, separators=['','\n'])
chunks = text_splitter.split_documents(langchain_documents)
len(chunks)
287/35:
from langchain_ollama import OllamaEmbeddings

embeddings = OllamaEmbeddings(model="llama3.2")

if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)

print(f"Vectorstore created with {vectorstore._collection.count()} documents")
#vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
# Convert loaded documents into strings by concatenating their content
# and ignoring metadata

"""def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

chain = {"docs": format_docs} | prompt | model | StrOutputParser()

question = "Summarize this document"

docs = vectorstore.similarity_search(question)

chain.invoke(docs)
"""
287/36:
# Get one vector and find how many dimensions it has

collection = vectorstore._collection
sample_embedding = collection.get(limit=1, include=["embeddings"])["embeddings"][0]
dimensions = len(sample_embedding)
print(f"The vectors have {dimensions:,} dimensions")
287/37:
# Get one vector and find how many dimensions it has

collection = vectorstore._collection
sample_embedding = collection.get(limit=1, include=["embeddings"])["embeddings"][0]
dimensions = len(sample_embedding)
print(f"The vectors have {dimensions:,} dimensions")
287/38:
"""llm = ChatOpenAI(temperature=0.7, model_name=MODEL)
retriever = vectorstore.as_retriever
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever)
"""
from langchain_ollama import OllamaLLM

PROMPT_TEMPLATE = """
Answer the question based only on the following context:

{context}

---

Answer the question based on the above context: {question}
"""
query_text = "Give examples about RAG evaluation tools"
results = vectorstore.similarity_search_with_score(query_text,k = 9) # The most important part

context_text = "\n\n\n\n".join([doc.page_content for doc, _score in results])
prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
prompt = prompt_template.format(context = context_text,question=query_text)

model = OllamaLLM(model="llama3.2")
response = model.invoke(prompt)


"""
model = "ollama-3.2"
# create a new Chat 
llm = ChatOllama(temperature=0.7, model_name=MODEL)

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)
"""
287/39:
from IPython.display import Markdown, display, update_display
display(Markdown(response))
287/40:
from IPython.display import Markdown, display, update_display
sources = [doc.metadata.get("authors", None) for doc, _score in results]
    formatted_response = f"Response: {response_text}\nSources: {sources}"
display(Markdown(response))
287/41:
from IPython.display import Markdown, display, update_display
sources = [doc.metadata.get("authors", None) for doc, _score in results]
formatted_response = f"Response: {response_text}\nSources: {sources}"
display(Markdown(response))
287/42:
from IPython.display import Markdown, display, update_display
sources = [doc.metadata.get("authors", None) for doc, _score in results]
formatted_response = f"Response: {response}\nSources: {sources}"
display(Markdown(response))
287/43:
from IPython.display import Markdown, display, update_display
sources = [doc.metadata.get("authors", None) for doc, _score in results]
formatted_response = f"Response: {response}\nSources: {sources}"
print(formatted_response)
287/44:
"""llm = ChatOpenAI(temperature=0.7, model_name=MODEL)
retriever = vectorstore.as_retriever
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever)
"""
from langchain_ollama import OllamaLLM

PROMPT_TEMPLATE = """
Answer the question based only on the following context:

{context}

---

Answer the question based on the above context: {question}
"""
query_text = "Give examples about RAG evaluation tools"
results = vectorstore.similarity_search_with_score(query_text,k = 9) # The most important part

context_text = "\n\n\n\n".join([doc.page_content for doc, _score in results])
prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
prompt = prompt_template.format(context = context_text,question=query_text)

model = OllamaLLM(model="llama3.2")
response = model.invoke(prompt)


"""
model = "ollama-3.2"
# create a new Chat 
llm = ChatOllama(temperature=0.7, model_name=MODEL)

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)
"""
287/45:
from IPython.display import Markdown, display, update_display
sources = [doc.metadata.get("authors", None) for doc, _score in results]
formatted_response = f"Response: {response}\nSources: {sources}"
print(formatted_response)
294/1:
from langchain_ollama import OllamaEmbeddings

def get_embedding_function():
    
    embeddings = OllamaEmbeddings(model="nomic-embed-text")
    
    return embeddings
295/1:
import argparse
import os
import shutil
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain_core.documents import Document
from get_embedding_function import get_embedding_function
from langchain_community.vectorstores import Chroma
295/2:
CHROMA_PATH = "chroma"
DATA_PATH = "data"
295/3:
def main():

    # Check if the database should be cleared (using the --clear flag).
    parser = argparse.ArgumentParser()
    parser.add_argument("--reset", action="store_true", help="Reset the database.")
    args = parser.parse_args()
    if args.reset:
        print("â¨ Clearing Database")
        clear_database()

    # Create (or update) the data store.
    documents = load_documents()
    print(f"Number of documents: {len(documents)}")
    
    print("-------------DOCUMENT-------------------")
    print("ANALYSIS FOR INFORMATION: What is the like of a document?\n")
    print(documents[0]) # what is the like of a document
    
    print("-------------CHUNK-------------------")
    print("ANALYSIS FOR INFORMATION: What is the like of a chunk?\n")
    chunks = split_documents(documents) 
    print(chunks[0]) # what is the like of a chunk
    
    # Print the number of chunks
    print(f"Number of chunks: {len(chunks)}")
    
    add_to_chroma(chunks)
295/4:
def load_documents():
    document_loader = PyPDFDirectoryLoader(DATA_PATH)
    return document_loader.load()
295/5:
#SPLITTING DOCUMENTS WITH CHUNKS
def split_documents(documents: list[Document]):
    
    #Give the text splitter the documents and it will return the chunks

    # split by character
    text_splitter = CharacterTextSplitter(
        separator=" ",
        chunk_size=600,
        chunk_overlap=80,
        length_function=len,
        is_separator_regex=False,
    )
    
    # Recursive character-based chunks
    '''
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=80,
        length_function=len,
        is_separator_regex=False,
    )
    '''
    return text_splitter.split_documents(documents)
295/6:
def add_to_chroma(chunks: list[Document]):
    # Load the existing database.
    db = Chroma(
        persist_directory=CHROMA_PATH, embedding_function=get_embedding_function()
    )

    # Calculate Page IDs.
    chunks_with_ids = calculate_chunk_ids(chunks)

    # Add or Update the documents.
    existing_items = db.get(include=[])  # IDs are always included by default
    existing_ids = set(existing_items["ids"])
    print(f"Number of existing documents in DB: {len(existing_ids)}")

    # Only add documents that don't exist in the DB.
    new_chunks = []
    for chunk in chunks_with_ids:
        if chunk.metadata["id"] not in existing_ids:
            new_chunks.append(chunk)

    if len(new_chunks):
        print(f"ð Adding new documents: {len(new_chunks)}")
        new_chunk_ids = [chunk.metadata["id"] for chunk in new_chunks]
        db.add_documents(new_chunks, ids=new_chunk_ids)
    else:
        print("â No new documents to add")
295/7:
def calculate_chunk_ids(chunks):

    # This will create IDs like "data/monopoly.pdf:6:2"
    # Page Source : Page Number : Chunk Index

    last_page_id = None
    current_chunk_index = 0

    for chunk in chunks:
        source = chunk.metadata.get("source")
        page = chunk.metadata.get("page")
        current_page_id = f"{source}:{page}"

        # If the page ID is the same as the last one, increment the index.
        if current_page_id == last_page_id:
            current_chunk_index += 1
        else:
            current_chunk_index = 0

        # Calculate the chunk ID.
        chunk_id = f"{current_page_id}:{current_chunk_index}"
        last_page_id = current_page_id

        # Add it to the page meta-data.
        chunk.metadata["id"] = chunk_id

    return chunks
295/8:
def clear_database():
    if os.path.exists(CHROMA_PATH):
        shutil.rmtree(CHROMA_PATH)
295/9:
if __name__ == "__main__":
    main()
296/1:
import argparse
from langchain.vectorstores import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain_ollama import OllamaLLM

from get_embedding_function import get_embedding_function
296/2:
CHROMA_PATH = "chroma"

PROMPT_TEMPLATE = """
Answer the question based only on the following context:

{context}

---

Answer the question based on the above context: {question}
"""
296/3:
def main():
    # Create CLI.
    parser = argparse.ArgumentParser()
    parser.add_argument("query_text", type=str, help="The query text.")
    args = parser.parse_args()
    query_text = args.query_text
    query_rag(query_text)
296/4:
def query_rag(query_text: str):
    # Prepare the DB.
    embedding_function = get_embedding_function()
    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)

    # Search the DB.
    results = db.similarity_search_with_score(query_text, k=5)

    context_text = "\n\n---\n\n".join([doc.page_content for doc, _score in results])
    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
    prompt = prompt_template.format(context=context_text, question=query_text)
    # print(prompt)

    model = OllamaLLM(model="llama3")
    response_text = model.invoke(prompt)

    sources = [doc.metadata.get("id", None) for doc, _score in results]
    formatted_response = f"Response: {response_text}\nSources: {sources}"
    print(formatted_response)
    return response_text
296/5:
if __name__ == "__main__":
    main()
296/6:
import argparse
from langchain.vectorstores import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain_ollama import OllamaLLM

from get_embedding_function.ipynb import get_embedding_function
296/7:
import argparse
from langchain.vectorstores import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain_ollama import OllamaLLM
from get_embedding_function import get_embedding_function
296/8:
import argparse
from langchain.vectorstores import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain_ollama import OllamaLLM
from get_embedding_function import get_embedding_function
296/9:
CHROMA_PATH = "chroma"

PROMPT_TEMPLATE = """
Answer the question based only on the following context:

{context}

---

Answer the question based on the above context: {question}
"""
296/10:
def main():
    # Create CLI.
    parser = argparse.ArgumentParser()
    parser.add_argument("query_text", type=str, help="The query text.")
    args = parser.parse_args()
    query_text = args.query_text
    query_rag(query_text)
296/11:
def query_rag(query_text: str):
    # Prepare the DB.
    embedding_function = get_embedding_function()
    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)

    # Search the DB.
    results = db.similarity_search_with_score(query_text, k=5)

    context_text = "\n\n---\n\n".join([doc.page_content for doc, _score in results])
    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
    prompt = prompt_template.format(context=context_text, question=query_text)
    # print(prompt)

    model = OllamaLLM(model="llama3")
    response_text = model.invoke(prompt)

    sources = [doc.metadata.get("id", None) for doc, _score in results]
    formatted_response = f"Response: {response_text}\nSources: {sources}"
    print(formatted_response)
    return response_text
296/12:
if __name__ == "__main__":
    main()
297/1:
import argparse
import os
import shutil
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain_core.documents import Document
from get_embedding_function import get_embedding_function
from langchain_community.vectorstores import Chroma
297/2:
import argparse
import os
import shutil
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain_core.documents import Document
from get_embedding_function.ipynb import get_embedding_function
from langchain_community.vectorstores import Chroma
298/1:
from langchain_ollama import OllamaEmbeddings

def get_embedding_function():
    
    embeddings = OllamaEmbeddings(model="nomic-embed-text")
    
    return embeddings
300/1:
from langchain_ollama import OllamaEmbeddings

def get_embedding_function():
    
    embeddings = OllamaEmbeddings(model="nomic-embed-text")
    
    return embeddings
300/2:
from langchain_ollama import OllamaEmbeddings

def get_embedding_function():
    
    embeddings = OllamaEmbeddings(model="nomic-embed-text")
    
    return embeddings
299/1:
from langchain_ollama import ChatOllama

model = ChatOllama(
    model="llama3.2",
)

# create a new Chat with OpenAI

memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)
302/1:
import os
import glob
from dotenv import load_dotenv
302/2:
# imports for langchain

from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
# from langchain_chroma import Chroma
from langchain.vectorstores import FAISS
import numpy as np
from sklearn.manifold import TSNE
import plotly.graph_objects as go
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
302/3:
# imports for langchain, plotly and Chroma

from langchain.prompts import ChatPromptTemplate
from langchain_community.llms.ollama import Ollama
from langchain_community.vectorstores import Chroma
302/4:
#MODEL = Ollama(model="llama3.2")
MODEL = "gpt-4o-mini"
db_name = "vector-database"
302/5: folders = glob.glob("example/*")
302/6: text_loader_kwargs = {'encoding': 'utf-8'}
302/7:
import jsonlines
with jsonlines.open("articles-Copy1.json","r") as documents:
    print(next(iter(documents)))
302/8: len(langchain_documents), langchain_documents[:1]
302/9:
langchain_documents = []

with jsonlines.open("articles.json", "r") as metadata_file:
    for file_path, metadata in zip(folders, metadata_file):
        try:
            pdf_loader = PyPDFLoader(file_path)
            pdf_documents = pdf_loader.load()

            for pdf_doc in pdf_documents:
            # Metadata'yÄ± dÃ¶kÃ¼mana ekle
                pdf_doc.metadata.update({
                    "url": metadata.get("url"),
                    "tags":", ".join(metadata.get("tags")),
                    "title": metadata.get("title"),
                    "authors":", ".join(metadata.get("authors"))
                }
                )

            # DÃ¶kÃ¼manÄ± listeye ekle
                langchain_documents.append( Document(
                        page_content=pdf_doc.page_content,
                        metadata=pdf_doc.metadata
                    )
                )
        except Exception as e:
            print(f"Error loading file {file_path}: {e}")

            metadata["tags"] = ", ".join(['Large Language Model', 'Telecom', 'Article', '5G'])
302/10:
import os
import glob
from dotenv import load_dotenv
302/11:
# imports for langchain

from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
# from langchain_chroma import Chroma
from langchain.vectorstores import FAISS
import numpy as np
from sklearn.manifold import TSNE
import plotly.graph_objects as go
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
302/12:
# imports for langchain, plotly and Chroma

from langchain.prompts import ChatPromptTemplate
from langchain_community.llms.ollama import Ollama
from langchain_community.vectorstores import Chroma
302/13:
#MODEL = Ollama(model="llama3.2")
MODEL = "gpt-4o-mini"
db_name = "vector-database"
302/14: folders = glob.glob("example/*")
302/15: text_loader_kwargs = {'encoding': 'utf-8'}
302/16:
import jsonlines
with jsonlines.open("articles-Copy1.json","r") as documents:
    print(next(iter(documents)))
302/17:
from langchain.document_loaders import PyPDFLoader
from langchain_core.documents import Document
folders = glob.glob("example/*")
302/18:
langchain_documents = []

with jsonlines.open("articles.json", "r") as metadata_file:
    for file_path, metadata in zip(folders, metadata_file):
        try:
            pdf_loader = PyPDFLoader(file_path)
            pdf_documents = pdf_loader.load()

            for pdf_doc in pdf_documents:
            # Metadata'yÄ± dÃ¶kÃ¼mana ekle
                pdf_doc.metadata.update({
                    "url": metadata.get("url"),
                    "tags":", ".join(metadata.get("tags")),
                    "title": metadata.get("title"),
                    "authors":", ".join(metadata.get("authors"))
                }
                )

            # DÃ¶kÃ¼manÄ± listeye ekle
                langchain_documents.append( Document(
                        page_content=pdf_doc.page_content,
                        metadata=pdf_doc.metadata
                    )
                )
        except Exception as e:
            print(f"Error loading file {file_path}: {e}")

            metadata["tags"] = ", ".join(['Large Language Model', 'Telecom', 'Article', '5G'])
302/19:
langchain_documents = []

with jsonlines.open("articles.json", "r") as metadata_file:
    for file_path, metadata in zip(folders, metadata_file):
        try:
            pdf_loader = PyPDFLoader(file_path)
            pdf_documents = pdf_loader.load()

            for pdf_doc in pdf_documents:
            # Metadata'yÄ± dÃ¶kÃ¼mana ekle
                pdf_doc.metadata.update({
                    "url": metadata.get("url"),
                    "tags":", ".join(metadata.get("tags")),
                    "title": metadata.get("title"),
                    "authors":", ".join(metadata.get("authors"))
                }
                )

            # DÃ¶kÃ¼manÄ± listeye ekle
                langchain_documents.append( Document(
                        page_content=pdf_doc.page_content,
                        metadata=pdf_doc.metadata
                    )
                )
        except Exception as e:
            print(f"Error loading file {file_path}: {e}")

            metadata["tags"] = ", ".join(['Large Language Model', 'Telecom', 'Article', '5G'])
302/20:
from langchain.document_loaders import PyPDFLoader
from langchain_core.documents import Document
folders = glob.glob("example/*")
302/21:
langchain_documents = []

with jsonlines.open("articles.json", "r") as metadata_file:
    for file_path, metadata in zip(folders, metadata_file):
        try:
            pdf_loader = PyPDFLoader(file_path)
            pdf_documents = pdf_loader.load()

            for pdf_doc in pdf_documents:
            # Metadata'yÄ± dÃ¶kÃ¼mana ekle
                pdf_doc.metadata.update({
                    "url": metadata.get("url"),
                    "tags":", ".join(metadata.get("tags")),
                    "title": metadata.get("title"),
                    "authors":", ".join(metadata.get("authors"))
                }
                )

            # DÃ¶kÃ¼manÄ± listeye ekle
                langchain_documents.append( Document(
                        page_content=pdf_doc.page_content,
                        metadata=pdf_doc.metadata
                    )
                )
        except Exception as e:
            print(f"Error loading file {file_path}: {e}")

            metadata["tags"] = ", ".join(['Large Language Model', 'Telecom', 'Article', '5G'])
302/22: len(langchain_documents), langchain_documents[:1]
302/23:
from langchain.vectorstores import Qdrant
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
302/24:
'''from langchain_ollama import OllamaEmbeddings

# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
embeddings = OpenAIEmbeddings()
'''
302/25:
#text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
text_splitter = RecursiveCharacterTextSplitter(chunk_size = 200, chunk_overlap = 20, separators=[])
chunks = text_splitter.split_documents(langchain_documents)
len(chunks)
302/26:
from langchain_ollama import OllamaEmbeddings

embeddings = OllamaEmbeddings(model="llama3.2")

if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)

print(f"Vectorstore created with {vectorstore._collection.count()} documents")
#vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
# Convert loaded documents into strings by concatenating their content
# and ignoring metadata

"""def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

chain = {"docs": format_docs} | prompt | model | StrOutputParser()

question = "Summarize this document"

docs = vectorstore.similarity_search(question)

chain.invoke(docs)
"""
302/27:
# Get one vector and find how many dimensions it has

collection = vectorstore._collection
sample_embedding = collection.get(limit=1, include=["embeddings"])["embeddings"][0]
dimensions = len(sample_embedding)
print(f"The vectors have {dimensions:,} dimensions")
302/28:
"""llm = ChatOpenAI(temperature=0.7, model_name=MODEL)
retriever = vectorstore.as_retriever
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever)
"""
from langchain_ollama import OllamaLLM

PROMPT_TEMPLATE = """
Answer the question based only on the following context:

{context}

---

Answer the question based on the above context: {question}
"""
query_text = "What is Time Sensitive Networking architecture"
results = vectorstore.similarity_search_with_score(query_text,k = 9) # The most important part

context_text = "\n\n\n\n".join([doc.page_content for doc, _score in results])
prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
prompt = prompt_template.format(context = context_text,question=query_text)

model = OllamaLLM(model="llama3.2")
response = model.invoke(prompt)


"""
model = "ollama-3.2"
# create a new Chat 
llm = ChatOllama(temperature=0.7, model_name=MODEL)

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)
"""
302/29: response
302/30:
"""llm = ChatOpenAI(temperature=0.7, model_name=MODEL)
retriever = vectorstore.as_retriever
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever)
"""
from langchain_ollama import OllamaLLM
from langchain_core.output_parsers import JsonOutputParser

#PROMPT_TEMPLATE = """
#Answer the question based only on the following context:

#{context}

#---

#Answer the question based on the above context: {question}
#"""
#query_text = "What is Time Sensitive Networking architecture"
#results = vectorstore.similarity_search_with_score(query_text,k = 9) # The most important part

context_text = "\n\n\n\n".join([doc.page_content for doc, _score in results])
llm = ChatOllama(model="llama3.2", format="json", temperature=0)
prompt = PromptTemplate(
    template="""<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance 
    of a retrieved document to a user question. If the document contains keywords related to the user question, 
    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \n
    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \n
    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.
     <|eot_id|><|start_header_id|>user<|end_header_id|>
    Here is the retrieved document: \n\n {document} \n\n
    Here is the user question: {question} \n <|eot_id|><|start_header_id|>assistant<|end_header_id|>
    """,
    input_variables=["question", "document"],
)
retrieval_grader = prompt | llm | JsonOutputParser()
question = "agent memory"
docs = retriever.invoke(question)
doc_txt = docs[1].page_content
prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
prompt = prompt_template.format(context = context_text,question=query_text)



response = model.invoke(prompt)
docs = retriever.invoke(question)

"""
model = "ollama-3.2"
# create a new Chat 
llm = ChatOllama(temperature=0.7, model_name=MODEL)

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)
"""
302/31: from langchain_ollama import ChatOllama
302/32:
"""llm = ChatOpenAI(temperature=0.7, model_name=MODEL)
retriever = vectorstore.as_retriever
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever)
"""
from langchain_ollama import OllamaLLM
from langchain_core.output_parsers import JsonOutputParser

#PROMPT_TEMPLATE = """
#Answer the question based only on the following context:

#{context}

#---

#Answer the question based on the above context: {question}
#"""
#query_text = "What is Time Sensitive Networking architecture"
#results = vectorstore.similarity_search_with_score(query_text,k = 9) # The most important part

#context_text = "\n\n\n\n".join([doc.page_content for doc, _score in results])
llm = ChatOllama(model="llama3.2", format="json", temperature=0)
prompt = PromptTemplate(
    template="""<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance 
    of a retrieved document to a user question. If the document contains keywords related to the user question, 
    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \n
    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \n
    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.
     <|eot_id|><|start_header_id|>user<|end_header_id|>
    Here is the retrieved document: \n\n {document} \n\n
    Here is the user question: {question} \n <|eot_id|><|start_header_id|>assistant<|end_header_id|>
    """,
    input_variables=["question", "document"],
)
retrieval_grader = prompt | llm | JsonOutputParser()
question = "agent memory"
docs = retriever.invoke(question)
doc_txt = docs[1].page_content
print(retrieval_grader.invoke({"question": question, "document": doc_txt}))
#prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
#prompt = prompt_template.format(context = context_text,question=query_text)



response = model.invoke(prompt)
docs = retriever.invoke(question)

"""
model = "ollama-3.2"
# create a new Chat 
llm = ChatOllama(temperature=0.7, model_name=MODEL)

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)
"""
302/33:
"""llm = ChatOpenAI(temperature=0.7, model_name=MODEL)
retriever = vectorstore.as_retriever
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever)
"""
from langchain_ollama import OllamaLLM
from langchain_core.output_parsers import JsonOutputParser
from langchain.prompts import PromptTemplate

#PROMPT_TEMPLATE = """
#Answer the question based only on the following context:

#{context}

#---

#Answer the question based on the above context: {question}
#"""
#query_text = "What is Time Sensitive Networking architecture"
#results = vectorstore.similarity_search_with_score(query_text,k = 9) # The most important part

#context_text = "\n\n\n\n".join([doc.page_content for doc, _score in results])
llm = ChatOllama(model="llama3.2", format="json", temperature=0)
prompt = PromptTemplate(
    template="""<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance 
    of a retrieved document to a user question. If the document contains keywords related to the user question, 
    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \n
    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \n
    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.
     <|eot_id|><|start_header_id|>user<|end_header_id|>
    Here is the retrieved document: \n\n {document} \n\n
    Here is the user question: {question} \n <|eot_id|><|start_header_id|>assistant<|end_header_id|>
    """,
    input_variables=["question", "document"],
)
retrieval_grader = prompt | llm | JsonOutputParser()
question = "agent memory"
docs = retriever.invoke(question)
doc_txt = docs[1].page_content
print(retrieval_grader.invoke({"question": question, "document": doc_txt}))
#prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
#prompt = prompt_template.format(context = context_text,question=query_text)



response = model.invoke(prompt)
docs = retriever.invoke(question)

"""
model = "ollama-3.2"
# create a new Chat 
llm = ChatOllama(temperature=0.7, model_name=MODEL)

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)
"""
302/34:
import os
import glob
from dotenv import load_dotenv
302/35:
# imports for langchain

from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
# from langchain_chroma import Chroma
from langchain.vectorstores import FAISS
import numpy as np
from sklearn.manifold import TSNE
import plotly.graph_objects as go
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
302/36:
# imports for langchain, plotly and Chroma

from langchain.prompts import ChatPromptTemplate
from langchain_community.llms.ollama import Ollama
from langchain_community.vectorstores import Chroma
302/37:
#MODEL = Ollama(model="llama3.2")
MODEL = "gpt-4o-mini"
db_name = "vector-database"
302/38: folders = glob.glob("example/*")
302/39: text_loader_kwargs = {'encoding': 'utf-8'}
302/40:
import jsonlines
with jsonlines.open("articles-Copy1.json","r") as documents:
    print(next(iter(documents)))
302/41:
from langchain.document_loaders import PyPDFLoader
from langchain_core.documents import Document
folders = glob.glob("example/*")
302/42:
langchain_documents = []

with jsonlines.open("articles.json", "r") as metadata_file:
    for file_path, metadata in zip(folders, metadata_file):
        try:
            pdf_loader = PyPDFLoader(file_path)
            pdf_documents = pdf_loader.load()

            for pdf_doc in pdf_documents:
            # Metadata'yÄ± dÃ¶kÃ¼mana ekle
                pdf_doc.metadata.update({
                    "url": metadata.get("url"),
                    "tags":", ".join(metadata.get("tags")),
                    "title": metadata.get("title"),
                    "authors":", ".join(metadata.get("authors"))
                }
                )

            # DÃ¶kÃ¼manÄ± listeye ekle
                langchain_documents.append( Document(
                        page_content=pdf_doc.page_content,
                        metadata=pdf_doc.metadata
                    )
                )
        except Exception as e:
            print(f"Error loading file {file_path}: {e}")

            metadata["tags"] = ", ".join(['Large Language Model', 'Telecom', 'Article', '5G'])
302/43:
langchain_documents = []

with jsonlines.open("articles-Copy1.json.json", "r") as metadata_file:
    for file_path, metadata in zip(folders, metadata_file):
        try:
            pdf_loader = PyPDFLoader(file_path)
            pdf_documents = pdf_loader.load()

            for pdf_doc in pdf_documents:
            # Metadata'yÄ± dÃ¶kÃ¼mana ekle
                pdf_doc.metadata.update({
                    "url": metadata.get("url"),
                    "tags":", ".join(metadata.get("tags")),
                    "title": metadata.get("title"),
                    "authors":", ".join(metadata.get("authors"))
                }
                )

            # DÃ¶kÃ¼manÄ± listeye ekle
                langchain_documents.append( Document(
                        page_content=pdf_doc.page_content,
                        metadata=pdf_doc.metadata
                    )
                )
        except Exception as e:
            print(f"Error loading file {file_path}: {e}")

            metadata["tags"] = ", ".join(['Large Language Model', 'Telecom', 'Article', '5G'])
302/44:
langchain_documents = []

with jsonlines.open("articles-Copy1.json", "r") as metadata_file:
    for file_path, metadata in zip(folders, metadata_file):
        try:
            pdf_loader = PyPDFLoader(file_path)
            pdf_documents = pdf_loader.load()

            for pdf_doc in pdf_documents:
            # Metadata'yÄ± dÃ¶kÃ¼mana ekle
                pdf_doc.metadata.update({
                    "url": metadata.get("url"),
                    "tags":", ".join(metadata.get("tags")),
                    "title": metadata.get("title"),
                    "authors":", ".join(metadata.get("authors"))
                }
                )

            # DÃ¶kÃ¼manÄ± listeye ekle
                langchain_documents.append( Document(
                        page_content=pdf_doc.page_content,
                        metadata=pdf_doc.metadata
                    )
                )
        except Exception as e:
            print(f"Error loading file {file_path}: {e}")

            metadata["tags"] = ", ".join(['Large Language Model', 'Telecom', 'Article', '5G'])
302/45: len(langchain_documents), langchain_documents[:1]
302/46: len(langchain_documents), langchain_documents[:1]
302/47:
from langchain.vectorstores import Qdrant
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
302/48:
'''from langchain_ollama import OllamaEmbeddings

# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
embeddings = OpenAIEmbeddings()
'''
302/49:
#text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
text_splitter = RecursiveCharacterTextSplitter(chunk_size = 200, chunk_overlap = 20, separators=[])
chunks = text_splitter.split_documents(langchain_documents)
len(chunks)
302/50:
from langchain_ollama import OllamaEmbeddings

embeddings = OllamaEmbeddings(model="llama3.2")

if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)

print(f"Vectorstore created with {vectorstore._collection.count()} documents")
#vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
# Convert loaded documents into strings by concatenating their content
# and ignoring metadata

"""def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

chain = {"docs": format_docs} | prompt | model | StrOutputParser()

question = "Summarize this document"

docs = vectorstore.similarity_search(question)

chain.invoke(docs)
"""
302/51:
# Get one vector and find how many dimensions it has

collection = vectorstore._collection
sample_embedding = collection.get(limit=1, include=["embeddings"])["embeddings"][0]
dimensions = len(sample_embedding)
print(f"The vectors have {dimensions:,} dimensions")
retriever = vectorstore.as_retriever()
302/52: from langchain_ollama import ChatOllama
302/53:
"""llm = ChatOpenAI(temperature=0.7, model_name=MODEL)
retriever = vectorstore.as_retriever
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever)
"""
from langchain_ollama import OllamaLLM
from langchain_core.output_parsers import JsonOutputParser
from langchain.prompts import PromptTemplate

#PROMPT_TEMPLATE = """
#Answer the question based only on the following context:

#{context}

#---

#Answer the question based on the above context: {question}
#"""
#query_text = "What is Time Sensitive Networking architecture"
#results = vectorstore.similarity_search_with_score(query_text,k = 9) # The most important part
retriever = vectorstore.as_retriever()
#context_text = "\n\n\n\n".join([doc.page_content for doc, _score in results])
llm = ChatOllama(model="llama3.2", format="json", temperature=0)
prompt = PromptTemplate(
    template="""<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance 
    of a retrieved document to a user question. If the document contains keywords related to the user question, 
    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \n
    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \n
    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.
     <|eot_id|><|start_header_id|>user<|end_header_id|>
    Here is the retrieved document: \n\n {document} \n\n
    Here is the user question: {question} \n <|eot_id|><|start_header_id|>assistant<|end_header_id|>
    """,
    input_variables=["question", "document"],
)
retrieval_grader = prompt | llm | JsonOutputParser()
question = "agent memory"
docs = retriever.invoke(question)
doc_txt = docs[1].page_content
print(retrieval_grader.invoke({"question": question, "document": doc_txt}))
#prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
#prompt = prompt_template.format(context = context_text,question=query_text)



response = model.invoke(prompt)
docs = retriever.invoke(question)

"""
model = "ollama-3.2"
# create a new Chat 
llm = ChatOllama(temperature=0.7, model_name=MODEL)

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)
"""
302/54:
"""llm = ChatOpenAI(temperature=0.7, model_name=MODEL)
retriever = vectorstore.as_retriever
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever)
"""
from langchain_ollama import OllamaLLM
from langchain_core.output_parsers import JsonOutputParser
from langchain.prompts import PromptTemplate

#PROMPT_TEMPLATE = """
#Answer the question based only on the following context:

#{context}

#---

#Answer the question based on the above context: {question}
#"""
#query_text = "What is Time Sensitive Networking architecture"
#results = vectorstore.similarity_search_with_score(query_text,k = 9) # The most important part
retriever = vectorstore.as_retriever()
#context_text = "\n\n\n\n".join([doc.page_content for doc, _score in results])
llm = ChatOllama(model="llama3.2", format="json", temperature=0)
prompt = PromptTemplate(
    template="""<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance 
    of a retrieved document to a user question. If the document contains keywords related to the user question, 
    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \n
    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \n
    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.
     <|eot_id|><|start_header_id|>user<|end_header_id|>
    Here is the retrieved document: \n\n {document} \n\n
    Here is the user question: {question} \n <|eot_id|><|start_header_id|>assistant<|end_header_id|>
    """,
    input_variables=["question", "document"],
)
retrieval_grader = prompt | llm | JsonOutputParser()
question = "agent memory"
docs = retriever.invoke(question)
doc_txt = docs[1].page_content
print(retrieval_grader.invoke({"question": question, "document": doc_txt}))
#prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
#prompt = prompt_template.format(context = context_text,question=query_text)



#response = model.invoke(prompt)
docs = retriever.invoke(question)

"""
model = "ollama-3.2"
# create a new Chat 
llm = ChatOllama(temperature=0.7, model_name=MODEL)

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)
"""
302/55:
"""llm = ChatOpenAI(temperature=0.7, model_name=MODEL)
retriever = vectorstore.as_retriever
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever)
"""
from langchain_ollama import OllamaLLM
from langchain_core.output_parsers import JsonOutputParser
from langchain.prompts import PromptTemplate

#PROMPT_TEMPLATE = """
#Answer the question based only on the following context:

#{context}

#---

#Answer the question based on the above context: {question}
#"""
#query_text = "What is Time Sensitive Networking architecture"
#results = vectorstore.similarity_search_with_score(query_text,k = 9) # The most important part
retriever = vectorstore.as_retriever()
#context_text = "\n\n\n\n".join([doc.page_content for doc, _score in results])
llm = ChatOllama(model="llama3.2", format="json", temperature=0)
prompt = PromptTemplate(
    template="""<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance 
    of a retrieved document to a user question. If the document contains keywords related to the user question, 
    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \n
    Give a score  between 0 to 1 to indicate whether the document is relevant to the question. \n
    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.
     <|eot_id|><|start_header_id|>user<|end_header_id|>
    Here is the retrieved document: \n\n {document} \n\n
    Here is the user question: {question} \n <|eot_id|><|start_header_id|>assistant<|end_header_id|>
    """,
    input_variables=["question", "document"],
)
retrieval_grader = prompt | llm | JsonOutputParser()
question = "agent memory"
docs = retriever.invoke(question)
doc_txt = docs[1].page_content
print(retrieval_grader.invoke({"question": question, "document": doc_txt}))
#prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
#prompt = prompt_template.format(context = context_text,question=query_text)



#response = model.invoke(prompt)
docs = retriever.invoke(question)

"""
model = "ollama-3.2"
# create a new Chat 
llm = ChatOllama(temperature=0.7, model_name=MODEL)

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)
"""
302/56:
"""llm = ChatOpenAI(temperature=0.7, model_name=MODEL)
retriever = vectorstore.as_retriever
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever)
"""
from langchain_ollama import OllamaLLM
from langchain_core.output_parsers import JsonOutputParser
from langchain.prompts import PromptTemplate

#PROMPT_TEMPLATE = """
#Answer the question based only on the following context:

#{context}

#---

#Answer the question based on the above context: {question}
#"""
#query_text = "What is Time Sensitive Networking architecture"
#results = vectorstore.similarity_search_with_score(query_text,k = 9) # The most important part
retriever = vectorstore.as_retriever()
#context_text = "\n\n\n\n".join([doc.page_content for doc, _score in results])
llm = ChatOllama(model="llama3.2", format="json", temperature=0)
prompt = PromptTemplate(
    template="""<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance 
    of a retrieved document to a user question. If the document contains keywords related to the user question, 
    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \n
    Give a float number score  between 0 to 1 to indicate whether the document is relevant to the question. \n
    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.
     <|eot_id|><|start_header_id|>user<|end_header_id|>
    Here is the retrieved document: \n\n {document} \n\n
    Here is the user question: {question} \n <|eot_id|><|start_header_id|>assistant<|end_header_id|>
    """,
    input_variables=["question", "document"],
)
retrieval_grader = prompt | llm | JsonOutputParser()
question = "What is 5G?"
docs = retriever.invoke(question)
doc_txt = docs[1].page_content
print(retrieval_grader.invoke({"question": question, "document": doc_txt}))
#prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
#prompt = prompt_template.format(context = context_text,question=query_text)



#response = model.invoke(prompt)
docs = retriever.invoke(question)

"""
model = "ollama-3.2"
# create a new Chat 
llm = ChatOllama(temperature=0.7, model_name=MODEL)

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)
"""
302/57:
"""llm = ChatOpenAI(temperature=0.7, model_name=MODEL)
retriever = vectorstore.as_retriever
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever)
"""
from langchain_ollama import OllamaLLM
from langchain_core.output_parsers import JsonOutputParser
from langchain.prompts import PromptTemplate

#PROMPT_TEMPLATE = """
#Answer the question based only on the following context:

#{context}

#---

#Answer the question based on the above context: {question}
#"""
#query_text = "What is Time Sensitive Networking architecture"
#results = vectorstore.similarity_search_with_score(query_text,k = 9) # The most important part
retriever = vectorstore.as_retriever()
#context_text = "\n\n\n\n".join([doc.page_content for doc, _score in results])
llm = ChatOllama(model="llama3.2", format="json", temperature=0)
prompt = PromptTemplate(
    template="""<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance 
    of a retrieved document to a user question. If the document contains keywords related to the user question, 
    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \n
    Give a float number score  between 0 to 1 to indicate whether the document is relevant to the question. \n
    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.
     <|eot_id|><|start_header_id|>user<|end_header_id|>
    Here is the retrieved document: \n\n {document} \n\n
    Here is the user question: {question} \n <|eot_id|><|start_header_id|>assistant<|end_header_id|>
    """,
    input_variables=["question", "document"],
)
retrieval_grader = prompt | llm | JsonOutputParser()
question = "What is Network Function?"
docs = retriever.invoke(question)
doc_txt = docs[1].page_content
print(retrieval_grader.invoke({"question": question, "document": doc_txt}))
#prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
#prompt = prompt_template.format(context = context_text,question=query_text)



#response = model.invoke(prompt)
#docs = retriever.invoke(question)

"""
model = "ollama-3.2"
# create a new Chat 
llm = ChatOllama(temperature=0.7, model_name=MODEL)

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)
"""
302/58:
langchain_documents = []

with jsonlines.open("articles-Copy1.json", "r") as metadata_file:
    for file_path, metadata in zip(folders, metadata_file):
        try:
            pdf_loader = PyPDFLoader(file_path)
            pdf_documents = pdf_loader.load()

            for pdf_doc in pdf_documents:
            # Metadata'yÄ± dÃ¶kÃ¼mana ekle
                pdf_doc.metadata.update({
                    "url": metadata.get("url"),
                    "tags":", ".join(metadata.get("tags")),
                    "title": metadata.get("title"),
                    "authors":", ".join(metadata.get("authors"))
                }
                )

            # DÃ¶kÃ¼manÄ± listeye ekle
                langchain_documents.append( Document(
                        page_content=pdf_doc.page_content,
                        metadata=pdf_doc.metadata
                    )
                )
        except Exception as e:
            print(f"Error loading file {file_path}: {e}")
302/59: len(langchain_documents), langchain_documents[:1]
302/60:
from langchain.vectorstores import Qdrant
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
302/61:
'''from langchain_ollama import OllamaEmbeddings

# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
embeddings = OpenAIEmbeddings()
'''
302/62:
#text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
text_splitter = RecursiveCharacterTextSplitter(chunk_size = 200, chunk_overlap = 20, separators=[])
chunks = text_splitter.split_documents(langchain_documents)
len(chunks)
302/63:
from langchain_ollama import OllamaEmbeddings

embeddings = OllamaEmbeddings(model="llama3.2")

if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)

print(f"Vectorstore created with {vectorstore._collection.count()} documents")
#vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
# Convert loaded documents into strings by concatenating their content
# and ignoring metadata

"""def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

chain = {"docs": format_docs} | prompt | model | StrOutputParser()

question = "Summarize this document"

docs = vectorstore.similarity_search(question)

chain.invoke(docs)
"""
302/64:
# Get one vector and find how many dimensions it has

collection = vectorstore._collection
sample_embedding = collection.get(limit=1, include=["embeddings"])["embeddings"][0]
dimensions = len(sample_embedding)
print(f"The vectors have {dimensions:,} dimensions")
retriever = vectorstore.as_retriever()
302/65: from langchain_ollama import ChatOllama
302/66:
"""llm = ChatOpenAI(temperature=0.7, model_name=MODEL)
retriever = vectorstore.as_retriever
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever)
"""
from langchain_ollama import OllamaLLM
from langchain_core.output_parsers import JsonOutputParser
from langchain.prompts import PromptTemplate

#PROMPT_TEMPLATE = """
#Answer the question based only on the following context:

#{context}

#---

#Answer the question based on the above context: {question}
#"""
#query_text = "What is Time Sensitive Networking architecture"
#results = vectorstore.similarity_search_with_score(query_text,k = 9) # The most important part
retriever = vectorstore.as_retriever()
#context_text = "\n\n\n\n".join([doc.page_content for doc, _score in results])
llm = ChatOllama(model="llama3.2", format="json", temperature=0)
prompt = PromptTemplate(
    template="""<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance 
    of a retrieved document to a user question. If the document contains keywords related to the user question, 
    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \n
    Give a float number score  between 0 to 1 to indicate whether the document is relevant to the question. \n
    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.
     <|eot_id|><|start_header_id|>user<|end_header_id|>
    Here is the retrieved document: \n\n {document} \n\n
    Here is the user question: {question} \n <|eot_id|><|start_header_id|>assistant<|end_header_id|>
    """,
    input_variables=["question", "document"],
)
retrieval_grader = prompt | llm | JsonOutputParser()
question = "What is Network Function?"
docs = retriever.invoke(question)
doc_txt = docs[1].page_content
print(retrieval_grader.invoke({"question": question, "document": doc_txt}))
#prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
#prompt = prompt_template.format(context = context_text,question=query_text)



#response = model.invoke(prompt)
#docs = retriever.invoke(question)

"""
model = "ollama-3.2"
# create a new Chat 
llm = ChatOllama(temperature=0.7, model_name=MODEL)

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)
"""
315/1:
import time
class Bank:
    amount = int(input("Write your initial amount")
    
    def __init__(self,balance):
        self.balance = balance
        assert >= 0

    def withdraw(amount):
        assert amount >= 0
        print("Withdrawing...")
        time.sleep(1)
        if amount <= self.balance:
            self.balance -= amount
                    
        else:
            print("Amount must be smaller than your balance")
            exit()
            
    def deposit(amount):
        balance+=amount
315/2:
import time
class Bank:
    amount = int(input("Write your initial amount"))
    
    def __init__(self,balance):
        self.balance = balance
        assert >= 0

    def withdraw(amount):
        assert amount >= 0
        print("Withdrawing...")
        time.sleep(1)
        if amount <= self.balance:
            self.balance -= amount
                    
        else:
            print("Amount must be smaller than your balance")
            exit()
            
    def deposit(amount):
        balance+=amount
317/1:
# imports

import os
import io
import sys
from dotenv import load_dotenv
from openai import OpenAI
import google.generativeai
import anthropic
from IPython.display import Markdown, display, update_display
import gradio as gr
import subprocess
317/2:
# environment

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
os.environ['ANTHROPIC_API_KEY'] = os.getenv('ANTHROPIC_API_KEY', 'your-key-if-not-using-env')
317/3:
# initialize
# NOTE - option to use ultra-low cost models by uncommenting last 2 lines

openai = OpenAI()
claude = anthropic.Anthropic()
OPENAI_MODEL = "gpt-4o"
CLAUDE_MODEL = "claude-3-5-sonnet-20240620"

# Want to keep costs ultra-low? Uncomment these lines:
# OPENAI_MODEL = "gpt-4o-mini"
# CLAUDE_MODEL = "claude-3-haiku-20240307"
322/1:
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain_openai import ChatOpenAI

question = "What are the approaches to Task Decomposition?"
llm = ChatOpenAI(temperature=0)
retriever_from_llm = MultiQueryRetriever.from_llm(
    retriever=vectordb.as_retriever(), llm=llm
)
322/2:
# imports for langchain

from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
# from langchain_chroma import Chroma
from langchain.vectorstores import FAISS
import numpy as np
from sklearn.manifold import TSNE
import plotly.graph_objects as go
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
322/3:
# imports for langchain, plotly and Chroma

from langchain.prompts import ChatPromptTemplate
from langchain_community.llms.ollama import Ollama
from langchain_community.vectorstores import Chroma
322/4:
#MODEL = Ollama(model="llama3.2")
MODEL = "gpt-4o-mini"
db_name = "vector-database"
322/5: folders = glob.glob("example/*")
322/6:
import os
import glob
from dotenv import load_dotenv
322/7: folders = glob.glob("example/*")
322/8: text_loader_kwargs = {'encoding': 'utf-8'}
322/9:
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain_openai import ChatOpenAI

question = "What are the approaches to Task Decomposition?"
llm = ChatOpenAI(temperature=0)
retriever_from_llm = MultiQueryRetriever.from_llm(
    retriever=vectordb.as_retriever(), llm=llm
)
322/10:
# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
322/11:
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain_openai import ChatOpenAI

question = "What are the approaches to Task Decomposition?"
llm = ChatOpenAI(temperature=0)
retriever_from_llm = MultiQueryRetriever.from_llm(
    retriever=vectordb.as_retriever(), llm=llm
)
326/1: import random
326/2:
# Task Class
class Task:
    def __init__(self, task_id, compute_demand, priority):
        self.task_id = task_id                # Unique identifier for the task
        self.compute_demand = compute_demand  # Computational requirement in GHz
        self.priority = priority              # Priority level of the task

# Server Class
class Server:
    def __init__(self, server_id, compute_capacity, server_type):
        self.server_id = server_id                # Unique identifier for the server
        self.compute_capacity = compute_capacity  # Maximum computational capacity
        self.server_type = server_type            # 'Edge' or 'Cloud'
326/3:
def generate_tasks(num_tasks):
    tasks = []
    for i in range(num_tasks):
        task_id = f"T{i}"
        priority = random.randint(0, 2)
        compute_demand = {
            0: random.randint(1, 4),  # Low priority
            1: random.randint(3, 7),  # Medium priority
            2: random.randint(6, 10)  # High priority
        }[priority]
        tasks.append(Task(task_id, compute_demand, priority))
    return tasks
326/4: generate_tasks(20)
326/5: print(generate_tasks(20))
326/6: print(vars(generate_tasks(20)))
326/7:
# Generate servers
def generate_servers(num_servers):
    servers = []
    for i in range(num_servers):
        server_id = f"S{i}"
        if random.random() > 0.5:
            server_type = "Edge"
            compute_capacity = random.randint(8, 12)
        else:
            server_type = "Cloud"
            compute_capacity = random.randint(16, 32)
        servers.append(Server(server_id, compute_capacity, server_type))
    return servers
326/8:
def fitness_function(chromosome, tasks, servers):
    """
    Compute the fitness score for a given chromosome.
    Fitness is the sum of compute_demand for tasks that fit within server capacities.
    Penalize if server capacity is exceeded.
    """
    server_loads = [0] * len(servers)  # Track current loads on servers
    total_fitness = 0  # Total fitness score

    for task_idx, assigned_server in enumerate(chromosome):
        if assigned_server == 1:  # If the task is assigned
            task = tasks[task_idx]
            selected_server_idx = task_idx % len(servers)  # Assign tasks to servers cyclically
            selected_server = servers[selected_server_idx]

            if server_loads[selected_server_idx] + task.compute_demand <= selected_server.compute_capacity:
                # Add task's demand to the server's load
                server_loads[selected_server_idx] += task.compute_demand
                total_fitness += task.compute_demand  # Add task's demand to the fitness
            else:
                # Penalize fitness for exceeding capacity
                total_fitness -= task.compute_demand * 2

    return total_fitness
326/9:

# Initialize population
def initialize_population(pop_size, num_tasks):
    return [random.choices([0, 1], k=num_tasks) for _ in range(pop_size)]
326/10:

# Selection (tournament)
def select_parents(population, fitness_scores):
    tournament = random.sample(range(len(population)), 2)
    return population[max(tournament, key=lambda idx: fitness_scores[idx])]
326/11:
# Crossover (one-point) with crossover rate check
def crossover(parent1, parent2, crossover_rate):
    # Check if a random value is less than the crossover rate
    if random.random() < crossover_rate:
        point = random.randint(1, len(parent1) - 1)  # Random crossover point
        child1 = parent1[:point] + parent2[point:]
        child2 = parent2[:point] + parent1[point:]
        return child1, child2
    else:
        # No crossover, return parents as children
        return parent1, parent2
326/12:
# Mutation
def mutate(chromosome, mutation_rate):
    return [gene if random.random() > mutation_rate else 1 - gene for gene in chromosome]
326/13:
def knapsack(tasks, servers):
    """
    Implements the knapsack algorithm to assign tasks to servers.
    Each task is considered an item with a weight (compute_demand),
    and each server is a knapsack with a capacity (compute_capacity).
    
    Returns a mapping of tasks to servers and the total demand handled.
    """
    # Initialize task-to-server mapping and server loads
    task_to_server = [-1] * len(tasks)  # -1 indicates a task is unassigned
    server_loads = [0] * len(servers)  # Track current loads on servers
    total_demand_handled = 0  # Total compute demand handled successfully
    
    for task_idx, task in enumerate(tasks):
        # Find the best server for the current task
        best_server_idx = -1
        for server_idx, server in enumerate(servers):
            if (server_loads[server_idx] + task.compute_demand <= server.compute_capacity):
                if best_server_idx == -1 or servers[server_idx].compute_capacity - server_loads[server_idx] < servers[best_server_idx].compute_capacity - server_loads[best_server_idx]:
                    best_server_idx = server_idx
        
        # Assign the task to the best server if found
        if best_server_idx != -1:
            server_loads[best_server_idx] += task.compute_demand
            task_to_server[task_idx] = best_server_idx
            total_demand_handled += task.compute_demand
    
    return task_to_server, total_demand_handled
326/14:
# Parameters
num_tasks = 100
num_servers = 10
pop_size = 50
generations = 200
p_m = 1/pop_size
p_x = 0.6

tasks = generate_tasks(num_tasks)
servers = generate_servers(num_servers)
326/15:
print("Sample Tasks:")
for task in tasks[:5]:
    print(vars(task))

print("\nSample Servers:")
for server in servers[:10]:
    print(vars(server))
326/16:

# Generate initial population
population = initialize_population(pop_size, num_tasks)

# Compute fitness scores for the initial population
fitness_scores = [fitness_function(individual, tasks, servers) for individual in population]

# Print fitness scores of the initial population
print("\nFitness Scores:")
for idx, fitness in enumerate(fitness_scores[:5]):
    print(f"Individual {idx}: Fitness = {fitness}")

# Run the knapsack algorithm
task_assignment, total_demand = knapsack(tasks, servers)

# Print the results
print("\nTask Assignment:")
for idx, server_idx in enumerate(task_assignment[:10]):
    print(f"Task {tasks[idx].task_id} assigned to Server {servers[server_idx].server_id if server_idx != -1 else 'Unassigned'}")

print(f"\nTotal Compute Demand Handled: {total_demand}")
326/17:

# Generate initial population
population = initialize_population(pop_size, num_tasks)

# Compute fitness scores for the initial population
fitness_scores = [fitness_function(individual, tasks, servers) for individual in population]

# Print fitness scores of the initial population
print("\nFitness Scores:")
for idx, fitness in enumerate(fitness_scores):
    print(f"Individual {idx}: Fitness = {fitness}")

# Run the knapsack algorithm
task_assignment, total_demand = knapsack(tasks, servers)

# Print the results
print("\nTask Assignment:")
for idx, server_idx in enumerate(task_assignment[:10]):
    print(f"Task {tasks[idx].task_id} assigned to Server {servers[server_idx].server_id if server_idx != -1 else 'Unassigned'}")

print(f"\nTotal Compute Demand Handled: {total_demand}")
326/18:

# Generate initial population
population = initialize_population(pop_size, num_tasks)

# Compute fitness scores for the initial population
fitness_scores = [fitness_function(individual, tasks, servers) for individual in population]

# Print fitness scores of the initial population
print("\nFitness Scores:")
for idx, fitness in enumerate(fitness_scores):
    print(f"Individual {idx}: Fitness = {fitness}")

# Run the knapsack algorithm
task_assignment, total_demand = knapsack(tasks, servers)

# Print the results
print("\nTask Assignment:")
for idx, server_idx in enumerate(task_assignment):
    print(f"Task {tasks[idx].task_id} assigned to Server {servers[server_idx].server_id if server_idx != -1 else 'Unassigned'}")

print(f"\nTotal Compute Demand Handled: {total_demand}")
326/19:
# Parameters
num_tasks = 100
num_servers = 20
pop_size = 50
generations = 200
p_m = 1/pop_size
p_x = 0.6

tasks = generate_tasks(num_tasks)
servers = generate_servers(num_servers)
326/20:

# Generate initial population
population = initialize_population(pop_size, num_tasks)

# Compute fitness scores for the initial population
fitness_scores = [fitness_function(individual, tasks, servers) for individual in population]

# Print fitness scores of the initial population
print("\nFitness Scores:")
for idx, fitness in enumerate(fitness_scores):
    print(f"Individual {idx}: Fitness = {fitness}")

# Run the knapsack algorithm
task_assignment, total_demand = knapsack(tasks, servers)

# Print the results
print("\nTask Assignment:")
for idx, server_idx in enumerate(task_assignment):
    print(f"Task {tasks[idx].task_id} assigned to Server {servers[server_idx].server_id if server_idx != -1 else 'Unassigned'}")

print(f"\nTotal Compute Demand Handled: {total_demand}")
327/1:
import os
import glob
from dotenv import load_dotenv
327/2:
# imports for langchain

from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
# from langchain_chroma import Chroma
from langchain.vectorstores import FAISS
import numpy as np
from sklearn.manifold import TSNE
import plotly.graph_objects as go
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
327/3:
# imports for langchain, plotly and Chroma

from langchain.prompts import ChatPromptTemplate
from langchain_community.llms.ollama import Ollama
from langchain_community.vectorstores import Chroma
327/4:
#MODEL = Ollama(model="llama3.2")
MODEL = "gpt-4o-mini"
db_name = "vector-database"
327/5: folders = glob.glob("example/*")
328/1:
from PyPDF2 import PdfReader
import re

# Load the PDF file
pdf_path = "example/38331-f60.pdf"
reader = PdfReader(pdf_path)

# Function to extract text and split it by headings
def split_pdf_by_headings(pdf_reader, heading_pattern=r"^\d+\.\s[A-Z][^\n]+"):
    """
    Splits PDF text into chunks based on headings.
    :param pdf_reader: PdfReader object.
    :param heading_pattern: Regex pattern for headings.
    :return: List of chunks as (heading, content) tuples.
    """
    chunks = []
    current_heading = None
    current_chunk = []
    
    # Read page by page
    for page in pdf_reader.pages:
        text = page.extract_text()
        lines = text.split("\n")
        
        for line in lines:
            # Check if the line matches the heading pattern
            if re.match(heading_pattern, line):
                # Save the current chunk if it exists
                if current_heading and current_chunk:
                    chunks.append((current_heading, "\n".join(current_chunk)))
                # Start a new chunk
                current_heading = line.strip()
                current_chunk = []
            else:
                # Add line to the current chunk
                current_chunk.append(line.strip())
    
    # Add the last chunk
    if current_heading and current_chunk:
        chunks.append((current_heading, "\n".join(current_chunk)))
    
    return chunks

# Split the PDF into chunks based on headings
heading_chunks = split_pdf_by_headings(reader)

# Display the first few chunks as examples
heading_chunks[:3]  # Show the first 3 chunks
328/2: !pip install PyPDF2
328/3:
from PyPDF2 import PdfReader
import re

# Load the PDF file
pdf_path = "example/38331-f60.pdf"
reader = PdfReader(pdf_path)

# Function to extract text and split it by headings
def split_pdf_by_headings(pdf_reader, heading_pattern=r"^\d+\.\s[A-Z][^\n]+"):
    """
    Splits PDF text into chunks based on headings.
    :param pdf_reader: PdfReader object.
    :param heading_pattern: Regex pattern for headings.
    :return: List of chunks as (heading, content) tuples.
    """
    chunks = []
    current_heading = None
    current_chunk = []
    
    # Read page by page
    for page in pdf_reader.pages:
        text = page.extract_text()
        lines = text.split("\n")
        
        for line in lines:
            # Check if the line matches the heading pattern
            if re.match(heading_pattern, line):
                # Save the current chunk if it exists
                if current_heading and current_chunk:
                    chunks.append((current_heading, "\n".join(current_chunk)))
                # Start a new chunk
                current_heading = line.strip()
                current_chunk = []
            else:
                # Add line to the current chunk
                current_chunk.append(line.strip())
    
    # Add the last chunk
    if current_heading and current_chunk:
        chunks.append((current_heading, "\n".join(current_chunk)))
    
    return chunks

# Split the PDF into chunks based on headings
heading_chunks = split_pdf_by_headings(reader)

# Display the first few chunks as examples
heading_chunks[:3]  # Show the first 3 chunks
328/4:
from PyPDF2 import PdfReader
import re

# Load the PDF file
pdf_path = "example/38331-f60.pdf"
reader = PdfReader(pdf_path)

# Function to extract text and split it by headings
def split_pdf_by_headings(pdf_reader, heading_pattern=r"^\d+\.\s[A-Z][^\n]+"):
    """
    Splits PDF text into chunks based on headings.
    :param pdf_reader: PdfReader object.
    :param heading_pattern: Regex pattern for headings.
    :return: List of chunks as (heading, content) tuples.
    """
    chunks = []
    current_heading = None
    current_chunk = []
    
    # Read page by page
    for page in pdf_reader.pages:
        text = page.extract_text()
        lines = text.split("\n\n")
        
        for line in lines:
            # Check if the line matches the heading pattern
            if re.match(heading_pattern, line):
                # Save the current chunk if it exists
                if current_heading and current_chunk:
                    chunks.append((current_heading, "\n".join(current_chunk)))
                # Start a new chunk
                current_heading = line.strip()
                current_chunk = []
            else:
                # Add line to the current chunk
                current_chunk.append(line.strip())
    
    # Add the last chunk
    if current_heading and current_chunk:
        chunks.append((current_heading, "\n".join(current_chunk)))
    
    return chunks

# Split the PDF into chunks based on headings
heading_chunks = split_pdf_by_headings(reader)

# Display the first few chunks as examples
heading_chunks[:3]  # Show the first 3 chunks
328/5:
from PyPDF2 import PdfReader
import re

# Load the PDF file
pdf_path = "example/38331-f60.pdf"
reader = PdfReader(pdf_path)

# Function to extract text and split it by headings
def split_pdf_by_headings(pdf_reader, heading_pattern=r"^\d+\.\s[A-Z][^\n]+"):
    """
    Splits PDF text into chunks based on headings.
    :param pdf_reader: PdfReader object.
    :param heading_pattern: Regex pattern for headings.
    :return: List of chunks as (heading, content) tuples.
    """
    chunks = []
    current_heading = None
    current_chunk = []
    
    # Read page by page
    for page in pdf_reader.pages:
        text = page.extract_text()
        lines = text.split("\n")
        
        for line in lines:
            # Check if the line matches the heading pattern
            if re.match(heading_pattern, line):
                # Save the current chunk if it exists
                if current_heading and current_chunk:
                    chunks.append((current_heading, "\n".join(current_chunk)))
                # Start a new chunk
                current_heading = line.strip()
                current_chunk = []
            else:
                # Add line to the current chunk
                current_chunk.append(line.strip())
    
    # Add the last chunk
    if current_heading and current_chunk:
        chunks.append((current_heading, "\n".join(current_chunk)))
    
    return chunks

# Split the PDF into chunks based on headings
heading_chunks = split_pdf_by_headings(reader)

# Display the first few chunks as examples
heading_chunks[]  # Show the first 3 chunks
328/6:
from PyPDF2 import PdfReader
import re

# Load the PDF file
pdf_path = "example/38331-f60.pdf"
reader = PdfReader(pdf_path)

# Function to extract text and split it by headings
def split_pdf_by_headings(pdf_reader, heading_pattern=r"^\d+\.\s[A-Z][^\n]+"):
    """
    Splits PDF text into chunks based on headings.
    :param pdf_reader: PdfReader object.
    :param heading_pattern: Regex pattern for headings.
    :return: List of chunks as (heading, content) tuples.
    """
    chunks = []
    current_heading = None
    current_chunk = []
    
    # Read page by page
    for page in pdf_reader.pages:
        text = page.extract_text()
        lines = text.split("\n")
        
        for line in lines:
            # Check if the line matches the heading pattern
            if re.match(heading_pattern, line):
                # Save the current chunk if it exists
                if current_heading and current_chunk:
                    chunks.append((current_heading, "\n".join(current_chunk)))
                # Start a new chunk
                current_heading = line.strip()
                current_chunk = []
            else:
                # Add line to the current chunk
                current_chunk.append(line.strip())
    
    # Add the last chunk
    if current_heading and current_chunk:
        chunks.append((current_heading, "\n".join(current_chunk)))
    
    return chunks

# Split the PDF into chunks based on headings
heading_chunks = split_pdf_by_headings(reader)

# Display the first few chunks as examples
heading_chunks  # Show the first 3 chunks
328/7:
from PyPDF2 import PdfReader
import re

# Load the PDF file
pdf_path = "example/38331-f60.pdf"
reader = PdfReader(pdf_path)

# Function to extract text and split it by headings
def split_pdf_by_headings(pdf_reader, heading_pattern=r"^\d+\.\s[A-Z][^\n]+"):
    """
    Splits PDF text into chunks based on headings.
    :param pdf_reader: PdfReader object.
    :param heading_pattern: Regex pattern for headings.
    :return: List of chunks as (heading, content) tuples.
    """
    chunks = []
    current_heading = None
    current_chunk = []
    
    # Read page by page
    for page in pdf_reader.pages:
        text = page.extract_text()
        lines = text.split("\n")
        
        for line in lines:
            # Check if the line matches the heading pattern
            if re.match(heading_pattern, line):
                # Save the current chunk if it exists
                if current_heading and current_chunk:
                    chunks.append((current_heading, "\n".join(current_chunk)))
                # Start a new chunk
                current_heading = line.strip()
                current_chunk = []
            else:
                # Add line to the current chunk
                current_chunk.append(line.strip())
    
    # Add the last chunk
    if current_heading and current_chunk:
        chunks.append((current_heading, "\n".join(current_chunk)))
    
    return chunks

# Split the PDF into chunks based on headings
heading_chunks = split_pdf_by_headings(reader)

# Display the first few chunks as examples

type(heading_chunks[0])
328/8: type(heading_chunks[0])
328/9:
import os
import glob
from dotenv import load_dotenv
328/10:
#MODEL = Ollama(model="llama3.2")
MODEL = "gpt-4o-mini"
db_name = "vector-database"
328/11:
from PyPDF2 import PdfReader
import re

# Load the PDF file
pdf_path = "example/38304-gb0.pdf"
reader = PdfReader(pdf_path)

# Function to extract text and split it by headings
def split_pdf_by_headings(pdf_reader, heading_pattern=r"^\d+\.\s[A-Z][^\n]+"):
    """
    Splits PDF text into chunks based on headings.
    :param pdf_reader: PdfReader object.
    :param heading_pattern: Regex pattern for headings.
    :return: List of chunks as (heading, content) tuples.
    """
    chunks = []
    current_heading = None
    current_chunk = []
    
    # Read page by page
    for page in pdf_reader.pages:
        text = page.extract_text()
        lines = text.split("\n")
        
        for line in lines:
            # Check if the line matches the heading pattern
            if re.match(heading_pattern, line):
                # Save the current chunk if it exists
                if current_heading and current_chunk:
                    chunks.append((current_heading, "\n".join(current_chunk)))
                # Start a new chunk
                current_heading = line.strip()
                current_chunk = []
            else:
                # Add line to the current chunk
                current_chunk.append(line.strip())
    
    # Add the last chunk
    if current_heading and current_chunk:
        chunks.append((current_heading, "\n".join(current_chunk)))
    
    return chunks

# Split the PDF into chunks based on headings
heading_chunks = split_pdf_by_headings(reader)

# Display the first few chunks as examples

type(heading_chunks[0])
328/12:
from PyPDF2 import PdfReader
import re

# Load the PDF file
pdf_path = "example/38304-gb0.pdf"
reader = PdfReader(pdf_path)

# Function to extract text and split it by headings
def split_pdf_by_headings(pdf_reader, heading_pattern=r"^\d+\.\s[A-Z][^\n]+"):
    """
    Splits PDF text into chunks based on headings.
    :param pdf_reader: PdfReader object.
    :param heading_pattern: Regex pattern for headings.
    :return: List of chunks as (heading, content) tuples.
    """
    chunks = []
    current_heading = None
    current_chunk = []
    
    # Read page by page
    for page in pdf_reader.pages:
        text = page.extract_text()
        lines = text.split("\n")
        
        for line in lines:
            # Check if the line matches the heading pattern
            if re.match(heading_pattern, line):
                # Save the current chunk if it exists
                if current_heading and current_chunk:
                    chunks.append((current_heading, "\n".join(current_chunk)))
                # Start a new chunk
                current_heading = line.strip()
                current_chunk = []
            else:
                # Add line to the current chunk
                current_chunk.append(line.strip())
    
    # Add the last chunk
    if current_heading and current_chunk:
        chunks.append((current_heading, "\n".join(current_chunk)))
    
    return chunks

# Split the PDF into chunks based on headings
heading_chunks = split_pdf_by_headings(reader)

# Display the first few chunks as examples

heading_chunks
328/13:
from PyPDF2 import PdfReader
import re

# Load the PDF file
pdf_path = "example/38304-gb0.pdf"
reader = PdfReader(pdf_path)

# Function to extract text and split it by headings
def split_pdf_by_headings(pdf_reader, heading_pattern = r"^\d+\t+[A-Z][^\n]+"):
    """
    Splits PDF text into chunks based on headings.
    :param pdf_reader: PdfReader object.
    :param heading_pattern: Regex pattern for headings.
    :return: List of chunks as (heading, content) tuples.
    """
    chunks = []
    current_heading = None
    current_chunk = []
    
    # Read page by page
    for page in pdf_reader.pages:
        text = page.extract_text()
        lines = text.split("\n")
        
        for line in lines:
            # Check if the line matches the heading pattern
            if re.match(heading_pattern, line):
                # Save the current chunk if it exists
                if current_heading and current_chunk:
                    chunks.append((current_heading, "\n".join(current_chunk)))
                # Start a new chunk
                current_heading = line.strip()
                current_chunk = []
            else:
                # Add line to the current chunk
                current_chunk.append(line.strip())
    
    # Add the last chunk
    if current_heading and current_chunk:
        chunks.append((current_heading, "\n".join(current_chunk)))
    
    return chunks

# Split the PDF into chunks based on headings
heading_chunks = split_pdf_by_headings(reader)

# Display the first few chunks as examples

heading_chunks
328/14:
from PyPDF2 import PdfReader
import re

# Load the PDF file
pdf_path = "example/38304-gb0.pdf"
reader = PdfReader(pdf_path)
"""
# Function to extract text and split it by headings
def split_pdf_by_headings(pdf_reader, heading_pattern = r"^\d+\t+[A-Z][^\n]+"):
    """
    Splits PDF text into chunks based on headings.
    :param pdf_reader: PdfReader object.
    :param heading_pattern: Regex pattern for headings.
    :return: List of chunks as (heading, content) tuples.
    """
    chunks = []
    current_heading = None
    current_chunk = []
 """   
    # Read page by page
    for page in pdf_reader.pages:
        text = page.extract_text()
        lines = text.split("\n")
"""        
        for line in lines:
            # Check if the line matches the heading pattern
            if re.match(heading_pattern, line):
                # Save the current chunk if it exists
                if current_heading and current_chunk:
                    chunks.append((current_heading, "\n".join(current_chunk)))
                # Start a new chunk
                current_heading = line.strip()
                current_chunk = []
            else:
                # Add line to the current chunk
                current_chunk.append(line.strip())
 """
"""
    # Add the last chunk
    if current_heading and current_chunk:
        chunks.append((current_heading, "\n".join(current_chunk)))
    
    return chunks
"""
"""
# Split the PDF into chunks based on headings
heading_chunks = split_pdf_by_headings(reader)

# Display the first few chunks as examples
"""
heading_chunks
328/15:
from PyPDF2 import PdfReader
import re

# Load the PDF file
pdf_path = "example/38304-gb0.pdf"
reader = PdfReader(pdf_path)
"""
# Function to extract text and split it by headings
def split_pdf_by_headings(pdf_reader, heading_pattern = r"^\d+\t+[A-Z][^\n]+"):
    """
   # Splits PDF text into chunks based on headings.
   # :param pdf_reader: PdfReader object.
   # :param heading_pattern: Regex pattern for headings.
   # :return: List of chunks as (heading, content) tuples.
    """
    chunks = []
    current_heading = None
    current_chunk = []
 """   
    # Read page by page
    for page in pdf_reader.pages:
        text = page.extract_text()
        lines = text.split("\n")
"""        
        for line in lines:
            # Check if the line matches the heading pattern
            if re.match(heading_pattern, line):
                # Save the current chunk if it exists
                if current_heading and current_chunk:
                    chunks.append((current_heading, "\n".join(current_chunk)))
                # Start a new chunk
                current_heading = line.strip()
                current_chunk = []
            else:
                # Add line to the current chunk
                current_chunk.append(line.strip())
 """
"""
    # Add the last chunk
    if current_heading and current_chunk:
        chunks.append((current_heading, "\n".join(current_chunk)))
    
    return chunks
"""
"""
# Split the PDF into chunks based on headings
heading_chunks = split_pdf_by_headings(reader)

# Display the first few chunks as examples
"""
heading_chunks
328/16:
from PyPDF2 import PdfReader
import re

# Load the PDF file
pdf_path = "example/38304-gb0.pdf"
reader = PdfReader(pdf_path)
"""
# Function to extract text and split it by headings
def split_pdf_by_headings(pdf_reader, heading_pattern = r"^\d+\t+[A-Z][^\n]+"):
    """
   # Splits PDF text into chunks based on headings.
   # :param pdf_reader: PdfReader object.
   # :param heading_pattern: Regex pattern for headings.
   # :return: List of chunks as (heading, content) tuples.
    """
    chunks = []
    current_heading = None
    current_chunk = []
"""   
    # Read page by page
    for page in pdf_reader.pages:
        text = page.extract_text()
        lines = text.split("\n")
"""        
        for line in lines:
            # Check if the line matches the heading pattern
            if re.match(heading_pattern, line):
                # Save the current chunk if it exists
                if current_heading and current_chunk:
                    chunks.append((current_heading, "\n".join(current_chunk)))
                # Start a new chunk
                current_heading = line.strip()
                current_chunk = []
            else:
                # Add line to the current chunk
                current_chunk.append(line.strip())
 """
"""
    # Add the last chunk
    if current_heading and current_chunk:
        chunks.append((current_heading, "\n".join(current_chunk)))
    
    return chunks
"""
"""
# Split the PDF into chunks based on headings
heading_chunks = split_pdf_by_headings(reader)

# Display the first few chunks as examples
"""
heading_chunks
328/17:
from PyPDF2 import PdfReader
import re

# Load the PDF file
pdf_path = "example/38304-gb0.pdf"
reader = PdfReader(pdf_path)
"""
# Function to extract text and split it by headings
def split_pdf_by_headings(pdf_reader, heading_pattern = r"^\d+\t+[A-Z][^\n]+"):
    """
   # Splits PDF text into chunks based on headings.
   # :param pdf_reader: PdfReader object.
   # :param heading_pattern: Regex pattern for headings.
   # :return: List of chunks as (heading, content) tuples.
"""
    chunks = []
    current_heading = None
    current_chunk = []
"""   
    # Read page by page
    for page in pdf_reader.pages:
        text = page.extract_text()
        lines = text.split("\n")
"""        
        for line in lines:
            # Check if the line matches the heading pattern
            if re.match(heading_pattern, line):
                # Save the current chunk if it exists
                if current_heading and current_chunk:
                    chunks.append((current_heading, "\n".join(current_chunk)))
                # Start a new chunk
                current_heading = line.strip()
                current_chunk = []
            else:
                # Add line to the current chunk
                current_chunk.append(line.strip())
"""
"""
    # Add the last chunk
    if current_heading and current_chunk:
        chunks.append((current_heading, "\n".join(current_chunk)))
    
    return chunks
"""
"""
# Split the PDF into chunks based on headings
heading_chunks = split_pdf_by_headings(reader)

# Display the first few chunks as examples
"""
heading_chunks
328/18:
from PyPDF2 import PdfReader
import re

# Load the PDF file
pdf_path = "example/38304-gb0.pdf"
reader = PdfReader(pdf_path)
"""
# Function to extract text and split it by headings
def split_pdf_by_headings(pdf_reader, heading_pattern = r"^\d+\t+[A-Z][^\n]+"):
    """
   # Splits PDF text into chunks based on headings.
   # :param pdf_reader: PdfReader object.
   # :param heading_pattern: Regex pattern for headings.
   # :return: List of chunks as (heading, content) tuples.
"""
    chunks = []
    current_heading = None
    current_chunk = []
"""   
    # Read page by page
for page in pdf_reader.pages:
    text = page.extract_text()
    lines = text.split("\n")
"""        
        for line in lines:
            # Check if the line matches the heading pattern
            if re.match(heading_pattern, line):
                # Save the current chunk if it exists
                if current_heading and current_chunk:
                    chunks.append((current_heading, "\n".join(current_chunk)))
                # Start a new chunk
                current_heading = line.strip()
                current_chunk = []
            else:
                # Add line to the current chunk
                current_chunk.append(line.strip())
"""
"""
    # Add the last chunk
    if current_heading and current_chunk:
        chunks.append((current_heading, "\n".join(current_chunk)))
    
    return chunks
"""
"""
# Split the PDF into chunks based on headings
heading_chunks = split_pdf_by_headings(reader)

# Display the first few chunks as examples
"""
heading_chunks
328/19:
from PyPDF2 import PdfReader
import re

# Load the PDF file
pdf_path = "example/38304-gb0.pdf"
reader = PdfReader(pdf_path)
"""
# Function to extract text and split it by headings
def split_pdf_by_headings(pdf_reader, heading_pattern = r"^\d+\t+[A-Z][^\n]+"):
    """
   # Splits PDF text into chunks based on headings.
   # :param pdf_reader: PdfReader object.
   # :param heading_pattern: Regex pattern for headings.
   # :return: List of chunks as (heading, content) tuples.
"""
    chunks = []
    current_heading = None
    current_chunk = []
"""   
    # Read page by page
for page in reader.pages:
    text = page.extract_text()
    lines = text.split("\n")
"""        
        for line in lines:
            # Check if the line matches the heading pattern
            if re.match(heading_pattern, line):
                # Save the current chunk if it exists
                if current_heading and current_chunk:
                    chunks.append((current_heading, "\n".join(current_chunk)))
                # Start a new chunk
                current_heading = line.strip()
                current_chunk = []
            else:
                # Add line to the current chunk
                current_chunk.append(line.strip())
"""
"""
    # Add the last chunk
    if current_heading and current_chunk:
        chunks.append((current_heading, "\n".join(current_chunk)))
    
    return chunks
"""
"""
# Split the PDF into chunks based on headings
heading_chunks = split_pdf_by_headings(reader)

# Display the first few chunks as examples
"""
#heading_chunks
328/20:
from PyPDF2 import PdfReader
import re

# Load the PDF file
pdf_path = "example/38304-gb0.pdf"
reader = PdfReader(pdf_path)
"""
# Function to extract text and split it by headings
def split_pdf_by_headings(pdf_reader, heading_pattern = r"^\d+\t+[A-Z][^\n]+"):
    """
   # Splits PDF text into chunks based on headings.
   # :param pdf_reader: PdfReader object.
   # :param heading_pattern: Regex pattern for headings.
   # :return: List of chunks as (heading, content) tuples.
"""
    chunks = []
    current_heading = None
    current_chunk = []
"""   
    # Read page by page

for page in reader.pages:
    text = page.extract_text()
    lines = text.split("\n")
"""        
        for line in lines:
            # Check if the line matches the heading pattern
            if re.match(heading_pattern, line):
                # Save the current chunk if it exists
                if current_heading and current_chunk:
                    chunks.append((current_heading, "\n".join(current_chunk)))
                # Start a new chunk
                current_heading = line.strip()
                current_chunk = []
            else:
                # Add line to the current chunk
                current_chunk.append(line.strip())
"""
"""
    # Add the last chunk
    if current_heading and current_chunk:
        chunks.append((current_heading, "\n".join(current_chunk)))
    
    return chunks
"""
"""
# Split the PDF into chunks based on headings
heading_chunks = split_pdf_by_headings(reader)

# Display the first few chunks as examples
"""
#heading_chunks
lines
328/21:
from PyPDF2 import PdfReader
import re

# Load the PDF file
pdf_path = "/mnt/data/38331-f60.pdf"
reader = PdfReader(pdf_path)

# Function to extract text and split it by headings
def split_pdf_by_headings(pdf_reader,heading_pattern = r"^\d+\n[A-Z][^\n]+"):
    """
    Splits PDF text into chunks based on headings.
    :param pdf_reader: PdfReader object.
    :param heading_pattern: Regex pattern for headings.
    :return: List of chunks as (heading, content) tuples.
    """
    chunks = []
    current_heading = None
    current_chunk = []
    
    # Read page by page
    for page in pdf_reader.pages:
        text = page.extract_text()
        lines = text.split("\n")
        
        for line in lines:
            # Check if the line matches the heading pattern
            if re.match(heading_pattern, line):
                # Save the current chunk if it exists
                if current_heading and current_chunk:
                    chunks.append((current_heading, "\n".join(current_chunk)))
                # Start a new chunk
                current_heading = line.strip()
                current_chunk = []
            else:
                # Add line to the current chunk
                current_chunk.append(line.strip())
    
    # Add the last chunk
    if current_heading and current_chunk:
        chunks.append((current_heading, "\n".join(current_chunk)))
    
    return chunks

# Split the PDF into chunks based on headings
heading_chunks = split_pdf_by_headings(reader)

# Display the first few chunks as examples
heading_chunks[:3]  # Show the first 3 chunks
328/22:
from PyPDF2 import PdfReader
import re

# Load the PDF file
pdf_path = "example/38331-f60.pdf"
reader = PdfReader(pdf_path)

# Function to extract text and split it by headings
def split_pdf_by_headings(pdf_reader,heading_pattern = r"^\d+\n[A-Z][^\n]+"):
    """
    Splits PDF text into chunks based on headings.
    :param pdf_reader: PdfReader object.
    :param heading_pattern: Regex pattern for headings.
    :return: List of chunks as (heading, content) tuples.
    """
    chunks = []
    current_heading = None
    current_chunk = []
    
    # Read page by page
    for page in pdf_reader.pages:
        text = page.extract_text()
        lines = text.split("\n")
        
        for line in lines:
            # Check if the line matches the heading pattern
            if re.match(heading_pattern, line):
                # Save the current chunk if it exists
                if current_heading and current_chunk:
                    chunks.append((current_heading, "\n".join(current_chunk)))
                # Start a new chunk
                current_heading = line.strip()
                current_chunk = []
            else:
                # Add line to the current chunk
                current_chunk.append(line.strip())
    
    # Add the last chunk
    if current_heading and current_chunk:
        chunks.append((current_heading, "\n".join(current_chunk)))
    
    return chunks

# Split the PDF into chunks based on headings
heading_chunks = split_pdf_by_headings(reader)

# Display the first few chunks as examples
heading_chunks[:3]  # Show the first 3 chunks
328/23:
from PyPDF2 import PdfReader
import re

# Load the PDF file
pdf_path = "example/38304-gb0.pdf"
reader = PdfReader(pdf_path)

# Function to extract text and split it by headings
def split_pdf_by_headings(pdf_reader,heading_pattern = r"^\d+\n[A-Z][^\n]+"):
    """
    Splits PDF text into chunks based on headings.
    :param pdf_reader: PdfReader object.
    :param heading_pattern: Regex pattern for headings.
    :return: List of chunks as (heading, content) tuples.
    """
    chunks = []
    current_heading = None
    current_chunk = []
    
    # Read page by page
    for page in pdf_reader.pages:
        text = page.extract_text()
        lines = text.split("\n")
        
        for line in lines:
            # Check if the line matches the heading pattern
            if re.match(heading_pattern, line):
                # Save the current chunk if it exists
                if current_heading and current_chunk:
                    chunks.append((current_heading, "\n".join(current_chunk)))
                # Start a new chunk
                current_heading = line.strip()
                current_chunk = []
            else:
                # Add line to the current chunk
                current_chunk.append(line.strip())
    
    # Add the last chunk
    if current_heading and current_chunk:
        chunks.append((current_heading, "\n".join(current_chunk)))
    
    return chunks

# Split the PDF into chunks based on headings
heading_chunks = split_pdf_by_headings(reader)

# Display the first few chunks as examples
heading_chunks[:3]  # Show the first 3 chunks
328/24:
from PyPDF2 import PdfReader
import re

# Load the PDF file
pdf_path = "example/38304-gb0.pdf"
reader = PdfReader(pdf_path)

# Function to extract text and split it by headings
def split_pdf_by_headings(pdf_reader,heading_pattern=r"^\d+\.\s[A-Z][^\n]+"):
    """
    Splits PDF text into chunks based on headings.
    :param pdf_reader: PdfReader object.
    :param heading_pattern: Regex pattern for headings.
    :return: List of chunks as (heading, content) tuples.
    """
    chunks = []
    current_heading = None
    current_chunk = []
    
    # Read page by page
    for page in pdf_reader.pages:
        text = page.extract_text()
        lines = text.split("\n")
        
        for line in lines:
            # Check if the line matches the heading pattern
            if re.match(heading_pattern, line):
                # Save the current chunk if it exists
                if current_heading and current_chunk:
                    chunks.append((current_heading, "\n".join(current_chunk)))
                # Start a new chunk
                current_heading = line.strip()
                current_chunk = []
            else:
                # Add line to the current chunk
                current_chunk.append(line.strip())
    
    # Add the last chunk
    if current_heading and current_chunk:
        chunks.append((current_heading, "\n".join(current_chunk)))
    
    return chunks

# Split the PDF into chunks based on headings
heading_chunks = split_pdf_by_headings(reader)

# Display the first few chunks as examples
heading_chunks[:3]  # Show the first 3 chunks
328/25:
from PyPDF2 import PdfReader
import re

# Load the PDF file
pdf_path = "example/38304-gb0.pdf"
reader = PdfReader(pdf_path)

# Function to extract text and split it by headings
def split_pdf_by_headings(pdf_reader,heading_pattern=r"^\d+\.\s[A-Z][^\n]+"):
    """
    Splits PDF text into chunks based on headings.
    :param pdf_reader: PdfReader object.
    :param heading_pattern: Regex pattern for headings.
    :return: List of chunks as (heading, content) tuples.
    """
    chunks = []
    current_heading = None
    current_chunk = []
    
    # Read page by page
    for page in pdf_reader.pages:
        text = page.extract_text()
        lines = text.split("\n")
        
        for line in lines:
            # Check if the line matches the heading pattern
            if re.match(heading_pattern, line):
                # Save the current chunk if it exists
                if current_heading and current_chunk:
                    chunks.append((current_heading, "\n".join(current_chunk)))
                # Start a new chunk
                current_heading = line.strip()
                current_chunk = []
            else:
                # Add line to the current chunk
                current_chunk.append(line.strip())
    
    # Add the last chunk
    if current_heading and current_chunk:
        chunks.append((current_heading, "\n".join(current_chunk)))
    
    return chunks

# Split the PDF into chunks based on headings
heading_chunks = split_pdf_by_headings(reader)

# Display the first few chunks as examples
heading_chunks  # Show the first 3 chunks
328/26: heading_chunks[0]
328/27: heading_chunks
328/28: len(heading_chunks)
328/29: import pymupdf
328/30: !pip install pymupdf
328/31: doc = pymupdf.open(pdf_path)  # or pymupdf.Document(filename)
328/32:
!pip install pymupdf
import pymupdf
328/33: doc = pymupdf.open(pdf_path)  # or pymupdf.Document(filename)
328/34: toc = doc.get_toc()
328/35: toc
328/36: type(toc)
328/37: type(doc)
328/38: doc.page_count
328/39: doc.metadata
328/40: doc.get_page_labels
328/41: doc.get_page_images
328/42:
for page in doc:
    links = page.get_links()
328/43:
link_list = []
for page in doc:
    links = page.get_links()
link_list
328/44:
link_list = []
for page in doc:
    links = page.get_links()
    link_list.append(links)
link_list
328/45:
link_list = []
for page in doc:
    links = page.get_links()
    if links ==[]:
        pass
    link_list.append(links)
328/46:
link_list = []
for page in doc:
    links = page.get_links()
    if links ==[]:
        pass
    link_list.append(links)
link_list
328/47:
link_list = []
for page in doc:
    links = page.get_links()

type(links)
328/48: doc.get_toc
328/49: doc.get_toc()
328/50:
for element in doc.get_toc():
    print(element[1])
328/51: doc.load_page(1)
328/52: print(doc.load_page(1))
328/53: doc.load_page(1)
328/54: doc[0]
328/55: doc.page_text()
328/56: doc[0].get_text()
328/57: doc[1].get_text()
328/58: doc[2].get_text()
328/59: doc[31].get_text()
328/60: doc[31].get_label()
328/61: doc.get_tov()
328/62: doc.get_toc()
328/63:
for item in toc:
    level, heading, page_num = item
    # Metadata formatÄ±nÄ± oluÅtur
    print(item)
328/64: !pip install pymupdf4llm
328/65:
import pymupdf4llm
md_text = pymupdf4llm.to_markdown(pdf_path)
328/66: md_text
328/67:
from IPython.display import Markdown, display, update_display
display(Markdown(md_text))
328/68:
import pymupdf4llm
md_text = pymupdf4llm.to_markdown(pdf_path)
del md_text
328/69: md_text
328/70:
for item in toc:
    level, heading, page_num = item
    # Metadata formatÄ±nÄ± oluÅtur
    metadata = f"{heading} (Page {page_num})"
    page = doc.load_page(page_num - 1)  # 0-indexed
    page.set_text("footer", metadata, fontsize=10, location=(20, 10))  # footer konumu, font boyutu vs.

# PDF'i kaydet
output_pdf_path = "output_with_metadata.pdf"
doc.save(output_pdf_path)
328/71:
from langchain_community.document_loaders import PyMuPDFLoader
loader = PyMuPDFLoader("example/38304-gb0.pdf")
documents = loader.load()
328/72:
for document in documents:
    print(document)
328/73:
for document in documents:
    print(document.page_content=
328/74: doc.get_toc()
328/75: documents.get_toc()
328/76: loader.get_toc()
328/77:
from langchain_community.document_loaders import PyMuPDFLoader
loader = PyMuPDFLoader(pdf_path)
documents = loader.load()
doc = pymupdf.open(pdf_path)  # or pymupdf.Document(filename)
for document in documents:
328/78:
from langchain_community.document_loaders import PyMuPDFLoader
loader = PyMuPDFLoader(pdf_path)
documents = loader.load()
pdf = pymupdf.open(pdf_path)  # or pymupdf.Document(filename)
for document in documents:
    print(document.metadata["page"])
328/79:
from langchain_community.document_loaders import PyMuPDFLoader
loader = PyMuPDFLoader(pdf_path)
documents = loader.load()
pdf = pymupdf.open(pdf_path)  # or pymupdf.Document(filename)
for document in documents:
    document.metadata["page"]+=1
    print(document.metadata["page"]
328/80:
from langchain_community.document_loaders import PyMuPDFLoader
loader = PyMuPDFLoader(pdf_path)
documents = loader.load()
pdf = pymupdf.open(pdf_path)  # or pymupdf.Document(filename)
for document in documents:
    document.metadata["page"]+=1
    print(document.metadata["page"])
328/81:
from langchain_community.document_loaders import PyMuPDFLoader
loader = PyMuPDFLoader(pdf_path)
documents = loader.load()
pdf = pymupdf.open(pdf_path)  # or pymupdf.Document(filename)
for document in documents:
    document.metadata["page"]+=1
    relevant_headings = [item[1] for item in pdf.get_toc() if item[2] == page_number]
    if relevant_headings:
        # Ä°lk bulunan baÅlÄ±ÄÄ± al (birden fazla baÅlÄ±k varsa birleÅtirilebilir)
        document.metadata["heading"] = ", ".join(relevant_headings)
328/82:
from langchain_community.document_loaders import PyMuPDFLoader
loader = PyMuPDFLoader(pdf_path)
documents = loader.load()
pdf = pymupdf.open(pdf_path)  # or pymupdf.Document(filename)
for document in documents:
    document.metadata["page"]+=1
    relevant_headings = [item[1] for item in pdf.get_toc() if item[2] == document.metadata["page"]]
    if relevant_headings:
        # Ä°lk bulunan baÅlÄ±ÄÄ± al (birden fazla baÅlÄ±k varsa birleÅtirilebilir)
        document.metadata["heading"] = ", ".join(relevant_headings)
328/83: document.metadata
328/84: documents[30].metadata
328/85: document.page_content
328/86: documents.metadata
328/87: document.metadata
328/88: documents[24].metadata
328/89: documents[28].metadata
328/90: len(documents)
328/91: documents[:1]
328/92: %pip install -qU matplotlib PyMuPDF pillow
328/93:
import fitz
import matplotlib.patches as patches
import matplotlib.pyplot as plt
from PIL import Image


def plot_pdf_with_boxes(pdf_page, segments):
    pix = pdf_page.get_pixmap()
    pil_image = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

    fig, ax = plt.subplots(1, figsize=(10, 10))
    ax.imshow(pil_image)
    categories = set()
    category_to_color = {
        "Title": "orchid",
        "Image": "forestgreen",
        "Table": "tomato",
    }
    for segment in segments:
        points = segment["coordinates"]["points"]
        layout_width = segment["coordinates"]["layout_width"]
        layout_height = segment["coordinates"]["layout_height"]
        scaled_points = [
            (x * pix.width / layout_width, y * pix.height / layout_height)
            for x, y in points
        ]
        box_color = category_to_color.get(segment["category"], "deepskyblue")
        categories.add(segment["category"])
        rect = patches.Polygon(
            scaled_points, linewidth=1, edgecolor=box_color, facecolor="none"
        )
        ax.add_patch(rect)

    # Make legend
    legend_handles = [patches.Patch(color="deepskyblue", label="Text")]
    for category in ["Title", "Image", "Table"]:
        if category in categories:
            legend_handles.append(
                patches.Patch(color=category_to_color[category], label=category)
            )
    ax.axis("off")
    ax.legend(handles=legend_handles, loc="upper right")
    plt.tight_layout()
    plt.show()


def render_page(doc_list: list, page_number: int, print_text=True) -> None:
    pdf_page = fitz.open(file_path).load_page(page_number - 1)
    page_docs = [
        doc for doc in doc_list if doc.metadata.get("page_number") == page_number
    ]
    segments = [doc.metadata for doc in page_docs]
    plot_pdf_with_boxes(pdf_page, segments)
    if print_text:
        for doc in page_docs:
            print(f"{doc.page_content}\n")
328/94: render_page(documents, 5)
328/95:
import fitz
import matplotlib.patches as patches
import matplotlib.pyplot as plt
from PIL import Image


def plot_pdf_with_boxes(pdf_page, segments):
    pix = pdf_page.get_pixmap()
    pil_image = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

    fig, ax = plt.subplots(1, figsize=(10, 10))
    ax.imshow(pil_image)
    categories = set()
    category_to_color = {
        "Title": "orchid",
        "Image": "forestgreen",
        "Table": "tomato",
    }
    for segment in segments:
        points = segment["coordinates"]["points"]
        layout_width = segment["coordinates"]["layout_width"]
        layout_height = segment["coordinates"]["layout_height"]
        scaled_points = [
            (x * pix.width / layout_width, y * pix.height / layout_height)
            for x, y in points
        ]
        box_color = category_to_color.get(segment["category"], "deepskyblue")
        categories.add(segment["category"])
        rect = patches.Polygon(
            scaled_points, linewidth=1, edgecolor=box_color, facecolor="none"
        )
        ax.add_patch(rect)

    # Make legend
    legend_handles = [patches.Patch(color="deepskyblue", label="Text")]
    for category in ["Title", "Image", "Table"]:
        if category in categories:
            legend_handles.append(
                patches.Patch(color=category_to_color[category], label=category)
            )
    ax.axis("off")
    ax.legend(handles=legend_handles, loc="upper right")
    plt.tight_layout()
    plt.show()


def render_page(doc_list: list, page_number: int, print_text=True) -> None:
    pdf_page = pymupdf.open(pdf_path).load_page(page_number - 1)
    page_docs = [
        doc for doc in doc_list if doc.metadata.get("page_number") == page_number
    ]
    segments = [doc.metadata for doc in page_docs]
    plot_pdf_with_boxes(pdf_page, segments)
    if print_text:
        for doc in page_docs:
            print(f"{doc.page_content}\n")
328/96:
pdf_path = "example/38304-gb0.pdf"
render_page(documents, 5)
328/97: documents[28].metadata.get("page")
328/98:
pdf_path = "example/38304-gb0.pdf"
render_page(documents, 1)
328/99:
import fitz
import matplotlib.patches as patches
import matplotlib.pyplot as plt
from PIL import Image


def plot_pdf_with_boxes(pdf_page, segments):
    pix = pdf_page.get_pixmap()
    pil_image = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

    fig, ax = plt.subplots(1, figsize=(10, 10))
    ax.imshow(pil_image)
    categories = set()
    category_to_color = {
        "Title": "orchid",
        "Image": "forestgreen",
        "Table": "tomato",
    }
    for segment in segments:
        points = segment["coordinates"]["points"]
        layout_width = segment["coordinates"]["layout_width"]
        layout_height = segment["coordinates"]["layout_height"]
        scaled_points = [
            (x * pix.width / layout_width, y * pix.height / layout_height)
            for x, y in points
        ]
        box_color = category_to_color.get(segment["category"], "deepskyblue")
        categories.add(segment["category"])
        rect = patches.Polygon(
            scaled_points, linewidth=1, edgecolor=box_color, facecolor="none"
        )
        ax.add_patch(rect)

    # Make legend
    legend_handles = [patches.Patch(color="deepskyblue", label="Text")]
    for category in ["Title", "Image", "Table"]:
        if category in categories:
            legend_handles.append(
                patches.Patch(color=category_to_color[category], label=category)
            )
    ax.axis("off")
    ax.legend(handles=legend_handles, loc="upper right")
    plt.tight_layout()
    plt.show()


def render_page(doc_list: list, page_number: int, print_text=True) -> None:
    pdf_page = pymupdf.open(pdf_path).load_page(page_number)
    page_docs = [
        doc for doc in doc_list if doc.metadata.get("page_number") == page_number
    ]
    segments = [doc.metadata for doc in page_docs]
    plot_pdf_with_boxes(pdf_page, segments)
    if print_text:
        for doc in page_docs:
            print(f"{doc.page_content}\n")
328/100:
pdf_path = "example/38304-gb0.pdf"
render_page(documents, 2)
328/101:
import fitz
import matplotlib.patches as patches
import matplotlib.pyplot as plt
from PIL import Image


def plot_pdf_with_boxes(pdf_page, segments):
    pix = pdf_page.get_pixmap()
    pil_image = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

    fig, ax = plt.subplots(1, figsize=(10, 10))
    ax.imshow(pil_image)
    categories = set()
    category_to_color = {
        "Title": "orchid",
        "Image": "forestgreen",
        "Table": "tomato",
    }
    for segment in segments:
        points = segment["coordinates"]["points"]
        layout_width = segment["coordinates"]["layout_width"]
        layout_height = segment["coordinates"]["layout_height"]
        scaled_points = [
            (x * pix.width / layout_width, y * pix.height / layout_height)
            for x, y in points
        ]
        box_color = category_to_color.get(segment["category"], "deepskyblue")
        categories.add(segment["category"])
        rect = patches.Polygon(
            scaled_points, linewidth=1, edgecolor=box_color, facecolor="none"
        )
        ax.add_patch(rect)

    # Make legend
    legend_handles = [patches.Patch(color="deepskyblue", label="Text")]
    for category in ["Title", "Image", "Table"]:
        if category in categories:
            legend_handles.append(
                patches.Patch(color=category_to_color[category], label=category)
            )
    ax.axis("off")
    ax.legend(handles=legend_handles, loc="upper right")
    plt.tight_layout()
    plt.show()


def render_page(doc_list: list, page_number: int, print_text=True) -> None:
    pdf_page = pymupdf.open(pdf_path).load_page(page_number -1)
    page_docs = [
        doc for doc in doc_list if doc.metadata.get("page_number") == page_number
    ]
    segments = [doc.metadata for doc in page_docs]
    plot_pdf_with_boxes(pdf_page, segments)
    if print_text:
        for doc in page_docs:
            print(f"{doc.page_content}\n")
328/102:
pdf_path = "example/38304-gb0.pdf"
render_page(documents, 2)
328/103: documents.metadata
328/104: document.metadata.get("page_number")
328/105: document.metadata.get("page_number")
328/106: document.metadata.get("page")
328/107:
import fitz
import matplotlib.patches as patches
import matplotlib.pyplot as plt
from PIL import Image


def plot_pdf_with_boxes(pdf_page, segments):
    pix = pdf_page.get_pixmap()
    pil_image = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

    fig, ax = plt.subplots(1, figsize=(10, 10))
    ax.imshow(pil_image)
    categories = set()
    category_to_color = {
        "Title": "orchid",
        "Image": "forestgreen",
        "Table": "tomato",
    }
    for segment in segments:
        points = segment["coordinates"]["points"]
        layout_width = segment["coordinates"]["layout_width"]
        layout_height = segment["coordinates"]["layout_height"]
        scaled_points = [
            (x * pix.width / layout_width, y * pix.height / layout_height)
            for x, y in points
        ]
        box_color = category_to_color.get(segment["category"], "deepskyblue")
        categories.add(segment["category"])
        rect = patches.Polygon(
            scaled_points, linewidth=1, edgecolor=box_color, facecolor="none"
        )
        ax.add_patch(rect)

    # Make legend
    legend_handles = [patches.Patch(color="deepskyblue", label="Text")]
    for category in ["Title", "Image", "Table"]:
        if category in categories:
            legend_handles.append(
                patches.Patch(color=category_to_color[category], label=category)
            )
    ax.axis("off")
    ax.legend(handles=legend_handles, loc="upper right")
    plt.tight_layout()
    plt.show()


def render_page(doc_list: list, page_number: int, print_text=True) -> None:
    pdf_page = pymupdf.open(pdf_path).load_page(page_number -1)
    page_docs = [
        doc for doc in doc_list if doc.metadata.get("page") == page_number
    ]
    segments = [doc.metadata for doc in page_docs]
    plot_pdf_with_boxes(pdf_page, segments)
    if print_text:
        for doc in page_docs:
            print(f"{doc.page_content}\n")
328/108: document.metadata.get("page")
328/109: render_page(docs, 5)
328/110: render_page(documents, 5)
328/111: pdf_page = pymupdf.open(pdf_path).load_page(page_number -1)
328/112:
page_number=10
pdf_page = pymupdf.open(pdf_path).load_page(page_number -1)
328/113: pdf_page
328/114: document.metadata.get("page")
328/115: render_page(doc, 5)
328/116: render_page(documents, 5)
328/117: render_page(documents, 10)
328/118:
import fitz
import matplotlib.patches as patches
import matplotlib.pyplot as plt
from PIL import Image


def plot_pdf_with_boxes(pdf_page, segments):
    pix = pdf_page.get_pixmap()
    pil_image = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

    fig, ax = plt.subplots(1, figsize=(10, 10))
    ax.imshow(pil_image)
    categories = set()
    category_to_color = {
        "Title": "orchid",
        "Image": "forestgreen",
        "Table": "tomato",
    }
    for segment in segments:
        points = segment["coordinates"]["points"]
        layout_width = segment["coordinates"]["layout_width"]
        layout_height = segment["coordinates"]["layout_height"]
        scaled_points = [
            (x * pix.width / layout_width, y * pix.height / layout_height)
            for x, y in points
        ]
        box_color = category_to_color.get(segment["page"], "deepskyblue")
        categories.add(segment["page"])
        rect = patches.Polygon(
            scaled_points, linewidth=1, edgecolor=box_color, facecolor="none"
        )
        ax.add_patch(rect)

    # Make legend
    legend_handles = [patches.Patch(color="deepskyblue", label="Text")]
    for category in ["Title", "Image", "Table"]:
        if category in categories:
            legend_handles.append(
                patches.Patch(color=category_to_color[category], label=category)
            )
    ax.axis("off")
    ax.legend(handles=legend_handles, loc="upper right")
    plt.tight_layout()
    plt.show()


def render_page(doc_list: list, page_number: int, print_text=True) -> None:
    pdf_page = pymupdf.open(pdf_path).load_page(page_number -1)
    page_docs = [
        doc for doc in doc_list if doc.metadata.get("page") == page_number
    ]
    segments = [doc.metadata for doc in page_docs]
    plot_pdf_with_boxes(pdf_page, segments)
    if print_text:
        for doc in page_docs:
            print(f"{doc.page_content}\n")
328/119: document.metadata.get("page")
328/120: render_page(documents, 10)
328/121:
for page in doc:
    pix = page.get_pixmap()
328/122:
for page in doc:
    pix = page.get_pixmap()
    pix.save("page-%i.png" % page.number)
344/1: documents[28].metadata
344/2: documents.metadata
344/3:
from PyPDF2 import PdfReader
import re

# Load the PDF file
pdf_path = "example/38304-gb0.pdf"
reader = PdfReader(pdf_path)

# Function to extract text and split it by headings
def split_pdf_by_headings(pdf_reader,heading_pattern=r"^\d+\.\s[A-Z][^\n]+"):
    """
    Splits PDF text into chunks based on headings.
    :param pdf_reader: PdfReader object.
    :param heading_pattern: Regex pattern for headings.
    :return: List of chunks as (heading, content) tuples.
    """
    chunks = []
    current_heading = None
    current_chunk = []
    
    # Read page by page
    for page in pdf_reader.pages:
        text = page.extract_text()
        lines = text.split("\n")
        
        for line in lines:
            # Check if the line matches the heading pattern
            if re.match(heading_pattern, line):
                # Save the current chunk if it exists
                if current_heading and current_chunk:
                    chunks.append((current_heading, "\n".join(current_chunk)))
                # Start a new chunk
                current_heading = line.strip()
                current_chunk = []
            else:
                # Add line to the current chunk
                current_chunk.append(line.strip())
    
    # Add the last chunk
    if current_heading and current_chunk:
        chunks.append((current_heading, "\n".join(current_chunk)))
    
    return chunks

# Split the PDF into chunks based on headings
heading_chunks = split_pdf_by_headings(reader)

# Display the first few chunks as examples
  # Show the first 3 chunks
344/4: import pymupdf
344/5: doc = pymupdf.open(pdf_path)  # or pymupdf.Document(filename)
344/6:
for page in doc:
    pix = page.get_pixmap()
344/7: doc.get_toc()
344/8:
for item in toc:
    level, heading, page_num = item
    # Metadata formatÄ±nÄ± oluÅtur
    metadata = f"{heading} (Page {page_num})"
    page = doc.load_page(page_num - 1)  # 0-indexed
    #page.set_text("footer", metadata, fontsize=10, location=(20, 10))  # footer konumu, font boyutu vs.

# PDF'i kaydet
output_pdf_path = "output_with_metadata.pdf"
doc.save(output_pdf_path)
344/9:
import os
import glob
from dotenv import load_dotenv
344/10:
#MODEL = Ollama(model="llama3.2")
MODEL = "gpt-4o-mini"
db_name = "vector-database"
344/11:
from langchain_community.document_loaders import PyMuPDFLoader
loader = PyMuPDFLoader(pdf_path)
documents = loader.load()
pdf = pymupdf.open(pdf_path)  # or pymupdf.Document(filename)
for document in documents:
    document.metadata["page"]+=1
    relevant_headings = [item[1] for item in pdf.get_toc() if item[2] == document.metadata["page"]]
    if relevant_headings:
        # Ä°lk bulunan baÅlÄ±ÄÄ± al (birden fazla baÅlÄ±k varsa birleÅtirilebilir)
        document.metadata["heading"] = ", ".join(relevant_headings)
344/12: documents.metadata
344/13: document.metadata
344/14: documentS[30].metadata
344/15: documents[30].metadata
344/16:
toc = pdf.get_toc()
for i, item in enumerate(toc):
    heading = item[1]
    start_page = item[2]
    print(heading)
    print(start_page)
344/17:
toc = pdf.get_toc()
for i, item in enumerate(toc):
    heading = item[1]
    start_page = item[2]
    print(heading,start_page)
    print()
344/18:
toc = pdf.get_toc()
for i, item in enumerate(toc,0):
    heading = item[1]
    start_page = item[2]
    print(heading,start_page)
    print()
344/19:
toc = pdf.get_toc()
for i, item in enumerate(toc):
    heading = item[1]
    start_page = item[2]
    print(heading,start_page)
    print()
344/20:
toc = pdf.get_toc()
for i, item in enumerate(toc,1):
    heading = item[1]
    start_page = item[2]
    print(heading,start_page)
    print()
344/21:
toc = pdf.get_toc()
for i, item in enumerate(toc):
    heading = item[1]
    start_page = item[2]
    print(heading,start_page)
    print()
344/22:

    from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
import fitz  # PyMuPDF

# PDF dosyasÄ±nÄ± yÃ¼kleme

loader = PyMuPDFLoader(pdf_path)
documents = loader.load()
pdf = fitz.open(pdf_path)

# TOC'yi al
toc = pdf.get_toc()

# LangChain iÃ§in yeni Document objeleri oluÅturma
chunked_documents = []

for i, item in enumerate(toc):
    heading = item[1]
    start_page = item[2]
    
    # Sonraki baÅlÄ±ÄÄ±n sayfasÄ±na kadar olan kÄ±smÄ± almak
    if i + 1 < len(toc):
        end_page = toc[i + 1][2] - 1
    else:
        end_page = pdf.page_count  # Son baÅlÄ±ksa PDF'in son sayfasÄ±na kadar

    # Metni birleÅtirerek chunk oluÅtur
    chunk_text = ""
    for page_num in range(start_page - 1, end_page):  # PyMuPDF iÃ§in 0-index
        chunk_text += pdf[page_num].get_text()

    # Ä°lk chunk'larÄ± oluÅtur ve listeye ekle
    chunked_documents.append(
        Document(
            page_content=chunk_text,
            metadata={"heading": heading, "start_page": start_page, "end_page": end_page}
        )
    )

# CharacterTextSplitter kullanarak ek parÃ§alama
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

# TOC'ye gÃ¶re bÃ¶lÃ¼nmÃ¼Å metinleri tekrar split ediyoruz
final_chunks = text_splitter.split_documents(chunked_documents)

# ÃÄ±ktÄ±yÄ± kontrol etmek iÃ§in
for chunk in final_chunks:
    print(f"Heading: {chunk.metadata['heading']}")
    print(f"Start Page: {chunk.metadata['start_page']}")
    print(f"End Page: {chunk.metadata['end_page']}")
    print(f"Content: {chunk.page_content[:200]}...")  # Ä°lk 200 karakter
    print("-" * 80)
344/23:

from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
import fitz  # PyMuPDF

# PDF dosyasÄ±nÄ± yÃ¼kleme

loader = PyMuPDFLoader(pdf_path)
documents = loader.load()
pdf = fitz.open(pdf_path)

# TOC'yi al
toc = pdf.get_toc()

# LangChain iÃ§in yeni Document objeleri oluÅturma
chunked_documents = []

for i, item in enumerate(toc):
    heading = item[1]
    start_page = item[2]
    
    # Sonraki baÅlÄ±ÄÄ±n sayfasÄ±na kadar olan kÄ±smÄ± almak
    if i + 1 < len(toc):
        end_page = toc[i + 1][2] - 1
    else:
        end_page = pdf.page_count  # Son baÅlÄ±ksa PDF'in son sayfasÄ±na kadar

    # Metni birleÅtirerek chunk oluÅtur
    chunk_text = ""
    for page_num in range(start_page - 1, end_page):  # PyMuPDF iÃ§in 0-index
        chunk_text += pdf[page_num].get_text()

    # Ä°lk chunk'larÄ± oluÅtur ve listeye ekle
    chunked_documents.append(
        Document(
            page_content=chunk_text,
            metadata={"heading": heading, "start_page": start_page, "end_page": end_page}
        )
    )

# CharacterTextSplitter kullanarak ek parÃ§alama
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

# TOC'ye gÃ¶re bÃ¶lÃ¼nmÃ¼Å metinleri tekrar split ediyoruz
final_chunks = text_splitter.split_documents(chunked_documents)

# ÃÄ±ktÄ±yÄ± kontrol etmek iÃ§in
for chunk in final_chunks:
    print(f"Heading: {chunk.metadata['heading']}")
    print(f"Start Page: {chunk.metadata['start_page']}")
    print(f"End Page: {chunk.metadata['end_page']}")
    print(f"Content: {chunk.page_content[:200]}...")  # Ä°lk 200 karakter
    print("-" * 80)
344/24:

from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
import fitz  # PyMuPDF

# PDF dosyasÄ±nÄ± yÃ¼kleme

loader = PyMuPDFLoader(pdf_path)
documents = loader.load()
pdf = fitz.open(pdf_path)

# TOC'yi al
toc = pdf.get_toc()

# LangChain iÃ§in yeni Document objeleri oluÅturma
chunked_documents = []

for i, item in enumerate(toc):
    heading = item[1]
    start_page = item[2]
    
    # Sonraki baÅlÄ±ÄÄ±n sayfasÄ±na kadar olan kÄ±smÄ± almak
    if i + 1 < len(toc):
        end_page = toc[i + 1][2] - 1
    else:
        end_page = pdf.page_count  # Son baÅlÄ±ksa PDF'in son sayfasÄ±na kadar

    # Metni birleÅtirerek chunk oluÅtur
    chunk_text = ""
    for page_num in range(start_page - 1, end_page):  # PyMuPDF iÃ§in 0-index
        chunk_text += pdf[page_num].get_text()

    # Ä°lk chunk'larÄ± oluÅtur ve listeye ekle
    chunked_documents.append(
        Document(
            page_content=chunk_text,
            metadata={"heading": heading, "start_page": start_page, "end_page": end_page}
        )
    )

# CharacterTextSplitter kullanarak ek parÃ§alama
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

# TOC'ye gÃ¶re bÃ¶lÃ¼nmÃ¼Å metinleri tekrar split ediyoruz
final_chunks = text_splitter.split_documents(chunked_documents)

# ÃÄ±ktÄ±yÄ± kontrol etmek iÃ§in
for chunk in final_chunks:
    print(f"Heading: {chunk.metadata['heading']}")
    print(f"Start Page: {chunk.metadata['start_page']}")
    print(f"End Page: {chunk.metadata['end_page']}")
    print(f"Content: {chunk.page_content[:500]}...")  # Ä°lk 200 karakter
    print("-" * 80)
344/25:

from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
import fitz  # PyMuPDF

# PDF dosyasÄ±nÄ± yÃ¼kleme

loader = PyMuPDFLoader(pdf_path)
documents = loader.load()
pdf = fitz.open(pdf_path)

# TOC'yi al
toc = pdf.get_toc()

# LangChain iÃ§in yeni Document objeleri oluÅturma
chunked_documents = []

for i, item in enumerate(toc):
    heading = item[1]
    start_page = item[2]
    
    # Sonraki baÅlÄ±ÄÄ±n sayfasÄ±na kadar olan kÄ±smÄ± almak
    if i + 1 < len(toc):
        end_page = toc[i + 1][2] - 1
    else:
        end_page = pdf.page_count  # Son baÅlÄ±ksa PDF'in son sayfasÄ±na kadar

    # Metni birleÅtirerek chunk oluÅtur
    chunk_text = ""
    for page_num in range(start_page - 1, end_page):  # PyMuPDF iÃ§in 0-index
        chunk_text += pdf[page_num].get_text()

    # Ä°lk chunk'larÄ± oluÅtur ve listeye ekle
    chunked_documents.append(
        Document(
            page_content=chunk_text,
            metadata={"heading": heading, "start_page": start_page, "end_page": end_page}
        )
    )

# CharacterTextSplitter kullanarak ek parÃ§alama
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

# TOC'ye gÃ¶re bÃ¶lÃ¼nmÃ¼Å metinleri tekrar split ediyoruz
final_chunks = text_splitter.split_documents(chunked_documents)

# ÃÄ±ktÄ±yÄ± kontrol etmek iÃ§in
documents[0].metadata
344/26:

from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
import fitz  # PyMuPDF

# PDF dosyasÄ±nÄ± yÃ¼kleme

loader = PyMuPDFLoader(pdf_path)
documents = loader.load()
pdf = fitz.open(pdf_path)

# TOC'yi al
toc = pdf.get_toc()

# LangChain iÃ§in yeni Document objeleri oluÅturma
chunked_documents = []

for i, item in enumerate(toc):
    heading = item[1]
    start_page = item[2]
    
    # Sonraki baÅlÄ±ÄÄ±n sayfasÄ±na kadar olan kÄ±smÄ± almak
    if i + 1 < len(toc):
        end_page = toc[i + 1][2] - 1
    else:
        end_page = pdf.page_count  # Son baÅlÄ±ksa PDF'in son sayfasÄ±na kadar

    # Metni birleÅtirerek chunk oluÅtur
    chunk_text = ""
    for page_num in range(start_page - 1, end_page):  # PyMuPDF iÃ§in 0-index
        chunk_text += pdf[page_num].get_text()

    # Ä°lk chunk'larÄ± oluÅtur ve listeye ekle
    chunked_documents.append(
        Document(
            page_content=chunk_text,
            metadata={"heading": heading, "start_page": start_page, "end_page": end_page}
        )
    )

# CharacterTextSplitter kullanarak ek parÃ§alama
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

# TOC'ye gÃ¶re bÃ¶lÃ¼nmÃ¼Å metinleri tekrar split ediyoruz
final_chunks = text_splitter.split_documents(chunked_documents)

# ÃÄ±ktÄ±yÄ± kontrol etmek iÃ§in
documents[1].metadata
344/27: documents[30].metadata
344/28: documents[23].metadata
344/29:

from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
import fitz  # PyMuPDF

# PDF dosyasÄ±nÄ± yÃ¼kleme

loader = PyMuPDFLoader(pdf_path)
documents = loader.load()
pdf = fitz.open(pdf_path)

# TOC'yi al
toc = pdf.get_toc()

# LangChain iÃ§in yeni Document objeleri oluÅturma
chunked_documents = []

for i, item in enumerate(toc):
    heading = item[1]
    start_page = item[2]
    
    # Sonraki baÅlÄ±ÄÄ±n sayfasÄ±na kadar olan kÄ±smÄ± almak
    if i + 1 < len(toc):
        end_page = toc[i + 1][2] - 1
    else:
        end_page = pdf.page_count  # Son baÅlÄ±ksa PDF'in son sayfasÄ±na kadar

    # Metni birleÅtirerek chunk oluÅtur
    chunk_text = ""
    for page_num in range(start_page - 1, end_page):  # PyMuPDF iÃ§in 0-index
        chunk_text += pdf[page_num].get_text()

    # Ä°lk chunk'larÄ± oluÅtur ve listeye ekle
    chunked_documents.append(
        Document(
            page_content=chunk_text,
            metadata={"heading": heading, "start_page": start_page, "end_page": end_page}
        )
    )

# CharacterTextSplitter kullanarak ek parÃ§alama
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

# TOC'ye gÃ¶re bÃ¶lÃ¼nmÃ¼Å metinleri tekrar split ediyoruz
final_chunks = text_splitter.split_documents(chunked_documents)

# ÃÄ±ktÄ±yÄ± kontrol etmek iÃ§in

final_chunks.metadata
344/30: final_chunks.metadata
344/31:

    from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
import fitz  # PyMuPDF

# PDF dosyasÄ±nÄ± yÃ¼kleme

loader = PyMuPDFLoader(pdf_path)
documents = loader.load()
pdf = fitz.open(pdf_path)

# TOC'yi al
toc = pdf.get_toc()

# LangChain iÃ§in yeni Document objeleri oluÅturma
chunked_documents = []

for i, item in enumerate(toc):
    heading = item[1]
    start_page = item[2]
    
    # Sonraki baÅlÄ±ÄÄ±n sayfasÄ±na kadar olan kÄ±smÄ± almak
    if i + 1 < len(toc):
        end_page = toc[i + 1][2] - 1
    else:
        end_page = pdf.page_count  # Son baÅlÄ±ksa PDF'in son sayfasÄ±na kadar

    # Metni birleÅtirerek chunk oluÅtur
    chunk_text = ""
    for page_num in range(start_page - 1, end_page):  # PyMuPDF iÃ§in 0-index
        chunk_text += pdf[page_num].get_text()

    # Ä°lk chunk'larÄ± oluÅtur ve listeye ekle
    chunked_documents.append(
        Document(
            page_content=chunk_text,
            metadata={"heading": heading, "start_page": start_page, "end_page": end_page}
        )
    )

# CharacterTextSplitter kullanarak ek parÃ§alama
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

# TOC'ye gÃ¶re bÃ¶lÃ¼nmÃ¼Å metinleri tekrar split ediyoruz
final_chunks = text_splitter.split_documents(chunked_documents)

# ÃÄ±ktÄ±yÄ± kontrol etmek iÃ§in
for chunk in final_chunks:
    print(f"Heading: {chunk.metadata['heading']}")
    print(f"Start Page: {chunk.metadata['start_page']}")
    print(f"End Page: {chunk.metadata['end_page']}")
    print(f"Heading: {chunk.metadata['heading']}")
    print(f"Content: {chunk.page_content[:200]}...")  # Ä°lk 200 karakter
    print("-" * 80)
344/32:

from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
import fitz  # PyMuPDF

# PDF dosyasÄ±nÄ± yÃ¼kleme

loader = PyMuPDFLoader(pdf_path)
documents = loader.load()
pdf = fitz.open(pdf_path)

# TOC'yi al
toc = pdf.get_toc()

# LangChain iÃ§in yeni Document objeleri oluÅturma
chunked_documents = []

for i, item in enumerate(toc):
    heading = item[1]
    start_page = item[2]
    
    # Sonraki baÅlÄ±ÄÄ±n sayfasÄ±na kadar olan kÄ±smÄ± almak
    if i + 1 < len(toc):
        end_page = toc[i + 1][2] - 1
    else:
        end_page = pdf.page_count  # Son baÅlÄ±ksa PDF'in son sayfasÄ±na kadar

    # Metni birleÅtirerek chunk oluÅtur
    chunk_text = ""
    for page_num in range(start_page - 1, end_page):  # PyMuPDF iÃ§in 0-index
        chunk_text += pdf[page_num].get_text()

    # Ä°lk chunk'larÄ± oluÅtur ve listeye ekle
    chunked_documents.append(
        Document(
            page_content=chunk_text,
            metadata={"heading": heading, "start_page": start_page, "end_page": end_page}
        )
    )

# CharacterTextSplitter kullanarak ek parÃ§alama
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

# TOC'ye gÃ¶re bÃ¶lÃ¼nmÃ¼Å metinleri tekrar split ediyoruz
final_chunks = text_splitter.split_documents(chunked_documents)

# ÃÄ±ktÄ±yÄ± kontrol etmek iÃ§in
for chunk in final_chunks:
    print(f"Heading: {chunk.metadata['heading']}")
    print(f"Start Page: {chunk.metadata['start_page']}")
    print(f"End Page: {chunk.metadata['end_page']}")
    print(f"Heading: {chunk.metadata['heading']}")
    print(f"Content: {chunk.page_content[:200]}...")  # Ä°lk 200 karakter
    print("-" * 80)
344/33:

from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
import fitz  # PyMuPDF

# PDF dosyasÄ±nÄ± yÃ¼kleme

loader = PyMuPDFLoader(pdf_path)
documents = loader.load()
pdf = fitz.open(pdf_path)

# TOC'yi al
toc = pdf.get_toc()

# LangChain iÃ§in yeni Document objeleri oluÅturma
chunked_documents = []

for i, item in enumerate(toc):
    heading = item[1]
    start_page = item[2]
    
    # Sonraki baÅlÄ±ÄÄ±n sayfasÄ±na kadar olan kÄ±smÄ± almak
    if i + 1 < len(toc):
        end_page = toc[i + 1][2] - 1
    else:
        end_page = pdf.page_count  # Son baÅlÄ±ksa PDF'in son sayfasÄ±na kadar

    # Metni birleÅtirerek chunk oluÅtur
    chunk_text = ""
    for page_num in range(start_page - 1, end_page):  # PyMuPDF iÃ§in 0-index
        chunk_text += pdf[page_num].get_text()

    # Ä°lk chunk'larÄ± oluÅtur ve listeye ekle
    chunked_documents.append(
        Document(
            page_content=chunk_text,
            metadata={"heading": heading, "start_page": start_page, "end_page": end_page}
        )
    )

# CharacterTextSplitter kullanarak ek parÃ§alama
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

# TOC'ye gÃ¶re bÃ¶lÃ¼nmÃ¼Å metinleri tekrar split ediyoruz
final_chunks = text_splitter.split_documents(chunked_documents)

# ÃÄ±ktÄ±yÄ± kontrol etmek iÃ§in
for chunk in final_chunks:
    print(f"Heading: {chunk.metadata['heading']}")
    print(f"Start Page: {chunk.metadata['start_page']}")
    print(f"End Page: {chunk.metadata['end_page']}")
    print(f"Heading: {chunk.metadata['heading']}")
    print(f"Content: {chunk.page_content[:200]}...")  # Ä°lk 200 karakter
    print("-" * 80)
344/34:

from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
import fitz  # PyMuPDF

# PDF dosyasÄ±nÄ± yÃ¼kleme

loader = PyMuPDFLoader(pdf_path)
documents = loader.load()
pdf = fitz.open(pdf_path)

# TOC'yi al
toc = pdf.get_toc()

# LangChain iÃ§in yeni Document objeleri oluÅturma
chunked_documents = []

for i, item in enumerate(toc):
    heading = item[1]
    start_page = item[2]
    
    # Sonraki baÅlÄ±ÄÄ±n sayfasÄ±na kadar olan kÄ±smÄ± almak
    if i + 1 < len(toc):
        end_page = toc[i + 1][2] - 1
    else:
        end_page = pdf.page_count  # Son baÅlÄ±ksa PDF'in son sayfasÄ±na kadar

    # Metni birleÅtirerek chunk oluÅtur
    chunk_text = ""
    for page_num in range(start_page - 1, end_page):  # PyMuPDF iÃ§in 0-index
        chunk_text += pdf[page_num].get_text()

    # Ä°lk chunk'larÄ± oluÅtur ve listeye ekle
    chunked_documents.append(
        Document(
            page_content=chunk_text,
            metadata={"heading": heading, "start_page": start_page, "end_page": end_page}
        )
    )

# CharacterTextSplitter kullanarak ek parÃ§alama
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

# TOC'ye gÃ¶re bÃ¶lÃ¼nmÃ¼Å metinleri tekrar split ediyoruz
final_chunks = text_splitter.split_documents(chunked_documents)

# ÃÄ±ktÄ±yÄ± kontrol etmek iÃ§in
for chunk in final_chunks:
    print(f"Heading: {chunk.metadata['heading']}")
    print(f"Start Page: {chunk.metadata['start_page']}")
    print(f"End Page: {chunk.metadata['end_page']}")
    print(f"Heading: {chunk.metadata['heading']}")
    print(f"Content: {chunk.page_content[]}...")  # Ä°lk 200 karakter
    print("-" * 80)
344/35:

from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
import fitz  # PyMuPDF

# PDF dosyasÄ±nÄ± yÃ¼kleme

loader = PyMuPDFLoader(pdf_path)
documents = loader.load()
pdf = fitz.open(pdf_path)

# TOC'yi al
toc = pdf.get_toc()

# LangChain iÃ§in yeni Document objeleri oluÅturma
chunked_documents = []

for i, item in enumerate(toc):
    heading = item[1]
    start_page = item[2]
    
    # Sonraki baÅlÄ±ÄÄ±n sayfasÄ±na kadar olan kÄ±smÄ± almak
    if i + 1 < len(toc):
        end_page = toc[i + 1][2] - 1
    else:
        end_page = pdf.page_count  # Son baÅlÄ±ksa PDF'in son sayfasÄ±na kadar

    # Metni birleÅtirerek chunk oluÅtur
    chunk_text = ""
    for page_num in range(start_page - 1, end_page):  # PyMuPDF iÃ§in 0-index
        chunk_text += pdf[page_num].get_text()

    # Ä°lk chunk'larÄ± oluÅtur ve listeye ekle
    chunked_documents.append(
        Document(
            page_content=chunk_text,
            metadata={"heading": heading, "start_page": start_page, "end_page": end_page}
        )
    )

# CharacterTextSplitter kullanarak ek parÃ§alama
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

# TOC'ye gÃ¶re bÃ¶lÃ¼nmÃ¼Å metinleri tekrar split ediyoruz
final_chunks = text_splitter.split_documents(chunked_documents)

# ÃÄ±ktÄ±yÄ± kontrol etmek iÃ§in
for chunk in final_chunks:
    print(f"Heading: {chunk.metadata['heading']}")
    print(f"Start Page: {chunk.metadata['start_page']}")
    print(f"End Page: {chunk.metadata['end_page']}")
    print(f"Heading: {chunk.metadata['heading']}")
    print(f"Content: {chunk.page_content}...")  # Ä°lk 200 karakter
    print("-" * 80)
344/36: len(final_chunks)
344/37:

from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
import fitz  # PyMuPDF

# PDF dosyasÄ±nÄ± yÃ¼kleme

loader = PyMuPDFLoader(pdf_path)
documents = loader.load()
pdf = fitz.open(pdf_path)

# TOC'yi al
toc = pdf.get_toc()

# LangChain iÃ§in yeni Document objeleri oluÅturma
chunked_documents = []

for i, item in enumerate(toc):
    heading = item[1]
    start_page = item[2]
    
    # Sonraki baÅlÄ±ÄÄ±n sayfasÄ±na kadar olan kÄ±smÄ± almak
    if i + 1 < len(toc):
        end_page = toc[i + 1][2] - 1
    else:
        end_page = pdf.page_count  # Son baÅlÄ±ksa PDF'in son sayfasÄ±na kadar

    # Metni birleÅtirerek chunk oluÅtur
    chunk_text = ""
    for page_num in range(start_page - 1, end_page):  # PyMuPDF iÃ§in 0-index
        chunk_text += pdf[page_num].get_text()

    # Ä°lk chunk'larÄ± oluÅtur ve listeye ekle
    chunked_documents.append(
        Document(
            page_content=chunk_text,
            metadata={"heading": heading, "start_page": start_page, "end_page": end_page}
        )
    )

# CharacterTextSplitter kullanarak ek parÃ§alama
text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=50)

# TOC'ye gÃ¶re bÃ¶lÃ¼nmÃ¼Å metinleri tekrar split ediyoruz
final_chunks = text_splitter.split_documents(chunked_documents)

# ÃÄ±ktÄ±yÄ± kontrol etmek iÃ§in
344/38: len(final_chunks)
344/39:

from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
import fitz  # PyMuPDF

# PDF dosyasÄ±nÄ± yÃ¼kleme

loader = PyMuPDFLoader(pdf_path)
documents = loader.load()
pdf = fitz.open(pdf_path)

# TOC'yi al
toc = pdf.get_toc()

# LangChain iÃ§in yeni Document objeleri oluÅturma
chunked_documents = []

for i, item in enumerate(toc):
    heading = item[1]
    start_page = item[2]
    
    # Sonraki baÅlÄ±ÄÄ±n sayfasÄ±na kadar olan kÄ±smÄ± almak
    if i + 1 < len(toc):
        end_page = toc[i + 1][2] - 1
    else:
        end_page = pdf.page_count  # Son baÅlÄ±ksa PDF'in son sayfasÄ±na kadar

    # Metni birleÅtirerek chunk oluÅtur
    chunk_text = ""
    for page_num in range(start_page - 1, end_page):  # PyMuPDF iÃ§in 0-index
        chunk_text += pdf[page_num].get_text()

    # Ä°lk chunk'larÄ± oluÅtur ve listeye ekle
    chunked_documents.append(
        Document(
            page_content=chunk_text,
            metadata={"heading": heading, "start_page": start_page, "end_page": end_page}
        )
    )

# CharacterTextSplitter kullanarak ek parÃ§alama
text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=10)

# TOC'ye gÃ¶re bÃ¶lÃ¼nmÃ¼Å metinleri tekrar split ediyoruz
final_chunks = text_splitter.split_documents(chunked_documents)

# ÃÄ±ktÄ±yÄ± kontrol etmek iÃ§in
344/40: len(final_chunks)
344/41: final_chunks
344/42: final_chunks.metadata
344/43:
for chunk in final_chunks:
    print(chunk)
    print(80*"--")
344/44: doc2 = pymupdf.open("example/36201-i00.pdf")
344/45: doc2.get_toc()
344/46: del doc2
344/47:
from langchain_ollama import OllamaEmbeddings

embedding = OllamaEmbeddings(
    model="llama3.2",
)
vectordb = Chroma.from_documents(documents=splits, embedding=embedding)
344/48: from langchain_chroma import Chroma
344/49:
from langchain_ollama import OllamaEmbeddings

embedding = OllamaEmbeddings(
    model="llama3.2",
)
vectordb = Chroma.from_documents(documents=splits, embedding=embedding)
344/50:
import os
import glob
from dotenv import load_dotenv
344/51:
#MODEL = Ollama(model="llama3.2")
MODEL = "gpt-4o-mini"
db_name = "vector-database"
344/52:
toc = pdf.get_toc()
for i, item in enumerate(toc):
    heading = item[1]
    start_page = item[2]
    print(heading,start_page)
    print()
344/53: from langchain_chroma import Chroma
344/54:
from langchain_ollama import OllamaEmbeddings

embedding = OllamaEmbeddings(
    model="llama3.2",
)
if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)
344/55:
from langchain_ollama import OllamaEmbeddings

embedding = OllamaEmbeddings(
    model="nomic-embed-text",
)
if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embedding, persist_directory=db_name)
344/56:
from langchain_ollama import OllamaEmbeddings

embedding = OllamaEmbeddings(
    model="nomic-embed-text",
)
if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embedding).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embedding, persist_directory=db_name)
344/57:
from langchain_ollama import OllamaEmbeddings

embedding = OllamaEmbeddings(
    model="nomic-embed-text",
)
if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embedding).delete_collection()

vectorstore = Chroma.from_documents(documents=final_chunks, embedding=embedding, persist_directory=db_name)
344/58:
question = "What are the three main states of a UE in the RRC protocol? How do they differ?"
len(vectorstore.similarity_search(question))
344/59:
from langchain.prompts import ChatPromptTemplate

# Multi Query: Different Perspectives
template = """You are an AI language model assistant. Your task is to generate five 
different versions of the given user question to retrieve relevant documents from a vector 
database. By generating multiple perspectives on the user question, your goal is to help
the user overcome some of the limitations of the distance-based similarity search. 
Provide these alternative questions separated by newlines. Original question: {question}"""
prompt_perspectives = ChatPromptTemplate.from_template(template)

from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI

generate_queries = (
    prompt_perspectives 
    | ChatOpenAI(temperature=0) 
    | StrOutputParser() 
    | (lambda x: x.split("\n"))
)
344/60:
load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
openai = OpenAI()
344/61:
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
openai = OpenAI()
344/62:
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from openai import OpenAI
load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
openai = OpenAI()
344/63:
from langchain.prompts import ChatPromptTemplate

# Multi Query: Different Perspectives
template = """You are an AI language model assistant. Your task is to generate five 
different versions of the given user question to retrieve relevant documents from a vector 
database. By generating multiple perspectives on the user question, your goal is to help
the user overcome some of the limitations of the distance-based similarity search. 
Provide these alternative questions separated by newlines. Original question: {question}"""
prompt_perspectives = ChatPromptTemplate.from_template(template)

from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI

generate_queries = (
    prompt_perspectives 
    | ChatOpenAI(temperature=0) 
    | StrOutputParser() 
    | (lambda x: x.split("\n"))
)
344/64:
from langchain.load import dumps, loads

def get_unique_union(documents: list[list]):
    """ Unique union of retrieved docs """
    # Flatten list of lists, and convert each Document to string
    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]
    # Get unique documents
    unique_docs = list(set(flattened_docs))
    # Return
    return [loads(doc) for doc in unique_docs]

# Retrieve
question = "What are the three main states of a UE in the RRC protocol? How do they differ?"
retrieval_chain = generate_queries | retriever.map() | get_unique_union
docs = retrieval_chain.invoke({"question":question})
len(docs)
344/65:
question = "What are the three main states of a UE in the RRC protocol? How do they differ?"
len(vectorstore.similarity_search(question))
retriever = vectorstore.as_retriever()
344/66:
from langchain.load import dumps, loads

def get_unique_union(documents: list[list]):
    """ Unique union of retrieved docs """
    # Flatten list of lists, and convert each Document to string
    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]
    # Get unique documents
    unique_docs = list(set(flattened_docs))
    # Return
    return [loads(doc) for doc in unique_docs]

# Retrieve
question = "What are the three main states of a UE in the RRC protocol? How do they differ?"
retrieval_chain = generate_queries | retriever.map() | get_unique_union
docs = retrieval_chain.invoke({"question":question})
len(docs)
344/67:
from langchain.load import dumps, loads

def get_unique_union(documents: list[list]):
    """ Unique union of retrieved docs """
    # Flatten list of lists, and convert each Document to string
    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]
    # Get unique documents
    unique_docs = list(set(flattened_docs))
    # Return
    return [loads(doc) for doc in unique_docs]

# Retrieve
question = "What are the three main states of a UE in the RRC protocol? How do they differ?"
retrieval_chain = generate_queries | retriever.map() | get_unique_union
docs = retrieval_chain.invoke({"question":question})
docs
344/68: generate_queries
344/69:
from langchain.load import dumps, loads

def get_unique_union(documents: list[list]):
    """ Unique union of retrieved docs """
    # Flatten list of lists, and convert each Document to string
    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]
    # Get unique documents
    unique_docs = list(set(flattened_docs))
    # Return
    return [loads(doc) for doc in unique_docs]

# Retrieve
question = "What are the three main states of a UE in the RRC protocol? How do they differ?"
retrieval_chain = generate_queries | retriever.map() | get_unique_union
docs = retrieval_chain.invoke({"question":question})
retrieval_chain
344/70:
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain_openai import ChatOpenAI

question = "What are the three main states of a UE in the RRC protocol? How do they differ?"
llm = ChatOpenAI(temperature=0)
retriever_from_llm = MultiQueryRetriever.from_llm(
    retriever=retriever, llm=llm
)
344/71:
# Set logging for the queries
import logging

logging.basicConfig()
logging.getLogger("langchain.retrievers.multi_query").setLevel(logging.INFO)
344/72:
unique_docs = retriever_from_llm.invoke(question)
len(unique_docs)
344/73:
from langchain.prompts import ChatPromptTemplate

# Decomposition
template = """You are a helpful assistant that generates multiple sub-questions related to an input question. \n
The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \n
Generate multiple search queries related to: {question} \n
Output (3 queries):"""
prompt_decomposition = ChatPromptTemplate.from_template(template)
344/74:
from langchain.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

def decomposition_of(question)
# Decomposition
    template = """You are a helpful assistant that generates multiple sub-questions related to an input question. \n
    The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \n
    Generate multiple search queries related to: {question} \n
    Output (3 queries):"""
    prompt_decomposition = ChatPromptTemplate.from_template(template)
    
    #LLM
    llm = ChatOpenAI(temperature=0)

    # Chain
    generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split("\n")))

    # Run
    questions = generate_queries_decomposition.invoke({"question":question})
344/75:
from langchain.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

def decomposition_of(question):
# Decomposition
    template = """You are a helpful assistant that generates multiple sub-questions related to an input question. \n
    The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \n
    Generate multiple search queries related to: {question} \n
    Output (3 queries):"""
    prompt_decomposition = ChatPromptTemplate.from_template(template)
    
    #LLM
    llm = ChatOpenAI(temperature=0)

    # Chain
    generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split("\n")))

    # Run
    questions = generate_queries_decomposition.invoke({"question":question})
344/76: decomposition_of("Which system information blocks (SIBs) are critical for UE operation during connection establishment?")
344/77:
from langchain.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

def decomposition_of(question):
# Decomposition
    template = """You are a helpful assistant that generates multiple sub-questions related to an input question. \n
    The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \n
    Generate multiple search queries related to: {question} \n
    Output (3 queries):"""
    prompt_decomposition = ChatPromptTemplate.from_template(template)
    
    #LLM
    llm = ChatOpenAI(temperature=0)

    # Chain
    generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split("\n")))

    # Run
    questions = generate_queries_decomposition.invoke({"question":question})
    return questions
344/78: decomposition_of("Which system information blocks (SIBs) are critical for UE operation during connection establishment?")
344/79:
# Prompt
template = """Here is the question you need to answer:

\n --- \n {question} \n --- \n

Here is any available background question + answer pairs:

\n --- \n {q_a_pairs} \n --- \n

Here is additional context relevant to the question: 

\n --- \n {context} \n --- \n

Use the above context and any background question + answer pairs to answer the question: \n {question}
"""

decomposition_prompt = ChatPromptTemplate.from_template(template)
344/80:
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser

def format_qa_pair(question, answer):
    """Format Q and A pair"""
    
    formatted_string = ""
    formatted_string += f"Question: {question}\nAnswer: {answer}\n\n"
    return formatted_string.strip()

# llm
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)

q_a_pairs = ""
for q in questions:
    
    rag_chain = (
    {"context": itemgetter("question") | retriever, 
     "question": itemgetter("question"),
     "q_a_pairs": itemgetter("q_a_pairs")} 
    | decomposition_prompt
    | llm
    | StrOutputParser())

    answer = rag_chain.invoke({"question":q,"q_a_pairs":q_a_pairs})
    q_a_pair = format_qa_pair(q,answer)
    q_a_pairs = q_a_pairs + "\n---\n"+  q_a_pair
344/81:
question = "Which system information blocks (SIBs) are critical for UE operation during connection establishment?"
questions = decomposition_of(question)
344/82:
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser

def format_qa_pair(question, answer):
    """Format Q and A pair"""
    
    formatted_string = ""
    formatted_string += f"Question: {question}\nAnswer: {answer}\n\n"
    return formatted_string.strip()

# llm
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)

q_a_pairs = ""
for q in questions:
    
    rag_chain = (
    {"context": itemgetter("question") | retriever, 
     "question": itemgetter("question"),
     "q_a_pairs": itemgetter("q_a_pairs")} 
    | decomposition_prompt
    | llm
    | StrOutputParser())

    answer = rag_chain.invoke({"question":q,"q_a_pairs":q_a_pairs})
    q_a_pair = format_qa_pair(q,answer)
    q_a_pairs = q_a_pairs + "\n---\n"+  q_a_pair
344/83: answer
344/84:
question = "What are the differences between the RRC_IDLE and RRC_INACTIVE states in terms of UE behavior?"
questions = decomposition_of(question)
344/85:
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser

def format_qa_pair(question, answer):
    """Format Q and A pair"""
    
    formatted_string = ""
    formatted_string += f"Question: {question}\nAnswer: {answer}\n\n"
    return formatted_string.strip()

# llm
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)

q_a_pairs = ""
for q in questions:
    
    rag_chain = (
    {"context": itemgetter("question") | retriever, 
     "question": itemgetter("question"),
     "q_a_pairs": itemgetter("q_a_pairs")} 
    | decomposition_prompt
    | llm
    | StrOutputParser())

    answer = rag_chain.invoke({"question":q,"q_a_pairs":q_a_pairs})
    q_a_pair = format_qa_pair(q,answer)
    q_a_pairs = q_a_pairs + "\n---\n"+  q_a_pair
344/86: answer
344/87:
from IPython.display import Markdown, display
display(Markdown(answer))
344/88:
question = "What are the table of contents of this document and display wih markdown"
questions = decomposition_of(question)
344/89:
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser

def format_qa_pair(question, answer):
    """Format Q and A pair"""
    
    formatted_string = ""
    formatted_string += f"Question: {question}\nAnswer: {answer}\n\n"
    return formatted_string.strip()

# llm
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)

q_a_pairs = ""
for q in questions:
    
    rag_chain = (
    {"context": itemgetter("question") | retriever, 
     "question": itemgetter("question"),
     "q_a_pairs": itemgetter("q_a_pairs")} 
    | decomposition_prompt
    | llm
    | StrOutputParser())

    answer = rag_chain.invoke({"question":q,"q_a_pairs":q_a_pairs})
    q_a_pair = format_qa_pair(q,answer)
    q_a_pairs = q_a_pairs + "\n---\n"+  q_a_pair
344/90:
from IPython.display import Markdown, display
display(Markdown(answer))
344/91:
question = "What are the table of contents of this document and display results with markdown"
questions = decomposition_of(question)
344/92:
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser

def format_qa_pair(question, answer):
    """Format Q and A pair"""
    
    formatted_string = ""
    formatted_string += f"Question: {question}\nAnswer: {answer}\n\n"
    return formatted_string.strip()

# llm
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)

q_a_pairs = ""
for q in questions:
    
    rag_chain = (
    {"context": itemgetter("question") | retriever, 
     "question": itemgetter("question"),
     "q_a_pairs": itemgetter("q_a_pairs")} 
    | decomposition_prompt
    | llm
    | StrOutputParser())

    answer = rag_chain.invoke({"question":q,"q_a_pairs":q_a_pairs})
    q_a_pair = format_qa_pair(q,answer)
    q_a_pairs = q_a_pairs + "\n---\n"+  q_a_pair
344/93:
from IPython.display import Markdown, display
display(Markdown(answer))
344/94:
from langchain.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_ollama import ChatOllama
def decomposition_of(question):
# Decomposition
    template = """You are a helpful assistant that generates multiple sub-questions related to an input question. \n
    The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \n
    Generate multiple search queries related to: {question} \n
    Output (3 queries):"""
    prompt_decomposition = ChatPromptTemplate.from_template(template)
    
    #LLM
    llm = ChatOllama(model="llama3.2",temperature=0)

    # Chain
    generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split("\n")))

    # Run
    questions = generate_queries_decomposition.invoke({"question":question})
    return questions
344/95: decomposition_of("Which system information blocks (SIBs) are critical for UE operation during connection establishment?")
344/96:
from langchain.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_ollama import ChatOllama
def decomposition_of(question):
# Decomposition
    template = """You are a helpful assistant that generates multiple sub-questions related to an input question. \n
    The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \n
    Generate multiple search queries related to: {question} \n
    Output (only just 3 queries):"""
    prompt_decomposition = ChatPromptTemplate.from_template(template)
    
    #LLM
    llm = ChatOllama(model="llama3.2",temperature=0)

    # Chain
    generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split("\n")))

    # Run
    questions = generate_queries_decomposition.invoke({"question":question})
    return questions
344/97: decomposition_of("Which system information blocks (SIBs) are critical for UE operation during connection establishment?")
344/98:
from langchain.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_ollama import ChatOllama
def decomposition_of(question):
# Decomposition
    template = """You are a helpful assistant that generates multiple sub-questions related to an input question. \n
    The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \n
    Generate multiple search queries related to: {question} \n
    Output (Just write 3 queries):"""
    prompt_decomposition = ChatPromptTemplate.from_template(template)
    
    #LLM
    llm = ChatOllama(model="llama3.2",temperature=0)

    # Chain
    generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split("\n")))

    # Run
    questions = generate_queries_decomposition.invoke({"question":question})
    return questions
344/99: decomposition_of("Which system information blocks (SIBs) are critical for UE operation during connection establishment?")
344/100:
from langchain.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_ollama import ChatOllama
def decomposition_of(question):
# Decomposition
    template = """You are a helpful assistant that generates multiple sub-questions related to an input question. \n
    The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \n
    Generate multiple search queries related to: {question} \n
    Output (Just write 3 queries, no another answer such as:Here are..):"""
    prompt_decomposition = ChatPromptTemplate.from_template(template)
    
    #LLM
    llm = ChatOllama(model="llama3.2",temperature=0)

    # Chain
    generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split("\n")))

    # Run
    questions = generate_queries_decomposition.invoke({"question":question})
    return questions
344/101: decomposition_of("Which system information blocks (SIBs) are critical for UE operation during connection establishment?")
344/102:
question = "Which system information blocks (SIBs) are critical for UE operation during connection establishment?"
questions = decomposition_of(question)
344/103:
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser

def format_qa_pair(question, answer):
    """Format Q and A pair"""
    
    formatted_string = ""
    formatted_string += f"Question: {question}\nAnswer: {answer}\n\n"
    return formatted_string.strip()

# llm
llm = ChatOllama(model="llama3.2",temperature=0

q_a_pairs = ""
for q in questions:
    
    rag_chain = (
    {"context": itemgetter("question") | retriever, 
     "question": itemgetter("question"),
     "q_a_pairs": itemgetter("q_a_pairs")} 
    | decomposition_prompt
    | llm
    | StrOutputParser())

    answer = rag_chain.invoke({"question":q,"q_a_pairs":q_a_pairs})
    q_a_pair = format_qa_pair(q,answer)
    q_a_pairs = q_a_pairs + "\n---\n"+  q_a_pair
344/104:
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser

def format_qa_pair(question, answer):
    """Format Q and A pair"""
    
    formatted_string = ""
    formatted_string += f"Question: {question}\nAnswer: {answer}\n\n"
    return formatted_string.strip()

# llm
llm = ChatOllama(model="llama3.2",temperature=0)

q_a_pairs = ""
for q in questions:
    
    rag_chain = (
    {"context": itemgetter("question") | retriever, 
     "question": itemgetter("question"),
     "q_a_pairs": itemgetter("q_a_pairs")} 
    | decomposition_prompt
    | llm
    | StrOutputParser())

    answer = rag_chain.invoke({"question":q,"q_a_pairs":q_a_pairs})
    q_a_pair = format_qa_pair(q,answer)
    q_a_pairs = q_a_pairs + "\n---\n"+  q_a_pair
344/105: answer
344/106: display(Markdown(answer))
344/107: decomposition_of("What is the significance of TreselectionNR, and how does it impact cell reselection?")
344/108:
# Prompt
template = """Here is the question you need to answer:

\n --- \n {question} \n --- \n

Here is any available background question + answer pairs:

\n --- \n {q_a_pairs} \n --- \n

Here is additional context relevant to the question: 

\n --- \n {context} \n --- \n

Use the above context and any background question + answer pairs to answer the question: \n {question}
"""

decomposition_prompt = ChatPromptTemplate.from_template(template)
344/109:
question = "What is the significance of TreselectionNR, and how does it impact cell reselection?"
questions = decomposition_of(question)
344/110:
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser

def format_qa_pair(question, answer):
    """Format Q and A pair"""
    
    formatted_string = ""
    formatted_string += f"Question: {question}\nAnswer: {answer}\n\n"
    return formatted_string.strip()

# llm
llm = ChatOllama(model="llama3.2",temperature=0)

q_a_pairs = ""
for q in questions:
    
    rag_chain = (
    {"context": itemgetter("question") | retriever, 
     "question": itemgetter("question"),
     "q_a_pairs": itemgetter("q_a_pairs")} 
    | decomposition_prompt
    | llm
    | StrOutputParser())

    answer = rag_chain.invoke({"question":q,"q_a_pairs":q_a_pairs})
    q_a_pair = format_qa_pair(q,answer)
    q_a_pairs = q_a_pairs + "\n---\n"+  q_a_pair
344/111: display(Markdown(answer))
344/112:
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser

import os
import openai

client = openai.OpenAI(
    api_key=os.environ.get("SAMBANOVA_API_KEY"),
    base_url="https://api.sambanova.ai/v1",
)

def format_qa_pair(question, answer):
    """Format Q and A pair"""
    
    formatted_string = ""
    formatted_string += f"Question: {question}\nAnswer: {answer}\n\n"
    return formatted_string.strip()

# llm
llm = ChatOllama(model="'Meta-Llama-3.1-8B-Instruct'",temperature=0)

q_a_pairs = ""
for q in questions:
    
    rag_chain = (
    {"context": itemgetter("question") | retriever, 
     "question": itemgetter("question"),
     "q_a_pairs": itemgetter("q_a_pairs")} 
    | decomposition_prompt
    | llm
    | StrOutputParser())

    answer = rag_chain.invoke({"question":q,"q_a_pairs":q_a_pairs})
    q_a_pair = format_qa_pair(q,answer)
    q_a_pairs = q_a_pairs + "\n---\n"+  q_a_pair
344/113:
question = "What is the significance of TreselectionNR, and how does it impact cell reselection?"
questions = decomposition_of(question)
344/114:
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser

import os
import openai
"""
client = openai.OpenAI(
    api_key=os.environ.get("SAMBANOVA_API_KEY"),
    base_url="https://api.sambanova.ai/v1",
)
"""
API_KEY = os.environ.get("SAMBANOVA_API_KEY")
BASE_URL = "https://api.sambanova.ai/v1"

def custom_retriever(question):
    """Kendi vectorstore.as_retriever() fonksiyonunuzla veriyi alÄ±n"""
    # Burada vectorstore.as_retriever() kullanarak context alÄ±n.
    results = retriever.get_relevant_documents(question)
    
    # AlÄ±nan sonuÃ§larÄ± birleÅtirip context olarak dÃ¶ndÃ¼rme
    context = " ".join([result.page_content for result in results])
    return context

def format_qa_pair(question, answer):
    """Format Q and A pair"""
    
    formatted_string = ""
    formatted_string += f"Question: {question}\nAnswer: {answer}\n\n"
    return formatted_string.strip()

def get_answer_from_api(question, context):
    """API Ã¼zerinden cevap almak iÃ§in fonksiyon"""
    data = {
        "model": "Meta-Llama-3.2",  # API'deki model adÄ±
        "messages": [
            {"role": "system", "content": decomposition_prompt},
            {"role": "user", "content": f"Context: {context}\nQuestion: {question}"}
        ],
        "temperature": 0.0
    }
    
    # API Ã§aÄrÄ±sÄ±
    response = requests.post(f"{BASE_URL}/chat/completions", json=data, headers=headers)
    
    if response.status_code == 200:
        answer = response.json().get('choices', [{}])[0].get('message', {}).get('content', '')
        return answer
    else:
        print(f"Error: {response.status_code}")
        return None

q_a_pairs = ""

questions = decomposition_of(question)

for q in questions:
    # Retriever'dan veri al
    context = custom_retriever(q)  # Kendi retriever'Ä±nÄ±zdan context alÄ±n
    
    # API Ã¼zerinden cevap al
    answer = get_answer_from_api(q, context)
    
    if answer:
        # Cevap ile Q&A formatÄ±nÄ± oluÅtur
        q_a_pair = format_qa_pair(q, answer)
        
        # Q&A Ã§iftlerini biriktir
        q_a_pairs += "\n---\n" + q_a_pair

# SonuÃ§larÄ± yazdÄ±r
display(Markdown(answer))

'''
# llm
llm = ChatOllama(model="'Meta-Llama-3.1-8B-Instruct'",temperature=0)

q_a_pairs = ""
for q in questions:
    
    rag_chain = (
    {"context": itemgetter("question") | retriever, 
     "question": itemgetter("question"),
     "q_a_pairs": itemgetter("q_a_pairs")} 
    | decomposition_prompt
    | llm
    | StrOutputParser())

    answer = rag_chain.invoke({"question":q,"q_a_pairs":q_a_pairs})
    q_a_pair = format_qa_pair(q,answer)
    q_a_pairs = q_a_pairs + "\n---\n"+  q_a_pair
'''
344/115:
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser
import requests
import os
import openai
"""
client = openai.OpenAI(
    api_key=os.environ.get("SAMBANOVA_API_KEY"),
    base_url="https://api.sambanova.ai/v1",
)
"""
API_KEY = os.environ.get("SAMBANOVA_API_KEY")
BASE_URL = "https://api.sambanova.ai/v1"

def custom_retriever(question):
    """Kendi vectorstore.as_retriever() fonksiyonunuzla veriyi alÄ±n"""
    # Burada vectorstore.as_retriever() kullanarak context alÄ±n.
    results = retriever.get_relevant_documents(question)
    
    # AlÄ±nan sonuÃ§larÄ± birleÅtirip context olarak dÃ¶ndÃ¼rme
    context = " ".join([result.page_content for result in results])
    return context

def format_qa_pair(question, answer):
    """Format Q and A pair"""
    
    formatted_string = ""
    formatted_string += f"Question: {question}\nAnswer: {answer}\n\n"
    return formatted_string.strip()

def get_answer_from_api(question, context):
    """API Ã¼zerinden cevap almak iÃ§in fonksiyon"""
    data = {
        "model": "Meta-Llama-3.2",  # API'deki model adÄ±
        "messages": [
            {"role": "system", "content": decomposition_prompt},
            {"role": "user", "content": f"Context: {context}\nQuestion: {question}"}
        ],
        "temperature": 0.0
    }
    
    # API Ã§aÄrÄ±sÄ±
    response = requests.post(f"{BASE_URL}/chat/completions", json=data, headers=headers)
    
    if response.status_code == 200:
        answer = response.json().get('choices', [{}])[0].get('message', {}).get('content', '')
        return answer
    else:
        print(f"Error: {response.status_code}")
        return None

q_a_pairs = ""

questions = decomposition_of(question)

for q in questions:
    # Retriever'dan veri al
    context = custom_retriever(q)  # Kendi retriever'Ä±nÄ±zdan context alÄ±n
    
    # API Ã¼zerinden cevap al
    answer = get_answer_from_api(q, context)
    
    if answer:
        # Cevap ile Q&A formatÄ±nÄ± oluÅtur
        q_a_pair = format_qa_pair(q, answer)
        
        # Q&A Ã§iftlerini biriktir
        q_a_pairs += "\n---\n" + q_a_pair

# SonuÃ§larÄ± yazdÄ±r
display(Markdown(answer))

'''
# llm
llm = ChatOllama(model="'Meta-Llama-3.1-8B-Instruct'",temperature=0)

q_a_pairs = ""
for q in questions:
    
    rag_chain = (
    {"context": itemgetter("question") | retriever, 
     "question": itemgetter("question"),
     "q_a_pairs": itemgetter("q_a_pairs")} 
    | decomposition_prompt
    | llm
    | StrOutputParser())

    answer = rag_chain.invoke({"question":q,"q_a_pairs":q_a_pairs})
    q_a_pair = format_qa_pair(q,answer)
    q_a_pairs = q_a_pairs + "\n---\n"+  q_a_pair
'''
344/116:
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser
import requests
import os
import openai
"""
client = openai.OpenAI(
    api_key=os.environ.get("SAMBANOVA_API_KEY"),
    base_url="https://api.sambanova.ai/v1",
)
"""
API_KEY = os.environ.get("SAMBANOVA_API_KEY")
BASE_URL = "https://api.sambanova.ai/v1"

headers = {
    "Authorization": f"Bearer {API_KEY}",
    "Content-Type": "application/json"
}

def custom_retriever(question):
    """Kendi vectorstore.as_retriever() fonksiyonunuzla veriyi alÄ±n"""
    # Burada vectorstore.as_retriever() kullanarak context alÄ±n.
    results = retriever.get_relevant_documents(question)
    
    # AlÄ±nan sonuÃ§larÄ± birleÅtirip context olarak dÃ¶ndÃ¼rme
    context = " ".join([result.page_content for result in results])
    return context

def format_qa_pair(question, answer):
    """Format Q and A pair"""
    
    formatted_string = ""
    formatted_string += f"Question: {question}\nAnswer: {answer}\n\n"
    return formatted_string.strip()

def get_answer_from_api(question, context):
    """API Ã¼zerinden cevap almak iÃ§in fonksiyon"""
    data = {
        "model": "Meta-Llama-3.2",  # API'deki model adÄ±
        "messages": [
            {"role": "system", "content": decomposition_prompt},
            {"role": "user", "content": f"Context: {context}\nQuestion: {question}"}
        ],
        "temperature": 0.0
    }
    
    # API Ã§aÄrÄ±sÄ±
    response = requests.post(f"{BASE_URL}/chat/completions", json=data, headers=headers)
    
    if response.status_code == 200:
        answer = response.json().get('choices', [{}])[0].get('message', {}).get('content', '')
        return answer
    else:
        print(f"Error: {response.status_code}")
        return None

q_a_pairs = ""

questions = decomposition_of(question)

for q in questions:
    # Retriever'dan veri al
    context = custom_retriever(q)  # Kendi retriever'Ä±nÄ±zdan context alÄ±n
    
    # API Ã¼zerinden cevap al
    answer = get_answer_from_api(q, context)
    
    if answer:
        # Cevap ile Q&A formatÄ±nÄ± oluÅtur
        q_a_pair = format_qa_pair(q, answer)
        
        # Q&A Ã§iftlerini biriktir
        q_a_pairs += "\n---\n" + q_a_pair

# SonuÃ§larÄ± yazdÄ±r
display(Markdown(answer))

'''
# llm
llm = ChatOllama(model="'Meta-Llama-3.1-8B-Instruct'",temperature=0)

q_a_pairs = ""
for q in questions:
    
    rag_chain = (
    {"context": itemgetter("question") | retriever, 
     "question": itemgetter("question"),
     "q_a_pairs": itemgetter("q_a_pairs")} 
    | decomposition_prompt
    | llm
    | StrOutputParser())

    answer = rag_chain.invoke({"question":q,"q_a_pairs":q_a_pairs})
    q_a_pair = format_qa_pair(q,answer)
    q_a_pairs = q_a_pairs + "\n---\n"+  q_a_pair
'''
344/117:
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser
import requests
import os
import openai
"""
client = openai.OpenAI(
    api_key=os.environ.get("SAMBANOVA_API_KEY"),
    base_url="https://api.sambanova.ai/v1",
)
"""
API_KEY = os.environ.get("SAMBANOVA_API_KEY")
BASE_URL = "https://api.sambanova.ai/v1"

headers = {
    "Authorization": f"Bearer {API_KEY}",
    "Content-Type": "application/json"
}

def custom_retriever(question):
    """Kendi vectorstore.as_retriever() fonksiyonunuzla veriyi alÄ±n"""
    # Burada vectorstore.as_retriever() kullanarak context alÄ±n.
    results = retriever.get_relevant_documents(question)
    
    # AlÄ±nan sonuÃ§larÄ± birleÅtirip context olarak dÃ¶ndÃ¼rme
    context = " ".join([result.page_content for result in results])
    return context

def format_qa_pair(question, answer):
    """Format Q and A pair"""
    
    formatted_string = ""
    formatted_string += f"Question: {question}\nAnswer: {answer}\n\n"
    return formatted_string.strip()

def get_answer_from_api(question, context):
    """API Ã¼zerinden cevap almak iÃ§in fonksiyon"""
    data = {
        "model": "Meta-Llama-3.2",  # API'deki model adÄ±
        "messages": [
            {"role": "system", "content": decomposition_prompt},
            {"role": "user", "content": f"Context: {context}\nQuestion: {question}"}
        ],
        "temperature": 0.0
    }
    
    # API Ã§aÄrÄ±sÄ±
    response = requests.post(f"{BASE_URL}/chat/completions", json=data, headers=headers)
    
    if response.status_code == 200:
        answer = response.json().get('choices', [{}])[0].get('message', {}).get('content', '')
        return answer
    else:
        print(f"Error: {response.status_code}")
        return None

q_a_pairs = ""

questions = decomposition_of(question)

for q in questions:
    # Retriever'dan veri al
    context = custom_retriever(q)  # Kendi retriever'Ä±nÄ±zdan context alÄ±n
    
    # API Ã¼zerinden cevap al
    answer = get_answer_from_api(q, context)
    
    if answer:
        # Cevap ile Q&A formatÄ±nÄ± oluÅtur
        q_a_pair = format_qa_pair(q, answer)
        
        # Q&A Ã§iftlerini biriktir
        q_a_pairs += "\n---\n" + q_a_pair

def get_combined_answer(q_a_pairs):
    """Q&A Ã§iftlerini kullanarak genel bir cevap oluÅtur"""
    
    # ChatPromptTemplate yerine metni direkt kullanÄ±yoruz
    combined_prompt = f"Please summarize the following Q&A pairs:\n{q_a_pairs}"
    
    # Retriever'dan context alÄ±n
    context = custom_retriever(question)  # Burada genel bir soru gÃ¶nderilebilir
    
    # API Ã¼zerinden toplu cevap alÄ±n
    combined_answer = get_answer_from_api(combined_prompt, context)
    
    return combined_answer
combined_answer = get_combined_answer(q_a_pairs)
print(combined_answer)

'''
# llm
llm = ChatOllama(model="'Meta-Llama-3.1-8B-Instruct'",temperature=0)

q_a_pairs = ""
for q in questions:
    
    rag_chain = (
    {"context": itemgetter("question") | retriever, 
     "question": itemgetter("question"),
     "q_a_pairs": itemgetter("q_a_pairs")} 
    | decomposition_prompt
    | llm
    | StrOutputParser())

    answer = rag_chain.invoke({"question":q,"q_a_pairs":q_a_pairs})
    q_a_pair = format_qa_pair(q,answer)
    q_a_pairs = q_a_pairs + "\n---\n"+  q_a_pair
'''
344/118: decomposition_of("how to use multi-modal models in a chain and turn chain into a rest api")
344/119:
question = "What is the significance of TreselectionNR, and how does it impact cell reselection?"
questions = decomposition_of(question)
344/120:
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser
import requests
import os
import openai



def format_qa_pair(question, answer):
    """Format Q and A pair"""
    
    formatted_string = ""
    formatted_string += f"Question: {question}\nAnswer: {answer}\n\n"
    return formatted_string.strip()



# llm
llm = ChatOllama(model="'llama3.2",temperature=0)

q_a_pairs = ""
for q in questions:
    
    rag_chain = (
    {"context": itemgetter("question") | retriever, 
     "question": itemgetter("question"),
     "q_a_pairs": itemgetter("q_a_pairs")} 
    | decomposition_prompt
    | llm
    | StrOutputParser())

    answer = rag_chain.invoke({"question":q,"q_a_pairs":q_a_pairs})
    q_a_pair = format_qa_pair(q,answer)
    q_a_pairs = q_a_pairs + "\n---\n"+  q_a_pair
344/121:
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser
import requests
import os
import openai



def format_qa_pair(question, answer):
    """Format Q and A pair"""
    
    formatted_string = ""
    formatted_string += f"Question: {question}\nAnswer: {answer}\n\n"
    return formatted_string.strip()



# llm
llm = ChatOllama(model="'llama3.2",temperature=0)

q_a_pairs = ""
for q in questions:
    
    rag_chain = (
    {"context": itemgetter("question") | retriever, 
     "question": itemgetter("question"),
     "q_a_pairs": itemgetter("q_a_pairs")} 
    | decomposition_prompt
    | llm
    | StrOutputParser())

    answer = rag_chain.invoke({"question":q,"q_a_pairs":q_a_pairs})
    q_a_pair = format_qa_pair(q,answer)
    q_a_pairs = q_a_pairs + "\n---\n"+  q_a_pair
344/122:
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser
import requests
import os
import openai



def format_qa_pair(question, answer):
    """Format Q and A pair"""
    
    formatted_string = ""
    formatted_string += f"Question: {question}\nAnswer: {answer}\n\n"
    return formatted_string.strip()



# llm
llm = ChatOllama(model="llama3.2",temperature=0)

q_a_pairs = ""
for q in questions:
    
    rag_chain = (
    {"context": itemgetter("question") | retriever, 
     "question": itemgetter("question"),
     "q_a_pairs": itemgetter("q_a_pairs")} 
    | decomposition_prompt
    | llm
    | StrOutputParser())

    answer = rag_chain.invoke({"question":q,"q_a_pairs":q_a_pairs})
    q_a_pair = format_qa_pair(q,answer)
    q_a_pairs = q_a_pairs + "\n---\n"+  q_a_pair
344/123: display(Markdown(answer))
344/124:
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser
import requests
import os
from OpenAI import openai



def format_qa_pair(question, answer):
    """Format Q and A pair"""
    
    formatted_string = ""
    formatted_string += f"Question: {question}\nAnswer: {answer}\n\n"
    return formatted_string.strip()


api_key = os.environ.get("SAMBANOVA_API_KEY")
# llm
llm = ChatOpenAI(base_url="https://api.sambanova.ai/v1/",
                 api_key = api_key,
                 streaming=True 
                 model = "Meta-Llama-3.2-1B-Instruct")

q_a_pairs = ""
for q in questions:
    
    rag_chain = (
    {"context": itemgetter("question") | retriever, 
     "question": itemgetter("question"),
     "q_a_pairs": itemgetter("q_a_pairs")} 
    | decomposition_prompt
    | llm
    | StrOutputParser())

    answer = rag_chain.invoke({"question":q,"q_a_pairs":q_a_pairs})
    q_a_pair = format_qa_pair(q,answer)
    q_a_pairs = q_a_pairs + "\n---\n"+  q_a_pair
344/125:
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser
import requests
import os
from OpenAI import openai



def format_qa_pair(question, answer):
    """Format Q and A pair"""
    
    formatted_string = ""
    formatted_string += f"Question: {question}\nAnswer: {answer}\n\n"
    return formatted_string.strip()


api_key = os.environ.get("SAMBANOVA_API_KEY")
# llm
llm = ChatOpenAI(base_url="https://api.sambanova.ai/v1/",
                 api_key = api_key,
                 streaming=True,
                 model = "Meta-Llama-3.2-1B-Instruct")

q_a_pairs = ""
for q in questions:
    
    rag_chain = (
    {"context": itemgetter("question") | retriever, 
     "question": itemgetter("question"),
     "q_a_pairs": itemgetter("q_a_pairs")} 
    | decomposition_prompt
    | llm
    | StrOutputParser())

    answer = rag_chain.invoke({"question":q,"q_a_pairs":q_a_pairs})
    q_a_pair = format_qa_pair(q,answer)
    q_a_pairs = q_a_pairs + "\n---\n"+  q_a_pair
344/126:
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser
import requests
import os
import openai



def format_qa_pair(question, answer):
    """Format Q and A pair"""
    
    formatted_string = ""
    formatted_string += f"Question: {question}\nAnswer: {answer}\n\n"
    return formatted_string.strip()


api_key = os.environ.get("SAMBANOVA_API_KEY")
# llm
llm = ChatOpenAI(base_url="https://api.sambanova.ai/v1/",
                 api_key = api_key,
                 streaming=True,
                 model = "Meta-Llama-3.2-1B-Instruct")

q_a_pairs = ""
for q in questions:
    
    rag_chain = (
    {"context": itemgetter("question") | retriever, 
     "question": itemgetter("question"),
     "q_a_pairs": itemgetter("q_a_pairs")} 
    | decomposition_prompt
    | llm
    | StrOutputParser())

    answer = rag_chain.invoke({"question":q,"q_a_pairs":q_a_pairs})
    q_a_pair = format_qa_pair(q,answer)
    q_a_pairs = q_a_pairs + "\n---\n"+  q_a_pair
344/127:
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser
import requests
import os
import openai



def format_qa_pair(question, answer):
    """Format Q and A pair"""
    
    formatted_string = ""
    formatted_string += f"Question: {question}\nAnswer: {answer}\n\n"
    return formatted_string.strip()

load_dotenv()
os.environ['SAMBANOVA_API_KEY'] = os.getenv('SAMBANOVA_API_KEY', 'your-key-if-not-using-env')

# llm
llm = ChatOpenAI(base_url="https://api.sambanova.ai/v1/",
                 api_key = os.environ['SAMBANOVA_API_KEY'],
                 streaming=True,
                 model = "Meta-Llama-3.2-1B-Instruct")

q_a_pairs = ""
for q in questions:
    
    rag_chain = (
    {"context": itemgetter("question") | retriever, 
     "question": itemgetter("question"),
     "q_a_pairs": itemgetter("q_a_pairs")} 
    | decomposition_prompt
    | llm
    | StrOutputParser())

    answer = rag_chain.invoke({"question":q,"q_a_pairs":q_a_pairs})
    q_a_pair = format_qa_pair(q,answer)
    q_a_pairs = q_a_pairs + "\n---\n"+  q_a_pair
344/128: answer
344/129: display(Markdown(answer)
344/130: display(Markdown(answer))
344/131:
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser
import requests
import os
import openai



def format_qa_pair(question, answer):
    """Format Q and A pair"""
    
    formatted_string = ""
    formatted_string += f"Question: {question}\nAnswer: {answer}\n\n"
    return formatted_string.strip()

load_dotenv()
os.environ['SAMBANOVA_API_KEY'] = os.getenv('SAMBANOVA_API_KEY', 'your-key-if-not-using-env')

# llm
llm = ChatOpenAI(base_url="https://api.sambanova.ai/v1/",
                 api_key = os.environ['SAMBANOVA_API_KEY'],
                 streaming=True,
                 model = "Meta-Llama-3.2-3B-Instruct")

q_a_pairs = ""
for q in questions:
    
    rag_chain = (
    {"context": itemgetter("question") | retriever, 
     "question": itemgetter("question"),
     "q_a_pairs": itemgetter("q_a_pairs")} 
    | decomposition_prompt
    | llm
    | StrOutputParser())

    answer = rag_chain.invoke({"question":q,"q_a_pairs":q_a_pairs})
    q_a_pair = format_qa_pair(q,answer)
    q_a_pairs = q_a_pairs + "\n---\n"+  q_a_pair
344/132: display(Markdown(answer))
344/133: display(Markdown(answer))
344/134: q_a_pairs
344/135:
question = "How does the UE determine a "suitable cell" for camping? Which parameters does it evaluate?"
questions = decomposition_of(question)
344/136:
question = "How does the UE determine a 'suitable cell' for camping? Which parameters does it evaluate?"
questions = decomposition_of(question)
344/137: answer
344/138:
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser
import requests
import os
import openai



def format_qa_pair(question, answer):
    """Format Q and A pair"""
    
    formatted_string = ""
    formatted_string += f"Question: {question}\nAnswer: {answer}\n\n"
    return formatted_string.strip()

    
load_dotenv()
os.environ['SAMBANOVA_API_KEY'] = os.getenv('SAMBANOVA_API_KEY', 'your-key-if-not-using-env')

# llm
llm = ChatOpenAI(base_url="https://api.sambanova.ai/v1/",
                 api_key = os.environ['SAMBANOVA_API_KEY'],
                 streaming=True,
                 model = "Meta-Llama-3.2-3B-Instruct")

q_a_pairs = ""

# HafÄ±za

for q in questions:
    relevant_docs = retriever.get_relevant_documents(q)  # Soruya uygun dokÃ¼manlar dÃ¶ner

    context = "\n\n".join([doc["content"] for doc in relevant_docs])
    rag_chain = (
    {"context": itemgetter("question") | retriever, 
     "question": itemgetter("question"),
     "q_a_pairs": itemgetter("q_a_pairs")} 
    | decomposition_prompt
    | llm
    | StrOutputParser())

    answer = rag_chain.invoke({"question":q,"q_a_pairs":q_a_pairs})
    q_a_pair = format_qa_pair(q,answer)
    q_a_pairs = q_a_pairs + "\n---\n"+  q_a_pair
344/139:
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser
import requests
import os
import openai



def format_qa_pair(question, answer):
    """Format Q and A pair"""
    
    formatted_string = ""
    formatted_string += f"Question: {question}\nAnswer: {answer}\n\n"
    return formatted_string.strip()

    
load_dotenv()
os.environ['SAMBANOVA_API_KEY'] = os.getenv('SAMBANOVA_API_KEY', 'your-key-if-not-using-env')

# llm
llm = ChatOpenAI(base_url="https://api.sambanova.ai/v1/",
                 api_key = os.environ['SAMBANOVA_API_KEY'],
                 streaming=True,
                 model = "Meta-Llama-3.2-3B-Instruct")

q_a_pairs = ""

# HafÄ±za

for q in questions:
    relevant_docs = retriever.get_relevant_documents(q)  # Soruya uygun dokÃ¼manlar dÃ¶ner
    context = "\n\n".join([doc["context"] for doc in relevant_docs])
    rag_chain = (
    {"context": itemgetter("question") | retriever, 
     "question": itemgetter("question"),
     "q_a_pairs": itemgetter("q_a_pairs")} 
    | decomposition_prompt
    | llm
    | StrOutputParser())

    answer = rag_chain.invoke({"question":q,"q_a_pairs":q_a_pairs})
    q_a_pair = format_qa_pair(q,answer)
    q_a_pairs = q_a_pairs + "\n---\n"+  q_a_pair
344/140:
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser
import requests
import os
import openai



def format_qa_pair(question, answer):
    """Format Q and A pair"""
    
    formatted_string = ""
    formatted_string += f"Question: {question}\nAnswer: {answer}\n\n"
    return formatted_string.strip()

    
load_dotenv()
os.environ['SAMBANOVA_API_KEY'] = os.getenv('SAMBANOVA_API_KEY', 'your-key-if-not-using-env')

# llm
llm = ChatOpenAI(base_url="https://api.sambanova.ai/v1/",
                 api_key = os.environ['SAMBANOVA_API_KEY'],
                 streaming=True,
                 model = "Meta-Llama-3.2-3B-Instruct")

q_a_pairs = ""

# HafÄ±za

for q in questions:
    relevant_docs = retriever.get_relevant_documents(q)  # Soruya uygun dokÃ¼manlar dÃ¶ner

    context = "\n\n".join([doc["content"] for doc in relevant_docs])
    rag_chain = (
    {"context": itemgetter("question") | retriever, 
     "question": itemgetter("question"),
     "q_a_pairs": itemgetter("q_a_pairs")} 
    | decomposition_prompt
    | llm
    | StrOutputParser())

    answer = rag_chain.invoke({"question":q,"q_a_pairs":q_a_pairs})
    q_a_pair = format_qa_pair(q,answer)
    q_a_pairs = q_a_pairs + "\n---\n"+  q_a_pair
344/141:
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser
import requests
import os
import openai



def format_qa_pair(question, answer):
    """Format Q and A pair"""
    
    formatted_string = ""
    formatted_string += f"Question: {question}\nAnswer: {answer}\n\n"
    return formatted_string.strip()

    
load_dotenv()
os.environ['SAMBANOVA_API_KEY'] = os.getenv('SAMBANOVA_API_KEY', 'your-key-if-not-using-env')

# llm
llm = ChatOpenAI(base_url="https://api.sambanova.ai/v1/",
                 api_key = os.environ['SAMBANOVA_API_KEY'],
                 streaming=True,
                 model = "Meta-Llama-3.2-3B-Instruct")

q_a_pairs = ""

# HafÄ±za

for q in questions:
    relevant_docs = retriever.get_relevant_documents(q)  # Soruya uygun dokÃ¼manlar dÃ¶ner

    context = "\n\n".join([doc.page_content for doc in relevant_docs])

    rag_chain = (
    {"context": itemgetter("question") | retriever, 
     "question": itemgetter("question"),
     "q_a_pairs": itemgetter("q_a_pairs")} 
    | decomposition_prompt
    | llm
    | StrOutputParser())

    answer = rag_chain.invoke({"question":q,"q_a_pairs":q_a_pairs})
    q_a_pair = format_qa_pair(q,answer)
    q_a_pairs = q_a_pairs + "\n---\n"+  q_a_pair
344/142: context
344/143:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
model = ChatOllama(
    model="gpt-4o",
    temperature=0
)

# HafÄ±za
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore Ã¼zerinden baÄlanÄ±yor


# Conversational Retrieval Chain tanÄ±mlÄ±yoruz
conversation_chain = ConversationalRetrievalChain.from_llm(llm=model, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score from 0 to 1 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1  
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

# Ãrnek inputlar (dict formatÄ±nda)
inputs = {
    questions:"A "suitable cell" is determined based on criteria like:
Signal Strength (Srxlev): Must exceed a minimum threshold.
Signal Quality (Squal): Must exceed a minimum quality level.
The cell must belong to the selected PLMN or SNPN and not be barred or restricted."}

# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma
for query, ground_truth in inputs.items():
    # Conversational Retrieval Chain'den yanÄ±t al
    response = answer  # Modelin yanÄ±tÄ±
    context = context  # Modelin baÄlamÄ±
    
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=context,
        question=query,
        response=response,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = model.invoke({"prompt": grader_prompt})
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": query,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result["text"]
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
344/144:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
model = ChatOllama(
    model="gpt-4o",
    temperature=0
)

# HafÄ±za
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore Ã¼zerinden baÄlanÄ±yor


# Conversational Retrieval Chain tanÄ±mlÄ±yoruz
conversation_chain = ConversationalRetrievalChain.from_llm(llm=model, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score from 0 to 1 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1  
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

# Ãrnek inputlar (dict formatÄ±nda)
inputs = {
    questions:"A 'suitable cell' is determined based on criteria like:
Signal Strength (Srxlev): Must exceed a minimum threshold.
Signal Quality (Squal): Must exceed a minimum quality level.
The cell must belong to the selected PLMN or SNPN and not be barred or restricted."}

# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma
for query, ground_truth in inputs.items():
    # Conversational Retrieval Chain'den yanÄ±t al
    response = answer  # Modelin yanÄ±tÄ±
    context = context  # Modelin baÄlamÄ±
    
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=context,
        question=query,
        response=response,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = model.invoke({"prompt": grader_prompt})
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": query,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result["text"]
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
344/145:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
model = ChatOllama(
    model="gpt-4o",
    temperature=0
)

# HafÄ±za
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore Ã¼zerinden baÄlanÄ±yor


# Conversational Retrieval Chain tanÄ±mlÄ±yoruz
conversation_chain = ConversationalRetrievalChain.from_llm(llm=model, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score from 0 to 1 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1  
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

# Ãrnek inputlar (dict formatÄ±nda)
inputs = {"questions": "A 'suitable cell' is determined based on criteria like:\n"
                       "Signal Strength (Srxlev): Must exceed a minimum threshold.\n"
                       "Signal Quality (Squal): Must exceed a minimum quality level.\n"
                       "The cell must belong to the selected PLMN or SNPN and not be barred or restricted."}


# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma
for query, ground_truth in inputs.items():
    # Conversational Retrieval Chain'den yanÄ±t al
    response = answer  # Modelin yanÄ±tÄ±
    context = context  # Modelin baÄlamÄ±
    
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=context,
        question=query,
        response=response,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = model.invoke({"prompt": grader_prompt})
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": query,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result["text"]
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
344/146:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
model = ChatOllama(
    model="gpt-4o",
    temperature=0
)

# HafÄ±za
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore Ã¼zerinden baÄlanÄ±yor


# Conversational Retrieval Chain tanÄ±mlÄ±yoruz
conversation_chain = ConversationalRetrievalChain.from_llm(llm=model, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score from 0 to 1 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1  
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

# Ãrnek inputlar (dict formatÄ±nda)
inputs = {"questions": "A 'suitable cell' is determined based on criteria like:\n"
                       "Signal Strength (Srxlev): Must exceed a minimum threshold.\n"
                       "Signal Quality (Squal): Must exceed a minimum quality level.\n"
                       "The cell must belong to the selected PLMN or SNPN and not be barred or restricted."}


# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma
for query, ground_truth in inputs.items():
    # Conversational Retrieval Chain'den yanÄ±t al
    response = answer  # Modelin yanÄ±tÄ±
    context = context  # Modelin baÄlamÄ±
    
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=context,
        question=query,
        response=answer,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = model.invoke({"prompt": grader_prompt})
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": query,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result["text"]
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
344/147:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
model = ChatOllama(
    model="gpt-4o",
    temperature=0
)

# HafÄ±za
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore Ã¼zerinden baÄlanÄ±yor


# Conversational Retrieval Chain tanÄ±mlÄ±yoruz
conversation_chain = ConversationalRetrievalChain.from_llm(llm=model, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score from 0 to 1 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1  
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

# Ãrnek inputlar (dict formatÄ±nda)
inputs = {"questions": "A 'suitable cell' is determined based on criteria like:\n"
                       "Signal Strength (Srxlev): Must exceed a minimum threshold.\n"
                       "Signal Quality (Squal): Must exceed a minimum quality level.\n"
                       "The cell must belong to the selected PLMN or SNPN and not be barred or restricted."}


# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma
for query, ground_truth in inputs.items():
    # Conversational Retrieval Chain'den yanÄ±t al
    response = answer  # Modelin yanÄ±tÄ±
    context = context  # Modelin baÄlamÄ±
    
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=context,
        question=query,
        response=answer,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = model.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": query,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result["text"]
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
344/148:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)

# HafÄ±za
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore Ã¼zerinden baÄlanÄ±yor


# Conversational Retrieval Chain tanÄ±mlÄ±yoruz
conversation_chain = ConversationalRetrievalChain.from_llm(llm=model, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score from 0 to 1 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1  
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

# Ãrnek inputlar (dict formatÄ±nda)
inputs = {"questions": "A 'suitable cell' is determined based on criteria like:\n"
                       "Signal Strength (Srxlev): Must exceed a minimum threshold.\n"
                       "Signal Quality (Squal): Must exceed a minimum quality level.\n"
                       "The cell must belong to the selected PLMN or SNPN and not be barred or restricted."}


# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma
for query, ground_truth in inputs.items():
    # Conversational Retrieval Chain'den yanÄ±t al
    response = answer  # Modelin yanÄ±tÄ±
    context = context  # Modelin baÄlamÄ±
    
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=context,
        question=query,
        response=answer,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = model.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": query,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result["text"]
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
344/149:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)

# HafÄ±za
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore Ã¼zerinden baÄlanÄ±yor


# Conversational Retrieval Chain tanÄ±mlÄ±yoruz
conversation_chain = ConversationalRetrievalChain.from_llm(llm=model, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score from 0 to 1 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1  
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

# Ãrnek inputlar (dict formatÄ±nda)
inputs = {"questions": "A 'suitable cell' is determined based on criteria like:\n"
                       "Signal Strength (Srxlev): Must exceed a minimum threshold.\n"
                       "Signal Quality (Squal): Must exceed a minimum quality level.\n"
                       "The cell must belong to the selected PLMN or SNPN and not be barred or restricted."}


# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma
for query, ground_truth in inputs.items():
    # Conversational Retrieval Chain'den yanÄ±t al
    response = answer  # Modelin yanÄ±tÄ±
    context = context  # Modelin baÄlamÄ±
    
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=context,
        question=query,
        response=answer,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = model.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": query,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
344/150:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)

# HafÄ±za
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore Ã¼zerinden baÄlanÄ±yor


# Conversational Retrieval Chain tanÄ±mlÄ±yoruz
conversation_chain = ConversationalRetrievalChain.from_llm(llm=model, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score from 0 to 1 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1  
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

# Ãrnek inputlar (dict formatÄ±nda)
inputs = {questions: "A 'suitable cell' is determined based on criteria like:\n"
                       "Signal Strength (Srxlev): Must exceed a minimum threshold.\n"
                       "Signal Quality (Squal): Must exceed a minimum quality level.\n"
                       "The cell must belong to the selected PLMN or SNPN and not be barred or restricted."}


# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma
for query, ground_truth in inputs.items():
    # Conversational Retrieval Chain'den yanÄ±t al
    response = answer  # Modelin yanÄ±tÄ±
    context = context  # Modelin baÄlamÄ±
    
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=context,
        question=query,
        response=answer,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = model.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": query,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
344/151:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)

# HafÄ±za
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore Ã¼zerinden baÄlanÄ±yor


# Conversational Retrieval Chain tanÄ±mlÄ±yoruz
conversation_chain = ConversationalRetrievalChain.from_llm(llm=model, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score from 0 to 1 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1  
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

# Ãrnek inputlar (dict formatÄ±nda)
inputs = {question: "A 'suitable cell' is determined based on criteria like:\n"
                       "Signal Strength (Srxlev): Must exceed a minimum threshold.\n"
                       "Signal Quality (Squal): Must exceed a minimum quality level.\n"
                       "The cell must belong to the selected PLMN or SNPN and not be barred or restricted."}


# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma
for query, ground_truth in inputs.items():
    # Conversational Retrieval Chain'den yanÄ±t al
    response = answer  # Modelin yanÄ±tÄ±
    context = context  # Modelin baÄlamÄ±
    
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=context,
        question=query,
        response=answer,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = model.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": query,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
344/152:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)

# HafÄ±za
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore Ã¼zerinden baÄlanÄ±yor


# Conversational Retrieval Chain tanÄ±mlÄ±yoruz
conversation_chain = ConversationalRetrievalChain.from_llm(llm=model, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score from 0 to 1 such as 0.5 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1  
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

# Ãrnek inputlar (dict formatÄ±nda)
inputs = {question: "A 'suitable cell' is determined based on criteria like:\n"
                       "Signal Strength (Srxlev): Must exceed a minimum threshold.\n"
                       "Signal Quality (Squal): Must exceed a minimum quality level.\n"
                       "The cell must belong to the selected PLMN or SNPN and not be barred or restricted."}


# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma
for query, ground_truth in inputs.items():
    # Conversational Retrieval Chain'den yanÄ±t al
    response = answer  # Modelin yanÄ±tÄ±
    context = context  # Modelin baÄlamÄ±
    
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=context,
        question=query,
        response=answer,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = model.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": query,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
344/153:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)

# HafÄ±za
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore Ã¼zerinden baÄlanÄ±yor


# Conversational Retrieval Chain tanÄ±mlÄ±yoruz
conversation_chain = ConversationalRetrievalChain.from_llm(llm=model, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.5 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1  
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

# Ãrnek inputlar (dict formatÄ±nda)
inputs = {question: "A 'suitable cell' is determined based on criteria like:\n"
                       "Signal Strength (Srxlev): Must exceed a minimum threshold.\n"
                       "Signal Quality (Squal): Must exceed a minimum quality level.\n"
                       "The cell must belong to the selected PLMN or SNPN and not be barred or restricted."}


# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma
for query, ground_truth in inputs.items():
    # Conversational Retrieval Chain'den yanÄ±t al
    response = answer  # Modelin yanÄ±tÄ±
    context = context  # Modelin baÄlamÄ±
    
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=context,
        question=query,
        response=answer,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = model.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": query,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
344/154:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)

# HafÄ±za
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore Ã¼zerinden baÄlanÄ±yor


# Conversational Retrieval Chain tanÄ±mlÄ±yoruz
conversation_chain = ConversationalRetrievalChain.from_llm(llm=model, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.5 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  --> such as 0.47/1
Explanation: [Your explanation]

Answer Relevance: X/1 --> such as 0.47/1 
Explanation: [Your explanation]

Context Relevance: X/1  --> such as 0.47/1
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  --> such as 0.47/1
Explanation: [Your explanation]
"""

# Ãrnek inputlar (dict formatÄ±nda)
inputs = {question: "A 'suitable cell' is determined based on criteria like:\n"
                       "Signal Strength (Srxlev): Must exceed a minimum threshold.\n"
                       "Signal Quality (Squal): Must exceed a minimum quality level.\n"
                       "The cell must belong to the selected PLMN or SNPN and not be barred or restricted."}


# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma
for query, ground_truth in inputs.items():
    # Conversational Retrieval Chain'den yanÄ±t al
    response = answer  # Modelin yanÄ±tÄ±
    context = context  # Modelin baÄlamÄ±
    
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=context,
        question=query,
        response=answer,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = model.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": query,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
344/155:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)

# HafÄ±za
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore Ã¼zerinden baÄlanÄ±yor


# Conversational Retrieval Chain tanÄ±mlÄ±yoruz
conversation_chain = ConversationalRetrievalChain.from_llm(llm=model, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context. Your score should be between 0 and 1. Such as 0.45.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.Your score should be between 0 and 1. Such as 0.45.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.Your score should be between 0 and 1. Such as 0.45.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.Your score should be between 0 and 1. Such as 0.45.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.534 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1 
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

# Ãrnek inputlar (dict formatÄ±nda)
inputs = {question: "A 'suitable cell' is determined based on criteria like:\n"
                       "Signal Strength (Srxlev): Must exceed a minimum threshold.\n"
                       "Signal Quality (Squal): Must exceed a minimum quality level.\n"
                       "The cell must belong to the selected PLMN or SNPN and not be barred or restricted."}


# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma
for query, ground_truth in inputs.items():
    # Conversational Retrieval Chain'den yanÄ±t al
    response = answer  # Modelin yanÄ±tÄ±
    context = context  # Modelin baÄlamÄ±
    
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=context,
        question=query,
        response=answer,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = model.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": query,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
344/156: question = "What are the procedures the UE follows during PLMN selection in both automatic and manual modes?"
344/157:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)

# HafÄ±za
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore Ã¼zerinden baÄlanÄ±yor


# Conversational Retrieval Chain tanÄ±mlÄ±yoruz
conversation_chain = ConversationalRetrievalChain.from_llm(llm=model, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context. Your score should be between 0 and 1. Such as 0.45.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.Your score should be between 0 and 1. Such as 0.45.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.Your score should be between 0 and 1. Such as 0.45.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.Your score should be between 0 and 1. Such as 0.45.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.534 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1 
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

# Ãrnek inputlar (dict formatÄ±nda)
inputs = {question: "In automatic mode, the UE searches for and selects a PLMN based on pre-configured priority lists or previously stored information.
In manual mode, the user selects the PLMN from a list of available networks provided by the UE after scanning."}


# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma
for query, ground_truth in inputs.items():
    # Conversational Retrieval Chain'den yanÄ±t al
    response = answer  # Modelin yanÄ±tÄ±
    context = context  # Modelin baÄlamÄ±
    
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=context,
        question=query,
        response=answer,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = model.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": query,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
344/158:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)

# HafÄ±za
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore Ã¼zerinden baÄlanÄ±yor


# Conversational Retrieval Chain tanÄ±mlÄ±yoruz
conversation_chain = ConversationalRetrievalChain.from_llm(llm=model, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context. Your score should be between 0 and 1. Such as 0.45.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.Your score should be between 0 and 1. Such as 0.45.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.Your score should be between 0 and 1. Such as 0.45.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.Your score should be between 0 and 1. Such as 0.45.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.534 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1 
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

# Ãrnek inputlar (dict formatÄ±nda)
inputs = {
    question: """In automatic mode, the UE searches for and selects a PLMN based on pre-configured priority lists or previously stored information.
In manual mode, the user selects the PLMN from a list of available networks provided by the UE after scanning."""
}



# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma
for query, ground_truth in inputs.items():
    # Conversational Retrieval Chain'den yanÄ±t al
    response = answer  # Modelin yanÄ±tÄ±
    context = context  # Modelin baÄlamÄ±
    
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=context,
        question=query,
        response=answer,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = model.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": query,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
356/1: doc.get_toc()
356/2: import pymupdf
356/3: doc = pymupdf.open(pdf_path)  # or pymupdf.Document(filename)
356/4:
pdf_path = "example/38304-gb0.pdf"
doc = pymupdf.open(pdf_path)  # or pymupdf.Document(filename)
356/5: doc.get_toc()
356/6:
toc = pdf.get_toc()
for i, item in enumerate(toc):
    heading = item[1]
    start_page = item[2]
    print(heading,start_page)
    print()
356/7:
for item in toc:
    level, heading, page_num = item
    # Metadata formatÄ±nÄ± oluÅtur
    metadata = f"{heading} (Page {page_num})"
    page = doc.load_page(page_num - 1)  # 0-indexed
    #page.set_text("footer", metadata, fontsize=10, location=(20, 10))  # footer konumu, font boyutu vs.

# PDF'i kaydet
output_pdf_path = "output_with_metadata.pdf"
doc.save(output_pdf_path)
356/8:
import os
import glob
from dotenv import load_dotenv
356/9:
#MODEL = Ollama(model="llama3.2")
MODEL = "gpt-4o-mini"
db_name = "vector-database"
356/10:
toc = pdf.get_toc()
for i, item in enumerate(toc):
    heading = item[1]
    start_page = item[2]
    print(heading,start_page)
    print()
356/11:
toc = doc.get_toc()
for i, item in enumerate(toc):
    heading = item[1]
    start_page = item[2]
    print(heading,start_page)
    print()
356/12:

from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
import fitz  # PyMuPDF

# PDF dosyasÄ±nÄ± yÃ¼kleme

loader = PyMuPDFLoader(pdf_path)
documents = loader.load()
pdf = fitz.open(pdf_path)

# TOC'yi al
toc = pdf.get_toc()

# LangChain iÃ§in yeni Document objeleri oluÅturma
chunked_documents = []

for i, item in enumerate(toc):
    heading = item[1]
    start_page = item[2]
    
    # Sonraki baÅlÄ±ÄÄ±n sayfasÄ±na kadar olan kÄ±smÄ± almak
    if i + 1 < len(toc):
        end_page = toc[i + 1][2] - 1
    else:
        end_page = pdf.page_count  # Son baÅlÄ±ksa PDF'in son sayfasÄ±na kadar

    # Metni birleÅtirerek chunk oluÅtur
    chunk_text = ""
    for page_num in range(start_page - 1, end_page):  # PyMuPDF iÃ§in 0-index
        chunk_text += pdf[page_num].get_text()

    # Ä°lk chunk'larÄ± oluÅtur ve listeye ekle
    chunked_documents.append(
        Document(
            page_content=chunk_text,
            metadata={"heading": heading, "start_page": start_page, "end_page": end_page}
        )
    )

# CharacterTextSplitter kullanarak ek parÃ§alama
text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=10)

# TOC'ye gÃ¶re bÃ¶lÃ¼nmÃ¼Å metinleri tekrar split ediyoruz
final_chunks = text_splitter.split_documents(chunked_documents)

# ÃÄ±ktÄ±yÄ± kontrol etmek iÃ§in
356/13:
for chunk in final_chunks:
    print(chunk)
    print(80*"--")
356/14:
question = "What are the three main states of a UE in the RRC protocol? How do they differ?"
len(vectorstore.similarity_search(question))
retriever = vectorstore.as_retriever()
356/15:
from langchain_ollama import OllamaEmbeddings

embedding = OllamaEmbeddings(
    model="nomic-embed-text",
)
if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embedding).delete_collection()

vectorstore = Chroma.from_documents(documents=final_chunks, embedding=embedding, persist_directory=db_name)
356/16: from langchain_chroma import Chroma
356/17:
from langchain_ollama import OllamaEmbeddings

embedding = OllamaEmbeddings(
    model="nomic-embed-text",
)
if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embedding).delete_collection()

vectorstore = Chroma.from_documents(documents=final_chunks, embedding=embedding, persist_directory=db_name)
356/18: answer
356/19:
question = "What are the three main states of a UE in the RRC protocol? How do they differ?"
len(vectorstore.similarity_search(question))
retriever = vectorstore.as_retriever()
356/20:
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from openai import OpenAI
load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
openai = OpenAI()
356/21:
from langchain.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_ollama import ChatOllama

def decomposition_of(question):
# Decomposition
    template = """You are a helpful assistant that generates multiple sub-questions related to an input question. \n
    The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \n
    Generate multiple search queries related to: {question} \n
    Output (Just write 3 queries, no another answer such as:Here are..):"""
    prompt_decomposition = ChatPromptTemplate.from_template(template)
    
    #LLM
    llm = ChatOpenAI(model="gpt-4o",temperature=0)

    # Chain
    generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split("\n")))

    # Run
    questions = generate_queries_decomposition.invoke({"question":question})
    return questions
356/22: decomposition_of("How does the UE handle cell reselection priorities during inter-RAT transitions?")
356/23:
# Prompt
template = """Here is the question you need to answer:

\n --- \n {question} \n --- \n

Here is any available background question + answer pairs:

\n --- \n {q_a_pairs} \n --- \n

Here is additional context relevant to the question: 

\n --- \n {context} \n --- \n

Use the above context and any background question + answer pairs to answer the question: \n {question}
"""

decomposition_prompt = ChatPromptTemplate.from_template(template)
356/24:
question = "How does the UE handle cell reselection priorities during inter-RAT transitions?"
questions = decomposition_of(question)
356/25:
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser
import requests
import os
import openai



def format_qa_pair(question, answer):
    """Format Q and A pair"""
    
    formatted_string = ""
    formatted_string += f"Question: {question}\nAnswer: {answer}\n\n"
    return formatted_string.strip()

    
load_dotenv()
os.environ['SAMBANOVA_API_KEY'] = os.getenv('SAMBANOVA_API_KEY', 'your-key-if-not-using-env')

# llm
llm = ChatOpenAI(base_url="https://api.sambanova.ai/v1/",
                 api_key = os.environ['SAMBANOVA_API_KEY'],
                 streaming=True,
                 model = "Meta-Llama-3.2-3B-Instruct")

q_a_pairs = ""

# HafÄ±za

for q in questions:
    relevant_docs = retriever.get_relevant_documents(q)  # Soruya uygun dokÃ¼manlar dÃ¶ner

    context = "\n\n".join([doc.page_content for doc in relevant_docs])

    rag_chain = (
    {"context": itemgetter("question") | retriever, 
     "question": itemgetter("question"),
     "q_a_pairs": itemgetter("q_a_pairs")} 
    | decomposition_prompt
    | llm
    | StrOutputParser())

    answer = rag_chain.invoke({"question":q,"q_a_pairs":q_a_pairs})
    q_a_pair = format_qa_pair(q,answer)
    q_a_pairs = q_a_pairs + "\n---\n"+  q_a_pair
356/26:
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser
import requests
import os
import openai



def format_qa_pair(question, answer):
    """Format Q and A pair"""
    
    formatted_string = ""
    formatted_string += f"Question: {question}\nAnswer: {answer}\n\n"
    return formatted_string.strip()

    
load_dotenv()
os.environ['SAMBANOVA_API_KEY'] = os.getenv('SAMBANOVA_API_KEY', 'your-key-if-not-using-env')

# llm
llm = ChatOpenAI(base_url="https://api.sambanova.ai/v1/",
                 api_key = os.environ['SAMBANOVA_API_KEY'],
                 streaming=True,
                 model = "Meta-Llama-3.2-3B-Instruct")

q_a_pairs = ""

# HafÄ±za

for q in questions:

    rag_chain = (
    {"context": itemgetter("question") | retriever, 
     "question": itemgetter("question"),
     "q_a_pairs": itemgetter("q_a_pairs")} 
    | decomposition_prompt
    | llm
    | StrOutputParser())

    answer = rag_chain.invoke({"question":q,"q_a_pairs":q_a_pairs})
    q_a_pair = format_qa_pair(q,answer)
    q_a_pairs = q_a_pairs + "\n---\n"+  q_a_pair
356/27:
question = " What parameters affect cell reselection in the presence of high mobility?"
questions = decomposition_of(question)
356/28:
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser
import requests
import os
import openai



def format_qa_pair(question, answer):
    """Format Q and A pair"""
    
    formatted_string = ""
    formatted_string += f"Question: {question}\nAnswer: {answer}\n\n"
    return formatted_string.strip()

    
load_dotenv()
os.environ['SAMBANOVA_API_KEY'] = os.getenv('SAMBANOVA_API_KEY', 'your-key-if-not-using-env')

# llm
llm = ChatOpenAI(base_url="https://api.sambanova.ai/v1/",
                 api_key = os.environ['SAMBANOVA_API_KEY'],
                 streaming=True,
                 model = "Meta-Llama-3.2-3B-Instruct")

q_a_pairs = ""

# HafÄ±za

for q in questions:

    rag_chain = (
    {"context": itemgetter("question") | retriever, 
     "question": itemgetter("question"),
     "q_a_pairs": itemgetter("q_a_pairs")} 
    | decomposition_prompt
    | llm
    | StrOutputParser())

    answer = rag_chain.invoke({"question":q,"q_a_pairs":q_a_pairs})
    q_a_pair = format_qa_pair(q,answer)
    q_a_pairs = q_a_pairs + "\n---\n"+  q_a_pair
356/29:
question = "What are the differences between the RRC_IDLE and RRC_INACTIVE states in terms of UE behavior??"
questions = decomposition_of(question)
356/30:
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser
import requests
import os
import openai



def format_qa_pair(question, answer):
    """Format Q and A pair"""
    
    formatted_string = ""
    formatted_string += f"Question: {question}\nAnswer: {answer}\n\n"
    return formatted_string.strip()

    
load_dotenv()
os.environ['SAMBANOVA_API_KEY'] = os.getenv('SAMBANOVA_API_KEY', 'your-key-if-not-using-env')

# llm
llm = ChatOpenAI(base_url="https://api.sambanova.ai/v1/",
                 api_key = os.environ['SAMBANOVA_API_KEY'],
                 streaming=True,
                 model = "Meta-Llama-3.2-3B-Instruct")

q_a_pairs = ""

# HafÄ±za

for q in questions:

    rag_chain = (
    {"context": itemgetter("question") | retriever, 
     "question": itemgetter("question"),
     "q_a_pairs": itemgetter("q_a_pairs")} 
    | decomposition_prompt
    | llm
    | StrOutputParser())

    answer = rag_chain.invoke({"question":q,"q_a_pairs":q_a_pairs})
    q_a_pair = format_qa_pair(q,answer)
    q_a_pairs = q_a_pairs + "\n---\n"+  q_a_pair
356/31:
question = "What are the differences between the RRC_IDLE and RRC_INACTIVE states in terms of UE behavior?"
questions = decomposition_of(question)
356/32:
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser
import requests
import os
import openai



def format_qa_pair(question, answer):
    """Format Q and A pair"""
    
    formatted_string = ""
    formatted_string += f"Question: {question}\nAnswer: {answer}\n\n"
    return formatted_string.strip()

    
load_dotenv()
os.environ['SAMBANOVA_API_KEY'] = os.getenv('SAMBANOVA_API_KEY', 'your-key-if-not-using-env')

# llm
llm = ChatOpenAI(base_url="https://api.sambanova.ai/v1/",
                 api_key = os.environ['SAMBANOVA_API_KEY'],
                 streaming=True,
                 model = "Meta-Llama-3.2-3B-Instruct")

q_a_pairs = ""

# HafÄ±za

for q in questions:

    rag_chain = (
    {"context": itemgetter("question") | retriever, 
     "question": itemgetter("question"),
     "q_a_pairs": itemgetter("q_a_pairs")} 
    | decomposition_prompt
    | llm
    | StrOutputParser())

    answer = rag_chain.invoke({"question":q,"q_a_pairs":q_a_pairs})
    q_a_pair = format_qa_pair(q,answer)
    q_a_pairs = q_a_pairs + "\n---\n"+  q_a_pair
356/33: decomposition_of("Explain the difference between acceptable cells and suitable cells.")
356/34:
question = "Explain the difference between \"acceptable cells\" and \"suitable cells.\""
"
questions = decomposition_of(question)
356/35:
question = "Explain the difference between \"acceptable cells\" and \"suitable cells.\""

questions = decomposition_of(question)
356/36:
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser
import requests
import os
import openai



def format_qa_pair(question, answer):
    """Format Q and A pair"""
    
    formatted_string = ""
    formatted_string += f"Question: {question}\nAnswer: {answer}\n\n"
    return formatted_string.strip()

    
load_dotenv()
os.environ['SAMBANOVA_API_KEY'] = os.getenv('SAMBANOVA_API_KEY', 'your-key-if-not-using-env')

# llm
llm = ChatOpenAI(base_url="https://api.sambanova.ai/v1/",
                 api_key = os.environ['SAMBANOVA_API_KEY'],
                 streaming=True,
                 model = "Meta-Llama-3.2-3B-Instruct")

q_a_pairs = ""

# HafÄ±za

for q in questions:

    rag_chain = (
    {"context": itemgetter("question") | retriever, 
     "question": itemgetter("question"),
     "q_a_pairs": itemgetter("q_a_pairs")} 
    | decomposition_prompt
    | llm
    | StrOutputParser())

    answer = rag_chain.invoke({"question":q,"q_a_pairs":q_a_pairs})
    q_a_pair = format_qa_pair(q,answer)
    q_a_pairs = q_a_pairs + "\n---\n"+  q_a_pair
356/37: display(Markdown(answer))
356/38:
# Prompt
template = """Here is the question you need to answer:

\n --- \n {question} \n --- \n

Here is any available background question + answer pairs:

\n --- \n {q_a_pairs} \n --- \n

Here is additional context relevant to the question: 

\n --- \n {context} \n --- \n

Use the above context and any background question + answer pairs to answer the question: \n {question}
"""

decomposition_prompt = ChatPromptTemplate.from_template(template)
356/39:
question = "Explain the difference between \"acceptable cells\" and \"suitable cells.\""

questions = decomposition_of(question)
356/40:
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser
import requests
import os
import openai



def format_qa_pair(question, answer):
    """Format Q and A pair"""
    
    formatted_string = ""
    formatted_string += f"Question: {question}\nAnswer: {answer}\n\n"
    return formatted_string.strip()

    
load_dotenv()
os.environ['SAMBANOVA_API_KEY'] = os.getenv('SAMBANOVA_API_KEY', 'your-key-if-not-using-env')

# llm
llm = ChatOpenAI(base_url="https://api.sambanova.ai/v1/",
                 api_key = os.environ['SAMBANOVA_API_KEY'],
                 streaming=True,
                 model = "Meta-Llama-3.2-3B-Instruct")

q_a_pairs = ""

# HafÄ±za

for q in questions:

    rag_chain = (
    {"context": itemgetter("question") | retriever, 
     "question": itemgetter("question"),
     "q_a_pairs": itemgetter("q_a_pairs")} 
    | decomposition_prompt
    | llm
    | StrOutputParser())

    answer = rag_chain.invoke({"question":q,"q_a_pairs":q_a_pairs})
    q_a_pair = format_qa_pair(q,answer)
    q_a_pairs = q_a_pairs + "\n---\n"+  q_a_pair
356/41:
from IPython.display import Markdown, display, update_display
display(Markdown(answer))
356/42:
question = "What are the three main states of a UE in the RRC protocol? How do they differ?"
len(vectorstore.similarity_search(question))
retriever = vectorstore.as_retriever
vectorstore.similarity_search(question
356/43:
question = "What are the three main states of a UE in the RRC protocol? How do they differ?"
len(vectorstore.similarity_search(question))
retriever = vectorstore.as_retriever
vectorstore.similarity_search(question)
356/44:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
'''
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)
'''
# Memory
#memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore 


# Conversational Retrieval Chain
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context. Your score should be between 0 and 1. Such as 0.45.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.Your score should be between 0 and 1. Such as 0.45.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.Your score should be between 0 and 1. Such as 0.45.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.Your score should be between 0 and 1. Such as 0.45.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.534 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1 
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

# Ãrnek inputlar (dict formatÄ±nda)
inputs = {
    question: """In automatic mode, the UE searches for and selects a PLMN based on pre-configured priority lists or previously stored information.
In manual mode, the user selects the PLMN from a list of available networks provided by the UE after scanning."""
}

questions_answers = [
    {
        "question": "What are the differences between the RRC_IDLE and RRC_INACTIVE states in terms of UE behavior?",
        "answer": "In RRC_IDLE, the UE camps on a cell, monitors system information, and performs paging while not connected. In RRC_INACTIVE, the UE maintains a context with the network but does not actively transmit or receive, allowing faster resumption of connection. Both states share procedures like PLMN selection and cell reselection."
    },
    {
        "question": "How does the UE determine a 'suitable cell' for camping? Which parameters does it evaluate?",
        "answer": "A 'suitable cell' is determined based on criteria like signal strength (Srxlev), signal quality (Squal), and whether the cell belongs to the selected PLMN or SNPN. The cell must not be barred or restricted."
    }
]

# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma

for qa_pair in questions_answers:
    question = qa_pair["question"]
    ground_truth = qa_pair["answer"]

    response = llm.invoke({"question": question, "context": vectorstore.similarity_search(question)})
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=vectorstore.similarity_search(question),
        question=query,
        response=response.content,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = llm.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": query,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
356/45:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
'''
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)
'''
# Memory
#memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore 


# Conversational Retrieval Chain
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context. Your score should be between 0 and 1. Such as 0.45.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.Your score should be between 0 and 1. Such as 0.45.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.Your score should be between 0 and 1. Such as 0.45.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.Your score should be between 0 and 1. Such as 0.45.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.534 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1 
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

# Ãrnek inputlar (dict formatÄ±nda)
inputs = {
    question: """In automatic mode, the UE searches for and selects a PLMN based on pre-configured priority lists or previously stored information.
In manual mode, the user selects the PLMN from a list of available networks provided by the UE after scanning."""
}

questions_answers = [
    {
        "question": "What are the differences between the RRC_IDLE and RRC_INACTIVE states in terms of UE behavior?",
        "answer": "In RRC_IDLE, the UE camps on a cell, monitors system information, and performs paging while not connected. In RRC_INACTIVE, the UE maintains a context with the network but does not actively transmit or receive, allowing faster resumption of connection. Both states share procedures like PLMN selection and cell reselection."
    },
    {
        "question": "How does the UE determine a 'suitable cell' for camping? Which parameters does it evaluate?",
        "answer": "A 'suitable cell' is determined based on criteria like signal strength (Srxlev), signal quality (Squal), and whether the cell belongs to the selected PLMN or SNPN. The cell must not be barred or restricted."
    }
]

# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma

for qa_pair in questions_answers:
    question = qa_pair["question"]
    ground_truth = qa_pair["answer"]

    response = llm.invoke(f"Context: {vectorstore.similarity_search(question)}\nQuestion: {question}")
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=vectorstore.similarity_search(question),
        question=query,
        response=response.content,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = llm.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": query,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
356/46:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
'''
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)
'''
# Memory
#memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore 


# Conversational Retrieval Chain
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context. Your score should be between 0 and 1. Such as 0.45.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.Your score should be between 0 and 1. Such as 0.45.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.Your score should be between 0 and 1. Such as 0.45.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.Your score should be between 0 and 1. Such as 0.45.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.534 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1 
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

# Ãrnek inputlar (dict formatÄ±nda)
inputs = {
    question: """In automatic mode, the UE searches for and selects a PLMN based on pre-configured priority lists or previously stored information.
In manual mode, the user selects the PLMN from a list of available networks provided by the UE after scanning."""
}

questions_answers = [
    {
        "question": "What are the differences between the RRC_IDLE and RRC_INACTIVE states in terms of UE behavior?",
        "answer": "In RRC_IDLE, the UE camps on a cell, monitors system information, and performs paging while not connected. In RRC_INACTIVE, the UE maintains a context with the network but does not actively transmit or receive, allowing faster resumption of connection. Both states share procedures like PLMN selection and cell reselection."
    },
    {
        "question": "How does the UE determine a 'suitable cell' for camping? Which parameters does it evaluate?",
        "answer": "A 'suitable cell' is determined based on criteria like signal strength (Srxlev), signal quality (Squal), and whether the cell belongs to the selected PLMN or SNPN. The cell must not be barred or restricted."
    }
]

# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma

for qa_pair in questions_answers:
    question = qa_pair["question"]
    ground_truth = qa_pair["answer"]

    response = llm.invoke(f"Context: {vectorstore.similarity_search(question)}\nQuestion: {question}")
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=vectorstore.similarity_search(question),
        question=query,
        response=response.content,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = llm.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": question,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
356/47:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
'''
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)
'''
# Memory
#memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore 


# Conversational Retrieval Chain
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context. Your score should be between 0 and 1. Such as 0.45.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.Your score should be between 0 and 1. Such as 0.45.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.Your score should be between 0 and 1. Such as 0.45.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.Your score should be between 0 and 1. Such as 0.45.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.534 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1 
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

# Ãrnek inputlar (dict formatÄ±nda)
inputs = {
    question: """In automatic mode, the UE searches for and selects a PLMN based on pre-configured priority lists or previously stored information.
In manual mode, the user selects the PLMN from a list of available networks provided by the UE after scanning."""
}

questions_answers = [
    {
        "question": "What are the differences between the RRC_IDLE and RRC_INACTIVE states in terms of UE behavior?",
        "answer": "In RRC_IDLE, the UE camps on a cell, monitors system information, and performs paging while not connected. In RRC_INACTIVE, the UE maintains a context with the network but does not actively transmit or receive, allowing faster resumption of connection. Both states share procedures like PLMN selection and cell reselection."
    },
    {
        "question": "How does the UE determine a 'suitable cell' for camping? Which parameters does it evaluate?",
        "answer": "A 'suitable cell' is determined based on criteria like signal strength (Srxlev), signal quality (Squal), and whether the cell belongs to the selected PLMN or SNPN. The cell must not be barred or restricted."
    }
]

# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma

for qa_pair in questions_answers:
    question = qa_pair["question"]
    ground_truth = qa_pair["answer"]

    response = llm.invoke(f"Context: {vectorstore.similarity_search(question)}\nQuestion: {question}")
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=vectorstore.similarity_search(question),
        question=question,
        response=response.content,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = llm.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": question,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
356/48:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
'''
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)
'''
# Memory
#memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore 


# Conversational Retrieval Chain
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context. Your score should be between 0 and 1. Such as 0.45.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.Your score should be between 0 and 1. Such as 0.45.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.Your score should be between 0 and 1. Such as 0.45.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.Your score should be between 0 and 1. Such as 0.45.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.534 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1 
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

MAX_TOKENS = 4096

def truncate_to_token_limit(text, token_limit=MAX_TOKENS, encoding_name="cl100k_base"):
    encoding = get_encoding(encoding_name)
    tokens = encoding.encode(text)
    if len(tokens) > token_limit:
        truncated_tokens = tokens[:token_limit]
        return encoding.decode(truncated_tokens)
    return text


questions_answers = [
    {
        "question": "What are the differences between the RRC_IDLE and RRC_INACTIVE states in terms of UE behavior?",
        "answer": "In RRC_IDLE, the UE camps on a cell, monitors system information, and performs paging while not connected. In RRC_INACTIVE, the UE maintains a context with the network but does not actively transmit or receive, allowing faster resumption of connection. Both states share procedures like PLMN selection and cell reselection."
    },
    {
        "question": "How does the UE determine a 'suitable cell' for camping? Which parameters does it evaluate?",
        "answer": "A 'suitable cell' is determined based on criteria like signal strength (Srxlev), signal quality (Squal), and whether the cell belongs to the selected PLMN or SNPN. The cell must not be barred or restricted."
    }
]

# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma

for qa_pair in questions_answers:
    question = qa_pair["question"]
    ground_truth = qa_pair["answer"]

    truncated_context = truncate_to_token_limit(vectorstore.similarity_search(question, token_limit=2000)
    input_message = HumanMessage(
        content=f"Context: {truncated_context}\n\nQuestion: {question}"
    )
    response = llm.invoke([input_message])
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=vectorstore.similarity_search(question),
        question=question,
        response=response.content,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = llm.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": question,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
356/49:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
'''
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)
'''
# Memory
#memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore 


# Conversational Retrieval Chain
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context. Your score should be between 0 and 1. Such as 0.45.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.Your score should be between 0 and 1. Such as 0.45.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.Your score should be between 0 and 1. Such as 0.45.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.Your score should be between 0 and 1. Such as 0.45.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.534 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1 
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

MAX_TOKENS = 4096

def truncate_to_token_limit(text, token_limit=MAX_TOKENS, encoding_name="cl100k_base"):
    encoding = get_encoding(encoding_name)
    tokens = encoding.encode(text)
    if len(tokens) > token_limit:
        truncated_tokens = tokens[:token_limit]
        return encoding.decode(truncated_tokens)
    return text


questions_answers = [
    {
        "question": "What are the differences between the RRC_IDLE and RRC_INACTIVE states in terms of UE behavior?",
        "answer": "In RRC_IDLE, the UE camps on a cell, monitors system information, and performs paging while not connected. In RRC_INACTIVE, the UE maintains a context with the network but does not actively transmit or receive, allowing faster resumption of connection. Both states share procedures like PLMN selection and cell reselection."
    },
    {
        "question": "How does the UE determine a 'suitable cell' for camping? Which parameters does it evaluate?",
        "answer": "A 'suitable cell' is determined based on criteria like signal strength (Srxlev), signal quality (Squal), and whether the cell belongs to the selected PLMN or SNPN. The cell must not be barred or restricted."
    }
]

# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma

for qa_pair in questions_answers:
    question = qa_pair["question"]
    ground_truth = qa_pair["answer"]

    truncated_context = truncate_to_token_limit(vectorstore.similarity_search(question, token_limit=2000))
    input_message = HumanMessage(
        content=f"Context: {truncated_context}\n\nQuestion: {question}"
    )
    response = llm.invoke([input_message])
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=vectorstore.similarity_search(question),
        question=question,
        response=response.content,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = llm.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": question,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
356/50:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
'''
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)
'''
# Memory
#memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore 


# Conversational Retrieval Chain
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context. Your score should be between 0 and 1. Such as 0.45.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.Your score should be between 0 and 1. Such as 0.45.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.Your score should be between 0 and 1. Such as 0.45.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.Your score should be between 0 and 1. Such as 0.45.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.534 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1 
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

MAX_TOKENS = 4096

def truncate_to_token_limit(text, token_limit=MAX_TOKENS, encoding_name="cl100k_base"):
    encoding = get_encoding(encoding_name)
    tokens = encoding.encode(text)
    if len(tokens) > token_limit:
        truncated_tokens = tokens[:token_limit]
        return encoding.decode(truncated_tokens)
    return text


questions_answers = [
    {
        "question": "What are the differences between the RRC_IDLE and RRC_INACTIVE states in terms of UE behavior?",
        "answer": "In RRC_IDLE, the UE camps on a cell, monitors system information, and performs paging while not connected. In RRC_INACTIVE, the UE maintains a context with the network but does not actively transmit or receive, allowing faster resumption of connection. Both states share procedures like PLMN selection and cell reselection."
    },
    {
        "question": "How does the UE determine a 'suitable cell' for camping? Which parameters does it evaluate?",
        "answer": "A 'suitable cell' is determined based on criteria like signal strength (Srxlev), signal quality (Squal), and whether the cell belongs to the selected PLMN or SNPN. The cell must not be barred or restricted."
    }
]

# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma

for qa_pair in questions_answers:
    question = qa_pair["question"]
    ground_truth = qa_pair["answer"]

    truncated_context = truncate_to_token_limit(vectorstore.similarity_search(question, token_limit=2000))
    input_message = HumanMessage(
        content=f"Context: {truncated_context}\n\nQuestion: {question}"
    )
    response = llm.invoke([input_message])
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=vectorstore.similarity_search(question),
        question=question,
        response=response.content,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = llm.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": question,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
356/51:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
'''
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)
'''
# Memory
#memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore 


# Conversational Retrieval Chain
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context. Your score should be between 0 and 1. Such as 0.45.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.Your score should be between 0 and 1. Such as 0.45.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.Your score should be between 0 and 1. Such as 0.45.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.Your score should be between 0 and 1. Such as 0.45.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.534 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1 
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

MAX_TOKENS = 4096

def truncate_to_token_limit(text, token_limit=MAX_TOKENS, encoding_name="cl100k_base"):
    encoding = get_encoding(encoding_name)
    tokens = encoding.encode(text)
    if len(tokens) > token_limit:
        truncated_tokens = tokens[:token_limit]
        return encoding.decode(truncated_tokens)
    return text


questions_answers = [
    {
        "question": "What are the differences between the RRC_IDLE and RRC_INACTIVE states in terms of UE behavior?",
        "answer": "In RRC_IDLE, the UE camps on a cell, monitors system information, and performs paging while not connected. In RRC_INACTIVE, the UE maintains a context with the network but does not actively transmit or receive, allowing faster resumption of connection. Both states share procedures like PLMN selection and cell reselection."
    },
    {
        "question": "How does the UE determine a 'suitable cell' for camping? Which parameters does it evaluate?",
        "answer": "A 'suitable cell' is determined based on criteria like signal strength (Srxlev), signal quality (Squal), and whether the cell belongs to the selected PLMN or SNPN. The cell must not be barred or restricted."
    }
]

# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma

for qa_pair in questions_answers:
    question = qa_pair["question"]
    ground_truth = qa_pair["answer"]

    truncated_context = truncate_to_token_limit(vectorstore.similarity_search(question,k=5), token_limit=2000))
    input_message = HumanMessage(
        content=f"Context: {truncated_context}\n\nQuestion: {question}"
    )
    response = llm.invoke([input_message])
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=vectorstore.similarity_search(question),
        question=question,
        response=response.content,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = llm.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": question,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
356/52:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
'''
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)
'''
# Memory
#memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore 


# Conversational Retrieval Chain
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context. Your score should be between 0 and 1. Such as 0.45.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.Your score should be between 0 and 1. Such as 0.45.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.Your score should be between 0 and 1. Such as 0.45.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.Your score should be between 0 and 1. Such as 0.45.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.534 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1 
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

MAX_TOKENS = 4096

def truncate_to_token_limit(text, token_limit=MAX_TOKENS, encoding_name="cl100k_base"):
    encoding = get_encoding(encoding_name)
    tokens = encoding.encode(text)
    if len(tokens) > token_limit:
        truncated_tokens = tokens[:token_limit]
        return encoding.decode(truncated_tokens)
    return text


questions_answers = [
    {
        "question": "What are the differences between the RRC_IDLE and RRC_INACTIVE states in terms of UE behavior?",
        "answer": "In RRC_IDLE, the UE camps on a cell, monitors system information, and performs paging while not connected. In RRC_INACTIVE, the UE maintains a context with the network but does not actively transmit or receive, allowing faster resumption of connection. Both states share procedures like PLMN selection and cell reselection."
    },
    {
        "question": "How does the UE determine a 'suitable cell' for camping? Which parameters does it evaluate?",
        "answer": "A 'suitable cell' is determined based on criteria like signal strength (Srxlev), signal quality (Squal), and whether the cell belongs to the selected PLMN or SNPN. The cell must not be barred or restricted."
    }
]

# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma

for qa_pair in questions_answers:
    question = qa_pair["question"]
    ground_truth = qa_pair["answer"]

    truncated_context = truncate_to_token_limit(vectorstore.similarity_search(question,k=5), token_limit=2000)
    input_message = HumanMessage(
        content=f"Context: {truncated_context}\n\nQuestion: {question}"
    )
    response = llm.invoke([input_message])
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=vectorstore.similarity_search(question),
        question=question,
        response=response.content,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = llm.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": question,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
356/53: !pip install tiktoken
356/54:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from tiktoken import get_encoding

# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
'''
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)
'''
# Memory
#memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore 


# Conversational Retrieval Chain
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context. Your score should be between 0 and 1. Such as 0.45.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.Your score should be between 0 and 1. Such as 0.45.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.Your score should be between 0 and 1. Such as 0.45.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.Your score should be between 0 and 1. Such as 0.45.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.534 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1 
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

MAX_TOKENS = 4096

def truncate_to_token_limit(text, token_limit=MAX_TOKENS, encoding_name="cl100k_base"):
    encoding = get_encoding(encoding_name)
    tokens = encoding.encode(text)
    if len(tokens) > token_limit:
        truncated_tokens = tokens[:token_limit]
        return encoding.decode(truncated_tokens)
    return text


questions_answers = [
    {
        "question": "What are the differences between the RRC_IDLE and RRC_INACTIVE states in terms of UE behavior?",
        "answer": "In RRC_IDLE, the UE camps on a cell, monitors system information, and performs paging while not connected. In RRC_INACTIVE, the UE maintains a context with the network but does not actively transmit or receive, allowing faster resumption of connection. Both states share procedures like PLMN selection and cell reselection."
    },
    {
        "question": "How does the UE determine a 'suitable cell' for camping? Which parameters does it evaluate?",
        "answer": "A 'suitable cell' is determined based on criteria like signal strength (Srxlev), signal quality (Squal), and whether the cell belongs to the selected PLMN or SNPN. The cell must not be barred or restricted."
    }
]

# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma

for qa_pair in questions_answers:
    question = qa_pair["question"]
    ground_truth = qa_pair["answer"]

    truncated_context = truncate_to_token_limit(vectorstore.similarity_search(question,k=5), token_limit=2000)
    input_message = HumanMessage(
        content=f"Context: {truncated_context}\n\nQuestion: {question}"
    )
    response = llm.invoke([input_message])
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=vectorstore.similarity_search(question),
        question=question,
        response=response.content,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = llm.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": question,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
356/55:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from tiktoken import get_encoding

# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
'''
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)
'''
# Memory
#memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore 


# Conversational Retrieval Chain
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context. Your score should be between 0 and 1. Such as 0.45.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.Your score should be between 0 and 1. Such as 0.45.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.Your score should be between 0 and 1. Such as 0.45.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.Your score should be between 0 and 1. Such as 0.45.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.534 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1 
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

MAX_TOKENS = 4096

def truncate_to_token_limit(text, token_limit=MAX_TOKENS, encoding_name="cl100k_base"):
    encoding = get_encoding(encoding_name)
    tokens = encoding.encode(text)
    if len(tokens) > token_limit:
        truncated_tokens = tokens[:token_limit]
        return encoding.decode(truncated_tokens)
    return text


questions_answers = [
    {
        "question": "What are the differences between the RRC_IDLE and RRC_INACTIVE states in terms of UE behavior?",
        "answer": "In RRC_IDLE, the UE camps on a cell, monitors system information, and performs paging while not connected. In RRC_INACTIVE, the UE maintains a context with the network but does not actively transmit or receive, allowing faster resumption of connection. Both states share procedures like PLMN selection and cell reselection."
    },
    {
        "question": "How does the UE determine a 'suitable cell' for camping? Which parameters does it evaluate?",
        "answer": "A 'suitable cell' is determined based on criteria like signal strength (Srxlev), signal quality (Squal), and whether the cell belongs to the selected PLMN or SNPN. The cell must not be barred or restricted."
    }
]

# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma

for qa_pair in questions_answers:
    question = qa_pair["question"]
    ground_truth = qa_pair["answer"]
    # similarity_search Ã§Ä±ktÄ±sÄ±nÄ± stringe dÃ¶nÃ¼ÅtÃ¼r
    retrieved_contexts = vectorstore.similarity_search(question, k=5)
    combined_context = " ".join([doc.page_content for doc in retrieved_contexts])

    truncated_context = truncate_to_token_limit(combined_context), token_limit=2000)
    input_message = HumanMessage(
        content=f"Context: {truncated_context}\n\nQuestion: {question}"
    )
    response = llm.invoke([input_message])
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=vectorstore.similarity_search(question),
        question=question,
        response=response.content,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = llm.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": question,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
356/56:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from tiktoken import get_encoding

# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
'''
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)
'''
# Memory
#memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore 


# Conversational Retrieval Chain
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context. Your score should be between 0 and 1. Such as 0.45.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.Your score should be between 0 and 1. Such as 0.45.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.Your score should be between 0 and 1. Such as 0.45.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.Your score should be between 0 and 1. Such as 0.45.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.534 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1 
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

MAX_TOKENS = 4096

def truncate_to_token_limit(text, token_limit=MAX_TOKENS, encoding_name="cl100k_base"):
    encoding = get_encoding(encoding_name)
    tokens = encoding.encode(text)
    if len(tokens) > token_limit:
        truncated_tokens = tokens[:token_limit]
        return encoding.decode(truncated_tokens)
    return text


questions_answers = [
    {
        "question": "What are the differences between the RRC_IDLE and RRC_INACTIVE states in terms of UE behavior?",
        "answer": "In RRC_IDLE, the UE camps on a cell, monitors system information, and performs paging while not connected. In RRC_INACTIVE, the UE maintains a context with the network but does not actively transmit or receive, allowing faster resumption of connection. Both states share procedures like PLMN selection and cell reselection."
    },
    {
        "question": "How does the UE determine a 'suitable cell' for camping? Which parameters does it evaluate?",
        "answer": "A 'suitable cell' is determined based on criteria like signal strength (Srxlev), signal quality (Squal), and whether the cell belongs to the selected PLMN or SNPN. The cell must not be barred or restricted."
    }
]

# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma

for qa_pair in questions_answers:
    question = qa_pair["question"]
    ground_truth = qa_pair["answer"]
    # similarity_search Ã§Ä±ktÄ±sÄ±nÄ± stringe dÃ¶nÃ¼ÅtÃ¼r
    retrieved_contexts = vectorstore.similarity_search(question, k=5)
    combined_context = " ".join([doc.page_content for doc in retrieved_contexts])

    truncated_context = truncate_to_token_limit((combined_context), token_limit=2000)
    input_message = HumanMessage(
        content=f"Context: {truncated_context}\n\nQuestion: {question}"
    )
    response = llm.invoke([input_message])
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=vectorstore.similarity_search(question),
        question=question,
        response=response.content,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = llm.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": question,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
356/57:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from tiktoken import get_encoding
from langchain.schema import HumanMessage
# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
'''
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)
'''
# Memory
#memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore 


# Conversational Retrieval Chain
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context. Your score should be between 0 and 1. Such as 0.45.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.Your score should be between 0 and 1. Such as 0.45.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.Your score should be between 0 and 1. Such as 0.45.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.Your score should be between 0 and 1. Such as 0.45.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.534 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1 
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

MAX_TOKENS = 4096

def truncate_to_token_limit(text, token_limit=MAX_TOKENS, encoding_name="cl100k_base"):
    encoding = get_encoding(encoding_name)
    tokens = encoding.encode(text)
    if len(tokens) > token_limit:
        truncated_tokens = tokens[:token_limit]
        return encoding.decode(truncated_tokens)
    return text


questions_answers = [
    {
        "question": "What are the differences between the RRC_IDLE and RRC_INACTIVE states in terms of UE behavior?",
        "answer": "In RRC_IDLE, the UE camps on a cell, monitors system information, and performs paging while not connected. In RRC_INACTIVE, the UE maintains a context with the network but does not actively transmit or receive, allowing faster resumption of connection. Both states share procedures like PLMN selection and cell reselection."
    },
    {
        "question": "How does the UE determine a 'suitable cell' for camping? Which parameters does it evaluate?",
        "answer": "A 'suitable cell' is determined based on criteria like signal strength (Srxlev), signal quality (Squal), and whether the cell belongs to the selected PLMN or SNPN. The cell must not be barred or restricted."
    }
]

# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma

for qa_pair in questions_answers:
    question = qa_pair["question"]
    ground_truth = qa_pair["answer"]
    # similarity_search Ã§Ä±ktÄ±sÄ±nÄ± stringe dÃ¶nÃ¼ÅtÃ¼r
    retrieved_contexts = vectorstore.similarity_search(question, k=5)
    combined_context = " ".join([doc.page_content for doc in retrieved_contexts])

    truncated_context = truncate_to_token_limit((combined_context), token_limit=2000)
    input_message = HumanMessage(
        content=f"Context: {truncated_context}\n\nQuestion: {question}"
    )
    response = llm.invoke([input_message])
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=vectorstore.similarity_search(question),
        question=question,
        response=response.content,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = llm.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": question,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
356/58:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from tiktoken import get_encoding
from langchain.schema import HumanMessage
# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
'''
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)
'''
# Memory
#memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore 


# Conversational Retrieval Chain
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context. Your score should be between 0 and 1. Such as 0.45.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.Your score should be between 0 and 1. Such as 0.45.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.Your score should be between 0 and 1. Such as 0.45.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.Your score should be between 0 and 1. Such as 0.45.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.534 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1 
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

MAX_TOKENS = 4096

def truncate_to_token_limit(text, token_limit=MAX_TOKENS, encoding_name="cl100k_base"):
    encoding = get_encoding(encoding_name)
    tokens = encoding.encode(text)
    if len(tokens) > token_limit:
        truncated_tokens = tokens[:token_limit]
        return encoding.decode(truncated_tokens)
    return text


questions_answers = [
    {
        "question": "What are the differences between the RRC_IDLE and RRC_INACTIVE states in terms of UE behavior?",
        "answer": "In RRC_IDLE, the UE camps on a cell, monitors system information, and performs paging while not connected. In RRC_INACTIVE, the UE maintains a context with the network but does not actively transmit or receive, allowing faster resumption of connection. Both states share procedures like PLMN selection and cell reselection."
    },
    {
        "question": "How does the UE determine a 'suitable cell' for camping? Which parameters does it evaluate?",
        "answer": "A 'suitable cell' is determined based on criteria like signal strength (Srxlev), signal quality (Squal), and whether the cell belongs to the selected PLMN or SNPN. The cell must not be barred or restricted."
    }
]

# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma

for qa_pair in questions_answers:
    question = qa_pair["question"]
    ground_truth = qa_pair["answer"]
    # similarity_search Ã§Ä±ktÄ±sÄ±nÄ± stringe dÃ¶nÃ¼ÅtÃ¼r
    retrieved_contexts = vectorstore.similarity_search(question, k=5)
    combined_context = " ".join([doc.page_content for doc in retrieved_contexts])

    truncated_context = truncate_to_token_limit((combined_context), token_limit=1100)
    input_message = HumanMessage(
        content=f"Context: {truncated_context}\n\nQuestion: {question}"
    )
    response = llm.invoke([input_message])
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=vectorstore.similarity_search(question),
        question=question,
        response=response.content,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = llm.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": question,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
356/59:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from tiktoken import get_encoding
from langchain.schema import HumanMessage
# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
'''
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)
'''
# Memory
#memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore 


# Conversational Retrieval Chain
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context. Your score should be between 0 and 1. Such as 0.45.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.Your score should be between 0 and 1. Such as 0.45.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.Your score should be between 0 and 1. Such as 0.45.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.Your score should be between 0 and 1. Such as 0.45.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.534 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1 
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

MAX_TOKENS = 4096

def truncate_to_token_limit(text, token_limit=MAX_TOKENS, encoding_name="cl100k_base"):
    encoding = get_encoding(encoding_name)
    tokens = encoding.encode(text)
    if len(tokens) > token_limit:
        truncated_tokens = tokens[:token_limit]
        return encoding.decode(truncated_tokens)
    return text


questions_answers = [
    {
        "question": "What are the differences between the RRC_IDLE and RRC_INACTIVE states in terms of UE behavior?",
        "answer": "In RRC_IDLE, the UE camps on a cell, monitors system information, and performs paging while not connected. In RRC_INACTIVE, the UE maintains a context with the network but does not actively transmit or receive, allowing faster resumption of connection. Both states share procedures like PLMN selection and cell reselection."
    },
    {
        "question": "How does the UE determine a 'suitable cell' for camping? Which parameters does it evaluate?",
        "answer": "A 'suitable cell' is determined based on criteria like signal strength (Srxlev), signal quality (Squal), and whether the cell belongs to the selected PLMN or SNPN. The cell must not be barred or restricted."
    }
]

# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma

for qa_pair in questions_answers:
    question = qa_pair["question"]
    ground_truth = qa_pair["answer"]
    # similarity_search Ã§Ä±ktÄ±sÄ±nÄ± stringe dÃ¶nÃ¼ÅtÃ¼r
    retrieved_contexts = vectorstore.similarity_search(question, k=5)
    combined_context = " ".join([doc.page_content for doc in retrieved_contexts])

    truncated_context = truncate_to_token_limit((combined_context), token_limit=100)
    input_message = HumanMessage(
        content=f"Context: {truncated_context}\n\nQuestion: {question}"
    )
    response = llm.invoke([input_message])
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=vectorstore.similarity_search(question),
        question=question,
        response=response.content,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = llm.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": question,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
356/60:
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser
import requests
import os
import openai



def format_qa_pair(question, answer):
    """Format Q and A pair"""
    
    formatted_string = ""
    formatted_string += f"Question: {question}\nAnswer: {answer}\n\n"
    return formatted_string.strip()

    
load_dotenv()
os.environ['SAMBANOVA_API_KEY'] = os.getenv('SAMBANOVA_API_KEY', 'your-key-if-not-using-env')

# llm
llm = ChatOpenAI(base_url="https://api.sambanova.ai/v1/",
                 api_key = os.environ['SAMBANOVA_API_KEY'],
                 streaming=True,
                 max_tokens=4096,
                 model = "Meta-Llama-3.2-3B-Instruct")

q_a_pairs = ""

# HafÄ±za
356/61:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from tiktoken import get_encoding
from langchain.schema import HumanMessage
# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
'''
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)
'''
# Memory
#memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore 


# Conversational Retrieval Chain
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context. Your score should be between 0 and 1. Such as 0.45.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.Your score should be between 0 and 1. Such as 0.45.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.Your score should be between 0 and 1. Such as 0.45.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.Your score should be between 0 and 1. Such as 0.45.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.534 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1 
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

MAX_TOKENS = 4096

def truncate_to_token_limit(text, token_limit=MAX_TOKENS, encoding_name="cl100k_base"):
    encoding = get_encoding(encoding_name)
    tokens = encoding.encode(text)
    if len(tokens) > token_limit:
        truncated_tokens = tokens[:token_limit]
        return encoding.decode(truncated_tokens)
    return text


questions_answers = [
    {
        "question": "What are the differences between the RRC_IDLE and RRC_INACTIVE states in terms of UE behavior?",
        "answer": "In RRC_IDLE, the UE camps on a cell, monitors system information, and performs paging while not connected. In RRC_INACTIVE, the UE maintains a context with the network but does not actively transmit or receive, allowing faster resumption of connection. Both states share procedures like PLMN selection and cell reselection."
    },
    {
        "question": "How does the UE determine a 'suitable cell' for camping? Which parameters does it evaluate?",
        "answer": "A 'suitable cell' is determined based on criteria like signal strength (Srxlev), signal quality (Squal), and whether the cell belongs to the selected PLMN or SNPN. The cell must not be barred or restricted."
    }
]

# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma

for qa_pair in questions_answers:
    question = qa_pair["question"]
    ground_truth = qa_pair["answer"]
    # similarity_search Ã§Ä±ktÄ±sÄ±nÄ± stringe dÃ¶nÃ¼ÅtÃ¼r
    retrieved_contexts = vectorstore.similarity_search(question, k=5)
    combined_context = " ".join([doc.page_content for doc in retrieved_contexts])

    truncated_context = truncate_to_token_limit((combined_context), token_limit=100)
    input_message = HumanMessage(
        content=f"Context: {truncated_context}\n\nQuestion: {question}"
    )
    response = llm.invoke([input_message])
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=vectorstore.similarity_search(question),
        question=question,
        response=response.content,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = llm.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": question,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
356/62:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from tiktoken import get_encoding
from langchain.schema import HumanMessage
# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
'''
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)
'''
# Memory
#memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore 


# Conversational Retrieval Chain
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context. Your score should be between 0 and 1. Such as 0.45.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.Your score should be between 0 and 1. Such as 0.45.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.Your score should be between 0 and 1. Such as 0.45.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.Your score should be between 0 and 1. Such as 0.45.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.534 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1 
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

MAX_TOKENS = 4096

def truncate_to_token_limit(text, token_limit=MAX_TOKENS, encoding_name="cl100k_base"):
    encoding = get_encoding(encoding_name)
    tokens = encoding.encode(text)
    if len(tokens) > token_limit:
        truncated_tokens = tokens[:token_limit]
        return encoding.decode(truncated_tokens)
    return text


questions_answers = [
    {
        "question": "What are the differences between the RRC_IDLE and RRC_INACTIVE states in terms of UE behavior?",
        "answer": "In RRC_IDLE, the UE camps on a cell, monitors system information, and performs paging while not connected. In RRC_INACTIVE, the UE maintains a context with the network but does not actively transmit or receive, allowing faster resumption of connection. Both states share procedures like PLMN selection and cell reselection."
    },
    {
        "question": "How does the UE determine a 'suitable cell' for camping? Which parameters does it evaluate?",
        "answer": "A 'suitable cell' is determined based on criteria like signal strength (Srxlev), signal quality (Squal), and whether the cell belongs to the selected PLMN or SNPN. The cell must not be barred or restricted."
    }
]

# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma

for qa_pair in questions_answers:
    question = qa_pair["question"]
    ground_truth = qa_pair["answer"]
    # similarity_search Ã§Ä±ktÄ±sÄ±nÄ± stringe dÃ¶nÃ¼ÅtÃ¼r
    retrieved_contexts = vectorstore.similarity_search(question, k=5)
    combined_context = " ".join([doc.page_content for doc in retrieved_contexts])

    truncated_context = truncate_to_token_limit((combined_context), token_limit=2000)
    input_message = HumanMessage(
        content=f"Context: {truncated_context}\n\nQuestion: {question}"
    )
    response = llm.invoke([input_message])
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=vectorstore.similarity_search(question),
        question=question,
        response=response.content,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = llm.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": question,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
356/63:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from tiktoken import get_encoding
from langchain.schema import HumanMessage
# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
'''
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)
'''
# Memory
#memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore 


# Conversational Retrieval Chain
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context. Your score should be between 0 and 1. Such as 0.45.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.Your score should be between 0 and 1. Such as 0.45.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.Your score should be between 0 and 1. Such as 0.45.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.Your score should be between 0 and 1. Such as 0.45.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.534 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1 
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

MAX_TOKENS = 4096

def truncate_to_token_limit(text, token_limit=MAX_TOKENS, encoding_name="cl100k_base"):
    encoding = get_encoding(encoding_name)
    tokens = encoding.encode(text)
    if len(tokens) > token_limit:
        truncated_tokens = tokens[:token_limit]
        return encoding.decode(truncated_tokens)
    return text


questions_answers = [
    {
        "question": "What are the differences between the RRC_IDLE and RRC_INACTIVE states in terms of UE behavior?",
        "answer": "In RRC_IDLE, the UE camps on a cell, monitors system information, and performs paging while not connected. In RRC_INACTIVE, the UE maintains a context with the network but does not actively transmit or receive, allowing faster resumption of connection. Both states share procedures like PLMN selection and cell reselection."
    },
    {
        "question": "How does the UE determine a 'suitable cell' for camping? Which parameters does it evaluate?",
        "answer": "A 'suitable cell' is determined based on criteria like signal strength (Srxlev), signal quality (Squal), and whether the cell belongs to the selected PLMN or SNPN. The cell must not be barred or restricted."
    }
]

# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma

for qa_pair in questions_answers:
    question = qa_pair["question"]
    ground_truth = qa_pair["answer"]
    # similarity_search Ã§Ä±ktÄ±sÄ±nÄ± stringe dÃ¶nÃ¼ÅtÃ¼r
    retrieved_contexts = vectorstore.similarity_search(question, k=5)
    combined_context = " ".join([doc.page_content for doc in retrieved_contexts])

    truncated_context = truncate_to_token_limit((combined_context), token_limit=4000)
    input_message = HumanMessage(
        content=f"Context: {truncated_context}\n\nQuestion: {question}"
    )
    response = llm.invoke([input_message])
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=vectorstore.similarity_search(question),
        question=question,
        response=response.content,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = llm.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": question,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
356/64:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from tiktoken import get_encoding
from langchain.schema import HumanMessage
# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
'''
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)
'''
# Memory
#memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore 


# Conversational Retrieval Chain
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context. Your score should be between 0 and 1. Such as 0.45.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.Your score should be between 0 and 1. Such as 0.45.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.Your score should be between 0 and 1. Such as 0.45.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.Your score should be between 0 and 1. Such as 0.45.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.534 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1 
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

MAX_TOKENS = 4096

def truncate_to_token_limit(text, token_limit=MAX_TOKENS, encoding_name="cl100k_base"):
    encoding = get_encoding(encoding_name)
    tokens = encoding.encode(text)
    token_size = len(tokens)
    global token_size
    if len(tokens) > token_limit:
        truncated_tokens = tokens[:token_limit]
        return encoding.decode(truncated_tokens)
    return text


questions_answers = [
    {
        "question": "What are the differences between the RRC_IDLE and RRC_INACTIVE states in terms of UE behavior?",
        "answer": "In RRC_IDLE, the UE camps on a cell, monitors system information, and performs paging while not connected. In RRC_INACTIVE, the UE maintains a context with the network but does not actively transmit or receive, allowing faster resumption of connection. Both states share procedures like PLMN selection and cell reselection."
    },
    {
        "question": "How does the UE determine a 'suitable cell' for camping? Which parameters does it evaluate?",
        "answer": "A 'suitable cell' is determined based on criteria like signal strength (Srxlev), signal quality (Squal), and whether the cell belongs to the selected PLMN or SNPN. The cell must not be barred or restricted."
    }
]

llm = ChatOpenAI(base_url="https://api.sambanova.ai/v1/",
                 api_key = os.environ['SAMBANOVA_API_KEY'],
                 streaming=True,
                 max_tokens=token_size,
                 model = "Meta-Llama-3.2-3B-Instruct")


# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma

for qa_pair in questions_answers:
    question = qa_pair["question"]
    ground_truth = qa_pair["answer"]
    # similarity_search Ã§Ä±ktÄ±sÄ±nÄ± stringe dÃ¶nÃ¼ÅtÃ¼r
    retrieved_contexts = vectorstore.similarity_search(question, k=5)
    combined_context = " ".join([doc.page_content for doc in retrieved_contexts])

    truncated_context = truncate_to_token_limit((combined_context), token_limit=4000)
    input_message = HumanMessage(
        content=f"Context: {truncated_context}\n\nQuestion: {question}"
    )
    response = llm.invoke([input_message])
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=vectorstore.similarity_search(question),
        question=question,
        response=response.content,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = llm.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": question,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
356/65:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from tiktoken import get_encoding
from langchain.schema import HumanMessage
# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
'''
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)
'''
# Memory
#memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore 


# Conversational Retrieval Chain
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context. Your score should be between 0 and 1. Such as 0.45.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.Your score should be between 0 and 1. Such as 0.45.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.Your score should be between 0 and 1. Such as 0.45.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.Your score should be between 0 and 1. Such as 0.45.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.534 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1 
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

MAX_TOKENS = 4096

def truncate_to_token_limit(text, token_limit=MAX_TOKENS, encoding_name="cl100k_base"):
    encoding = get_encoding(encoding_name)
    tokens = encoding.encode(text)
     global token_size
    token_size = len(tokens)
    if len(tokens) > token_limit:
        truncated_tokens = tokens[:token_limit]
        return encoding.decode(truncated_tokens)
    return text


questions_answers = [
    {
        "question": "What are the differences between the RRC_IDLE and RRC_INACTIVE states in terms of UE behavior?",
        "answer": "In RRC_IDLE, the UE camps on a cell, monitors system information, and performs paging while not connected. In RRC_INACTIVE, the UE maintains a context with the network but does not actively transmit or receive, allowing faster resumption of connection. Both states share procedures like PLMN selection and cell reselection."
    },
    {
        "question": "How does the UE determine a 'suitable cell' for camping? Which parameters does it evaluate?",
        "answer": "A 'suitable cell' is determined based on criteria like signal strength (Srxlev), signal quality (Squal), and whether the cell belongs to the selected PLMN or SNPN. The cell must not be barred or restricted."
    }
]

llm = ChatOpenAI(base_url="https://api.sambanova.ai/v1/",
                 api_key = os.environ['SAMBANOVA_API_KEY'],
                 streaming=True,
                 max_tokens=token_size,
                 model = "Meta-Llama-3.2-3B-Instruct")


# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma

for qa_pair in questions_answers:
    question = qa_pair["question"]
    ground_truth = qa_pair["answer"]
    # similarity_search Ã§Ä±ktÄ±sÄ±nÄ± stringe dÃ¶nÃ¼ÅtÃ¼r
    retrieved_contexts = vectorstore.similarity_search(question, k=5)
    combined_context = " ".join([doc.page_content for doc in retrieved_contexts])

    truncated_context = truncate_to_token_limit((combined_context), token_limit=4000)
    input_message = HumanMessage(
        content=f"Context: {truncated_context}\n\nQuestion: {question}"
    )
    response = llm.invoke([input_message])
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=vectorstore.similarity_search(question),
        question=question,
        response=response.content,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = llm.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": question,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
356/66:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from tiktoken import get_encoding
from langchain.schema import HumanMessage
# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
'''
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)
'''
# Memory
#memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore 


# Conversational Retrieval Chain
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context. Your score should be between 0 and 1. Such as 0.45.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.Your score should be between 0 and 1. Such as 0.45.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.Your score should be between 0 and 1. Such as 0.45.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.Your score should be between 0 and 1. Such as 0.45.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.534 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1 
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

MAX_TOKENS = 4096

def truncate_to_token_limit(text, token_limit=MAX_TOKENS, encoding_name="cl100k_base"):
    encoding = get_encoding(encoding_name)
    tokens = encoding.encode(text)
    global token_size
    token_size = len(tokens)
    if len(tokens) > token_limit:
        truncated_tokens = tokens[:token_limit]
        return encoding.decode(truncated_tokens)
    return text


questions_answers = [
    {
        "question": "What are the differences between the RRC_IDLE and RRC_INACTIVE states in terms of UE behavior?",
        "answer": "In RRC_IDLE, the UE camps on a cell, monitors system information, and performs paging while not connected. In RRC_INACTIVE, the UE maintains a context with the network but does not actively transmit or receive, allowing faster resumption of connection. Both states share procedures like PLMN selection and cell reselection."
    },
    {
        "question": "How does the UE determine a 'suitable cell' for camping? Which parameters does it evaluate?",
        "answer": "A 'suitable cell' is determined based on criteria like signal strength (Srxlev), signal quality (Squal), and whether the cell belongs to the selected PLMN or SNPN. The cell must not be barred or restricted."
    }
]

llm = ChatOpenAI(base_url="https://api.sambanova.ai/v1/",
                 api_key = os.environ['SAMBANOVA_API_KEY'],
                 streaming=True,
                 max_tokens=token_size,
                 model = "Meta-Llama-3.2-3B-Instruct")


# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma

for qa_pair in questions_answers:
    question = qa_pair["question"]
    ground_truth = qa_pair["answer"]
    # similarity_search Ã§Ä±ktÄ±sÄ±nÄ± stringe dÃ¶nÃ¼ÅtÃ¼r
    retrieved_contexts = vectorstore.similarity_search(question, k=5)
    combined_context = " ".join([doc.page_content for doc in retrieved_contexts])

    truncated_context = truncate_to_token_limit((combined_context), token_limit=4000)
    input_message = HumanMessage(
        content=f"Context: {truncated_context}\n\nQuestion: {question}"
    )
    response = llm.invoke([input_message])
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=vectorstore.similarity_search(question),
        question=question,
        response=response.content,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = llm.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": question,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
356/67:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from tiktoken import get_encoding
from langchain.schema import HumanMessage
# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
'''
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)
'''
# Memory
#memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore 


# Conversational Retrieval Chain
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context. Your score should be between 0 and 1. Such as 0.45.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.Your score should be between 0 and 1. Such as 0.45.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.Your score should be between 0 and 1. Such as 0.45.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.Your score should be between 0 and 1. Such as 0.45.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.534 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1 
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

MAX_TOKENS = 4096

def truncate_to_token_limit(text, token_limit=MAX_TOKENS, encoding_name="cl100k_base"):
    encoding = get_encoding(encoding_name)
    tokens = encoding.encode(text)
    global token_size = len(tokens)
    token_size = len(tokens)
    if len(tokens) > token_limit:
        truncated_tokens = tokens[:token_limit]
        return encoding.decode(truncated_tokens)
    return text


questions_answers = [
    {
        "question": "What are the differences between the RRC_IDLE and RRC_INACTIVE states in terms of UE behavior?",
        "answer": "In RRC_IDLE, the UE camps on a cell, monitors system information, and performs paging while not connected. In RRC_INACTIVE, the UE maintains a context with the network but does not actively transmit or receive, allowing faster resumption of connection. Both states share procedures like PLMN selection and cell reselection."
    },
    {
        "question": "How does the UE determine a 'suitable cell' for camping? Which parameters does it evaluate?",
        "answer": "A 'suitable cell' is determined based on criteria like signal strength (Srxlev), signal quality (Squal), and whether the cell belongs to the selected PLMN or SNPN. The cell must not be barred or restricted."
    }
]

llm = ChatOpenAI(base_url="https://api.sambanova.ai/v1/",
                 api_key = os.environ['SAMBANOVA_API_KEY'],
                 streaming=True,
                 max_tokens=token_size,
                 model = "Meta-Llama-3.2-3B-Instruct")


# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma

for qa_pair in questions_answers:
    question = qa_pair["question"]
    ground_truth = qa_pair["answer"]
    # similarity_search Ã§Ä±ktÄ±sÄ±nÄ± stringe dÃ¶nÃ¼ÅtÃ¼r
    retrieved_contexts = vectorstore.similarity_search(question, k=5)
    combined_context = " ".join([doc.page_content for doc in retrieved_contexts])

    truncated_context = truncate_to_token_limit((combined_context), token_limit=4000)
    input_message = HumanMessage(
        content=f"Context: {truncated_context}\n\nQuestion: {question}"
    )
    response = llm.invoke([input_message])
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=vectorstore.similarity_search(question),
        question=question,
        response=response.content,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = llm.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": question,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
356/68:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from tiktoken import get_encoding
from langchain.schema import HumanMessage
# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
'''
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)
'''
# Memory
#memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore 


# Conversational Retrieval Chain
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context. Your score should be between 0 and 1. Such as 0.45.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.Your score should be between 0 and 1. Such as 0.45.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.Your score should be between 0 and 1. Such as 0.45.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.Your score should be between 0 and 1. Such as 0.45.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.534 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1 
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

MAX_TOKENS = 4096

def truncate_to_token_limit(text, token_limit=MAX_TOKENS, encoding_name="'gpt-4o-mini'"):
    encoding = get_encoding(encoding_name)
    encoding = tiktoken.encoding_for_model("gpt-4o-mini")
    tokens = encoding.encode(text)
    global token_size = len(tokens)
    token_size = len(tokens)
    if len(tokens) > token_limit:
        truncated_tokens = tokens[:token_limit]
        return encoding.decode(truncated_tokens)
    return text


questions_answers = [
    {
        "question": "What are the differences between the RRC_IDLE and RRC_INACTIVE states in terms of UE behavior?",
        "answer": "In RRC_IDLE, the UE camps on a cell, monitors system information, and performs paging while not connected. In RRC_INACTIVE, the UE maintains a context with the network but does not actively transmit or receive, allowing faster resumption of connection. Both states share procedures like PLMN selection and cell reselection."
    },
    {
        "question": "How does the UE determine a 'suitable cell' for camping? Which parameters does it evaluate?",
        "answer": "A 'suitable cell' is determined based on criteria like signal strength (Srxlev), signal quality (Squal), and whether the cell belongs to the selected PLMN or SNPN. The cell must not be barred or restricted."
    }
]

llm = ChatOpenAI(base_url="https://api.sambanova.ai/v1/",
                 api_key = os.environ['OPENAI_API_KEY'],
                 streaming=True,
                 max_tokens=token_size,
                 model = "Meta-Llama-3.2-3B-Instruct")


# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma

for qa_pair in questions_answers:
    question = qa_pair["question"]
    ground_truth = qa_pair["answer"]
    # similarity_search Ã§Ä±ktÄ±sÄ±nÄ± stringe dÃ¶nÃ¼ÅtÃ¼r
    retrieved_contexts = vectorstore.similarity_search(question, k=5)
    combined_context = " ".join([doc.page_content for doc in retrieved_contexts])

    truncated_context = truncate_to_token_limit((combined_context), token_limit=4000)
    input_message = HumanMessage(
        content=f"Context: {truncated_context}\n\nQuestion: {question}"
    )
    response = llm.invoke([input_message])
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=vectorstore.similarity_search(question),
        question=question,
        response=response.content,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = llm.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": question,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
356/69:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from tiktoken import get_encoding
from langchain.schema import HumanMessage
# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
'''
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)
'''
# Memory
#memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore 


# Conversational Retrieval Chain
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context. Your score should be between 0 and 1. Such as 0.45.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.Your score should be between 0 and 1. Such as 0.45.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.Your score should be between 0 and 1. Such as 0.45.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.Your score should be between 0 and 1. Such as 0.45.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.534 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1 
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

MAX_TOKENS = 4096

def truncate_to_token_limit(text, token_limit=MAX_TOKENS, encoding_name="'gpt-4o-mini'"):
    encoding = get_encoding(encoding_name)
    encoding = tiktoken.encoding_for_model("gpt-4o-mini")
    tokens = encoding.encode(text)
    global token_size = len(tokens)
    token_size = len(tokens)
    if len(tokens) > token_limit:
        truncated_tokens = tokens[:token_limit]
        return encoding.decode(truncated_tokens)
    return text


questions_answers = [
    {
        "question": "What are the differences between the RRC_IDLE and RRC_INACTIVE states in terms of UE behavior?",
        "answer": "In RRC_IDLE, the UE camps on a cell, monitors system information, and performs paging while not connected. In RRC_INACTIVE, the UE maintains a context with the network but does not actively transmit or receive, allowing faster resumption of connection. Both states share procedures like PLMN selection and cell reselection."
    },
    {
        "question": "How does the UE determine a 'suitable cell' for camping? Which parameters does it evaluate?",
        "answer": "A 'suitable cell' is determined based on criteria like signal strength (Srxlev), signal quality (Squal), and whether the cell belongs to the selected PLMN or SNPN. The cell must not be barred or restricted."
    }
]
'''
llm = ChatOpenAI(base_url="https://api.sambanova.ai/v1/",
                 api_key = os.environ['SAMBANOVA_API_KEY'],
                 streaming=True,
                 max_tokens=token_size,
                 model = "Meta-Llama-3.2-3B-Instruct")
'''

llm = ChatOpenAI(
                 api_key = os.environ['OPENAI_API_KEY'],
                 streaming=True,
                 max_tokens=token_size,
                 model = "gpt-4o-mini")


# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma

for qa_pair in questions_answers:
    question = qa_pair["question"]
    ground_truth = qa_pair["answer"]
    # similarity_search Ã§Ä±ktÄ±sÄ±nÄ± stringe dÃ¶nÃ¼ÅtÃ¼r
    retrieved_contexts = vectorstore.similarity_search(question, k=5)
    combined_context = " ".join([doc.page_content for doc in retrieved_contexts])

    truncated_context = truncate_to_token_limit((combined_context), token_limit=4000)
    input_message = HumanMessage(
        content=f"Context: {truncated_context}\n\nQuestion: {question}"
    )
    response = llm.invoke([input_message])
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=vectorstore.similarity_search(question),
        question=question,
        response=response.content,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = llm.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": question,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
356/70:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from tiktoken import get_encoding
from langchain.schema import HumanMessage
# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
'''
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)
'''
# Memory
#memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore 


# Conversational Retrieval Chain
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context. Your score should be between 0 and 1. Such as 0.45.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.Your score should be between 0 and 1. Such as 0.45.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.Your score should be between 0 and 1. Such as 0.45.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.Your score should be between 0 and 1. Such as 0.45.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.534 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1 
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

MAX_TOKENS = 4096

def truncate_to_token_limit(text, token_limit=MAX_TOKENS, encoding_name="'gpt-4o-mini'"):
    encoding = get_encoding(encoding_name)
    encoding = tiktoken.encoding_for_model("gpt-4o-mini")
    tokens = encoding.encode(text)
    if len(tokens) > token_limit:
        truncated_tokens = tokens[:token_limit]
        return encoding.decode(truncated_tokens)
    return text


questions_answers = [
    {
        "question": "What are the differences between the RRC_IDLE and RRC_INACTIVE states in terms of UE behavior?",
        "answer": "In RRC_IDLE, the UE camps on a cell, monitors system information, and performs paging while not connected. In RRC_INACTIVE, the UE maintains a context with the network but does not actively transmit or receive, allowing faster resumption of connection. Both states share procedures like PLMN selection and cell reselection."
    },
    {
        "question": "How does the UE determine a 'suitable cell' for camping? Which parameters does it evaluate?",
        "answer": "A 'suitable cell' is determined based on criteria like signal strength (Srxlev), signal quality (Squal), and whether the cell belongs to the selected PLMN or SNPN. The cell must not be barred or restricted."
    }
]
'''
llm = ChatOpenAI(base_url="https://api.sambanova.ai/v1/",
                 api_key = os.environ['SAMBANOVA_API_KEY'],
                 streaming=True,
                 max_tokens=token_size,
                 model = "Meta-Llama-3.2-3B-Instruct")
'''

llm = ChatOpenAI(
                 api_key = os.environ['OPENAI_API_KEY'],
                 streaming=True,
                 max_tokens=token_size,
                 model = "gpt-4o-mini")


# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma

for qa_pair in questions_answers:
    question = qa_pair["question"]
    ground_truth = qa_pair["answer"]
    # similarity_search Ã§Ä±ktÄ±sÄ±nÄ± stringe dÃ¶nÃ¼ÅtÃ¼r
    retrieved_contexts = vectorstore.similarity_search(question, k=5)
    combined_context = " ".join([doc.page_content for doc in retrieved_contexts])

    truncated_context = truncate_to_token_limit((combined_context), token_limit=4000)
    input_message = HumanMessage(
        content=f"Context: {truncated_context}\n\nQuestion: {question}"
    )
    response = llm.invoke([input_message])
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=vectorstore.similarity_search(question),
        question=question,
        response=response.content,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = llm.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": question,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
356/71:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from tiktoken import get_encoding
from langchain.schema import HumanMessage
# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
'''
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)
'''
# Memory
#memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore 


# Conversational Retrieval Chain
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context. Your score should be between 0 and 1. Such as 0.45.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.Your score should be between 0 and 1. Such as 0.45.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.Your score should be between 0 and 1. Such as 0.45.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.Your score should be between 0 and 1. Such as 0.45.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.534 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1 
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

MAX_TOKENS = 4096

def truncate_to_token_limit(text, token_limit=MAX_TOKENS, encoding_name="'gpt-4o-mini'"):
    encoding = get_encoding(encoding_name)
    encoding = tiktoken.encoding_for_model("gpt-4o-mini")
    tokens = encoding.encode(text)
    if len(tokens) > token_limit:
        truncated_tokens = tokens[:token_limit]
        return encoding.decode(truncated_tokens)
    return text


questions_answers = [
    {
        "question": "What are the differences between the RRC_IDLE and RRC_INACTIVE states in terms of UE behavior?",
        "answer": "In RRC_IDLE, the UE camps on a cell, monitors system information, and performs paging while not connected. In RRC_INACTIVE, the UE maintains a context with the network but does not actively transmit or receive, allowing faster resumption of connection. Both states share procedures like PLMN selection and cell reselection."
    },
    {
        "question": "How does the UE determine a 'suitable cell' for camping? Which parameters does it evaluate?",
        "answer": "A 'suitable cell' is determined based on criteria like signal strength (Srxlev), signal quality (Squal), and whether the cell belongs to the selected PLMN or SNPN. The cell must not be barred or restricted."
    }
]
'''
llm = ChatOpenAI(base_url="https://api.sambanova.ai/v1/",
                 api_key = os.environ['SAMBANOVA_API_KEY'],
                 streaming=True,
                 max_tokens=token_size,
                 model = "Meta-Llama-3.2-3B-Instruct")
'''

llm = ChatOpenAI(
                 api_key = os.environ['OPENAI_API_KEY'],
                 streaming=True,
                 model = "gpt-4o-mini")


# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma

for qa_pair in questions_answers:
    question = qa_pair["question"]
    ground_truth = qa_pair["answer"]
    # similarity_search Ã§Ä±ktÄ±sÄ±nÄ± stringe dÃ¶nÃ¼ÅtÃ¼r
    retrieved_contexts = vectorstore.similarity_search(question, k=5)
    combined_context = " ".join([doc.page_content for doc in retrieved_contexts])

    truncated_context = truncate_to_token_limit((combined_context), token_limit=4000)
    input_message = HumanMessage(
        content=f"Context: {truncated_context}\n\nQuestion: {question}"
    )
    response = llm.invoke([input_message])
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=vectorstore.similarity_search(question),
        question=question,
        response=response.content,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = llm.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": question,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
356/72:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from tiktoken import get_encoding
from langchain.schema import HumanMessage
# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
'''
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)
'''
# Memory
#memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore 


# Conversational Retrieval Chain
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context. Your score should be between 0 and 1. Such as 0.45.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.Your score should be between 0 and 1. Such as 0.45.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.Your score should be between 0 and 1. Such as 0.45.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.Your score should be between 0 and 1. Such as 0.45.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.534 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1 
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

MAX_TOKENS = 4096

def truncate_to_token_limit(text, token_limit=MAX_TOKENS, encoding_name="cl100k_base"):
    encoding = get_encoding(encoding_name)
    encoding = tiktoken.encoding_for_model("gpt-4o-mini")
    tokens = encoding.encode(text)
    if len(tokens) > token_limit:
        truncated_tokens = tokens[:token_limit]
        return encoding.decode(truncated_tokens)
    return text


questions_answers = [
    {
        "question": "What are the differences between the RRC_IDLE and RRC_INACTIVE states in terms of UE behavior?",
        "answer": "In RRC_IDLE, the UE camps on a cell, monitors system information, and performs paging while not connected. In RRC_INACTIVE, the UE maintains a context with the network but does not actively transmit or receive, allowing faster resumption of connection. Both states share procedures like PLMN selection and cell reselection."
    },
    {
        "question": "How does the UE determine a 'suitable cell' for camping? Which parameters does it evaluate?",
        "answer": "A 'suitable cell' is determined based on criteria like signal strength (Srxlev), signal quality (Squal), and whether the cell belongs to the selected PLMN or SNPN. The cell must not be barred or restricted."
    }
]
'''
llm = ChatOpenAI(base_url="https://api.sambanova.ai/v1/",
                 api_key = os.environ['SAMBANOVA_API_KEY'],
                 streaming=True,
                 max_tokens=token_size,
                 model = "Meta-Llama-3.2-3B-Instruct")
'''

llm = ChatOpenAI(
                 api_key = os.environ['OPENAI_API_KEY'],
                 streaming=True,
                 model = "gpt-4o-mini")


# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma

for qa_pair in questions_answers:
    question = qa_pair["question"]
    ground_truth = qa_pair["answer"]
    # similarity_search Ã§Ä±ktÄ±sÄ±nÄ± stringe dÃ¶nÃ¼ÅtÃ¼r
    retrieved_contexts = vectorstore.similarity_search(question, k=5)
    combined_context = " ".join([doc.page_content for doc in retrieved_contexts])

    truncated_context = truncate_to_token_limit((combined_context), token_limit=4000)
    input_message = HumanMessage(
        content=f"Context: {truncated_context}\n\nQuestion: {question}"
    )
    response = llm.invoke([input_message])
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=vectorstore.similarity_search(question),
        question=question,
        response=response.content,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = llm.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": question,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
356/73:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from tiktoken import get_encoding
from langchain.schema import HumanMessage
# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
'''
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)
'''
# Memory
#memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore 


# Conversational Retrieval Chain
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context. Your score should be between 0 and 1. Such as 0.45.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.Your score should be between 0 and 1. Such as 0.45.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.Your score should be between 0 and 1. Such as 0.45.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.Your score should be between 0 and 1. Such as 0.45.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.534 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1 
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

MAX_TOKENS = 4096

def truncate_to_token_limit(text, token_limit=MAX_TOKENS, encoding_name="cl100k_base"):
    encoding = get_encoding(encoding_name)
    tokens = encoding.encode(text)
    if len(tokens) > token_limit:
        truncated_tokens = tokens[:token_limit]
        return encoding.decode(truncated_tokens)
    return text


questions_answers = [
    {
        "question": "What are the differences between the RRC_IDLE and RRC_INACTIVE states in terms of UE behavior?",
        "answer": "In RRC_IDLE, the UE camps on a cell, monitors system information, and performs paging while not connected. In RRC_INACTIVE, the UE maintains a context with the network but does not actively transmit or receive, allowing faster resumption of connection. Both states share procedures like PLMN selection and cell reselection."
    },
    {
        "question": "How does the UE determine a 'suitable cell' for camping? Which parameters does it evaluate?",
        "answer": "A 'suitable cell' is determined based on criteria like signal strength (Srxlev), signal quality (Squal), and whether the cell belongs to the selected PLMN or SNPN. The cell must not be barred or restricted."
    }
]
'''
llm = ChatOpenAI(base_url="https://api.sambanova.ai/v1/",
                 api_key = os.environ['SAMBANOVA_API_KEY'],
                 streaming=True,
                 max_tokens=token_size,
                 model = "Meta-Llama-3.2-3B-Instruct")
'''

llm = ChatOpenAI(
                 api_key = os.environ['OPENAI_API_KEY'],
                 streaming=True,
                 model = "gpt-4o-mini")


# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma

for qa_pair in questions_answers:
    question = qa_pair["question"]
    ground_truth = qa_pair["answer"]
    # similarity_search Ã§Ä±ktÄ±sÄ±nÄ± stringe dÃ¶nÃ¼ÅtÃ¼r
    retrieved_contexts = vectorstore.similarity_search(question, k=5)
    combined_context = " ".join([doc.page_content for doc in retrieved_contexts])

    truncated_context = truncate_to_token_limit((combined_context), token_limit=4000)
    input_message = HumanMessage(
        content=f"Context: {truncated_context}\n\nQuestion: {question}"
    )
    response = llm.invoke([input_message])
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=vectorstore.similarity_search(question),
        question=question,
        response=response.content,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = llm.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": question,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
356/74:
from langchain.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_ollama import ChatOllama

def decomposition_of(question):
# Decomposition
    template = """You are a helpful assistant that generates multiple sub-questions related to an input question. \n
    The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \n
    Generate multiple search queries related to: {question} \n
    Output (Just write 3 queries, no another answer such as:Here are..):"""
    prompt_decomposition = ChatPromptTemplate.from_template(template)
    
    #LLM
    llm = ChatOllama(model="llama3.2",temperature=0)

    # Chain
    generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split("\n")))

    # Run
    questions = generate_queries_decomposition.invoke({"question":question})
    return questions
356/75:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from tiktoken import get_encoding
from langchain.schema import HumanMessage
# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
'''
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)
'''
# Memory
#memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore 


# Conversational Retrieval Chain
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context. Your score should be between 0 and 1. Such as 0.45.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.Your score should be between 0 and 1. Such as 0.45.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.Your score should be between 0 and 1. Such as 0.45.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.Your score should be between 0 and 1. Such as 0.45.

---

Evaluate the following inputs:

**Context**:  
{context}

**Question**:  
{question}

**Response**:  
{response}

**Ground Truth**:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.534 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1 
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

MAX_TOKENS = 4096

def truncate_to_token_limit(text, token_limit=MAX_TOKENS, encoding_name="cl100k_base"):
    encoding = get_encoding(encoding_name)
    tokens = encoding.encode(text)
    if len(tokens) > token_limit:
        truncated_tokens = tokens[:token_limit]
        return encoding.decode(truncated_tokens)
    return text


questions_answers = [
    {
        "question": "What are the differences between the RRC_IDLE and RRC_INACTIVE states in terms of UE behavior?",
        "answer": "In RRC_IDLE, the UE camps on a cell, monitors system information, and performs paging while not connected. In RRC_INACTIVE, the UE maintains a context with the network but does not actively transmit or receive, allowing faster resumption of connection. Both states share procedures like PLMN selection and cell reselection."
    },
    {
        "question": "How does the UE determine a 'suitable cell' for camping? Which parameters does it evaluate?",
        "answer": "A 'suitable cell' is determined based on criteria like signal strength (Srxlev), signal quality (Squal), and whether the cell belongs to the selected PLMN or SNPN. The cell must not be barred or restricted."
    }
]
'''
llm = ChatOpenAI(base_url="https://api.sambanova.ai/v1/",
                 api_key = os.environ['SAMBANOVA_API_KEY'],
                 streaming=True,
                 max_tokens=token_size,
                 model = "Meta-Llama-3.2-3B-Instruct")
'''

llm = ChatOllama(
                 model = "llama3.2")


# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma

for qa_pair in questions_answers:
    question = qa_pair["question"]
    ground_truth = qa_pair["answer"]
    # similarity_search Ã§Ä±ktÄ±sÄ±nÄ± stringe dÃ¶nÃ¼ÅtÃ¼r
    retrieved_contexts = vectorstore.similarity_search(question, k=5)
    combined_context = " ".join([doc.page_content for doc in retrieved_contexts])

    truncated_context = truncate_to_token_limit((combined_context), token_limit=4000)
    input_message = HumanMessage(
        content=f"Context: {truncated_context}\n\nQuestion: {question}"
    )
    response = llm.invoke([input_message])
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=vectorstore.similarity_search(question),
        question=question,
        response=response.content,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = llm.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": question,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
356/76:
import csv

# Prepare the header for the CSV file
csv_header = [
    "Question", 
    "Ground Truth", 
    "Context", 
    "Groundedness", 
    "Answer Relevance", 
    "Context Relevance", 
    "Accuracy Compared to Ground Truth"
]

# Define the CSV file path
csv_file_path = "evaluation_results.csv"

# Open the CSV file in write mode
with open(csv_file_path, mode="w", newline="", encoding="utf-8") as file:
    writer = csv.writer(file)
    writer.writerow(csv_header)
    
    # Loop through the evaluation results and write each one to the CSV
    for result in evaluation_results:
        # Extract the scores and context from the evaluation result
        evaluation = result["evaluation"]
        
        # Parse the evaluation string to extract the individual scores
        # Assuming the evaluation string is structured in the format:
        # "Groundedness: X/1 Explanation: ... Answer Relevance: X/1 ..."
        scores = {}
        for line in evaluation.split("\n"):
            if line.startswith("Groundedness"):
                scores["Groundedness"] = line.split(":")[1].strip().split()[0]
            elif line.startswith("Answer Relevance"):
                scores["Answer Relevance"] = line.split(":")[1].strip().split()[0]
            elif line.startswith("Context Relevance"):
                scores["Context Relevance"] = line.split(":")[1].strip().split()[0]
            elif line.startswith("Accuracy Compared to Ground Truth"):
                scores["Accuracy Compared to Ground Truth"] = line.split(":")[1].strip().split()[0]
        
        # Write the row to the CSV
        writer.writerow([
            result["query"], 
            result["ground_truth"], 
            result["response"].content,  # assuming the response content is the string to be saved
            scores.get("Groundedness", ""),
            scores.get("Answer Relevance", ""),
            scores.get("Context Relevance", ""),
            scores.get("Accuracy Compared to Ground Truth", "")
        ])

print(f"Results saved to {csv_file_path}")
356/77:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from tiktoken import get_encoding
from langchain.schema import HumanMessage
# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
'''
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)
'''
# Memory
#memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore 


# Conversational Retrieval Chain
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. **Groundedness**: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context. Your score should be between 0 and 1. Such as 0.45.

2. **Answer Relevance**: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.Your score should be between 0 and 1. Such as 0.45.

3. **Context Relevance**: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.Your score should be between 0 and 1. Such as 0.45.

4. **Accuracy Compared to Ground Truth**:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.Your score should be between 0 and 1. Such as 0.45.

---

Evaluate the following inputs:

Context:  
{context}

Question:  
{question}

Response:  
{response}

Ground Truth:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.534 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1 
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

MAX_TOKENS = 4096

def truncate_to_token_limit(text, token_limit=MAX_TOKENS, encoding_name="cl100k_base"):
    encoding = get_encoding(encoding_name)
    tokens = encoding.encode(text)
    if len(tokens) > token_limit:
        truncated_tokens = tokens[:token_limit]
        return encoding.decode(truncated_tokens)
    return text


questions_answers = [
    {
        "question": "What are the differences between the RRC_IDLE and RRC_INACTIVE states in terms of UE behavior?",
        "answer": "In RRC_IDLE, the UE camps on a cell, monitors system information, and performs paging while not connected. In RRC_INACTIVE, the UE maintains a context with the network but does not actively transmit or receive, allowing faster resumption of connection. Both states share procedures like PLMN selection and cell reselection."
    }
]
'''
llm = ChatOpenAI(base_url="https://api.sambanova.ai/v1/",
                 api_key = os.environ['SAMBANOVA_API_KEY'],
                 streaming=True,
                 max_tokens=token_size,
                 model = "Meta-Llama-3.2-3B-Instruct")
'''

llm = ChatOllama(
                 model = "llama3.2")


# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma

for qa_pair in questions_answers:
    question = qa_pair["question"]
    ground_truth = qa_pair["answer"]
    # similarity_search Ã§Ä±ktÄ±sÄ±nÄ± stringe dÃ¶nÃ¼ÅtÃ¼r
    retrieved_contexts = vectorstore.similarity_search(question, k=5)
    combined_context = " ".join([doc.page_content for doc in retrieved_contexts])

    truncated_context = truncate_to_token_limit((combined_context), token_limit=4000)
    input_message = HumanMessage(
        content=f"Context: {truncated_context}\n\nQuestion: {question}"
    )
    response = llm.invoke([input_message])
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=vectorstore.similarity_search(question),
        question=question,
        response=response.content,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = llm.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": question,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
356/78:
import csv

# Prepare the header for the CSV file
csv_header = [
    "Question", 
    "Ground Truth", 
    "Context", 
    "Groundedness", 
    "Answer Relevance", 
    "Context Relevance", 
    "Accuracy Compared to Ground Truth"
]

# Define the CSV file path
csv_file_path = "evaluation_results.csv"

# Open the CSV file in write mode
with open(csv_file_path, mode="w", newline="", encoding="utf-8") as file:
    writer = csv.writer(file)
    writer.writerow(csv_header)
    
    # Loop through the evaluation results and write each one to the CSV
    for result in evaluation_results:
        # Extract the scores and context from the evaluation result
        evaluation = result["evaluation"]
        
        # Parse the evaluation string to extract the individual scores
        # Assuming the evaluation string is structured in the format:
        # "Groundedness: X/1 Explanation: ... Answer Relevance: X/1 ..."
        scores = {}
        for line in evaluation.split("\n"):
            if line.startswith("Groundedness"):
                scores["Groundedness"] = line.split(":")[1].strip().split()[0]
            elif line.startswith("Answer Relevance"):
                scores["Answer Relevance"] = line.split(":")[1].strip().split()[0]
            elif line.startswith("Context Relevance"):
                scores["Context Relevance"] = line.split(":")[1].strip().split()[0]
            elif line.startswith("Accuracy Compared to Ground Truth"):
                scores["Accuracy Compared to Ground Truth"] = line.split(":")[1].strip().split()[0]
        
        # Write the row to the CSV
        writer.writerow([
            result["query"], 
            result["ground_truth"], 
            result["response"].content,  # assuming the response content is the string to be saved
            scores.get("Groundedness", ""),
            scores.get("Answer Relevance", ""),
            scores.get("Context Relevance", ""),
            scores.get("Accuracy Compared to Ground Truth", "")
        ])

print(f"Results saved to {csv_file_path}")
356/79:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from tiktoken import get_encoding
from langchain.schema import HumanMessage
# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
'''
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)
'''
# Memory
#memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore 


# Conversational Retrieval Chain
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. Groundedness: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context. Your score should be between 0 and 1. Such as 0.45.

2. Answer Relevance: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.Your score should be between 0 and 1. Such as 0.45.

3. Context Relevance: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.Your score should be between 0 and 1. Such as 0.45.

4. Accuracy Compared to Ground Truth:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.Your score should be between 0 and 1. Such as 0.45.

---

Evaluate the following inputs:

Context:  
{context}

Question:  
{question}

Response:  
{response}

Ground Truth:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.534 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1 
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]
"""

MAX_TOKENS = 4096

def truncate_to_token_limit(text, token_limit=MAX_TOKENS, encoding_name="cl100k_base"):
    encoding = get_encoding(encoding_name)
    tokens = encoding.encode(text)
    if len(tokens) > token_limit:
        truncated_tokens = tokens[:token_limit]
        return encoding.decode(truncated_tokens)
    return text


questions_answers = [
    {
        "question": "What are the differences between the RRC_IDLE and RRC_INACTIVE states in terms of UE behavior?",
        "answer": "In RRC_IDLE, the UE camps on a cell, monitors system information, and performs paging while not connected. In RRC_INACTIVE, the UE maintains a context with the network but does not actively transmit or receive, allowing faster resumption of connection. Both states share procedures like PLMN selection and cell reselection."
    }
]
'''
llm = ChatOpenAI(base_url="https://api.sambanova.ai/v1/",
                 api_key = os.environ['SAMBANOVA_API_KEY'],
                 streaming=True,
                 max_tokens=token_size,
                 model = "Meta-Llama-3.2-3B-Instruct")
'''

llm = ChatOllama(
                 model = "llama3.2")


# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma

for qa_pair in questions_answers:
    question = qa_pair["question"]
    ground_truth = qa_pair["answer"]
    # similarity_search Ã§Ä±ktÄ±sÄ±nÄ± stringe dÃ¶nÃ¼ÅtÃ¼r
    retrieved_contexts = vectorstore.similarity_search(question, k=5)
    combined_context = " ".join([doc.page_content for doc in retrieved_contexts])

    truncated_context = truncate_to_token_limit((combined_context), token_limit=4000)
    input_message = HumanMessage(
        content=f"Context: {truncated_context}\n\nQuestion: {question}"
    )
    response = llm.invoke([input_message])
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=vectorstore.similarity_search(question),
        question=question,
        response=response.content,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = llm.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": question,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
356/80:
import csv

# Prepare the header for the CSV file
csv_header = [
    "Question", 
    "Ground Truth", 
    "Context", 
    "Groundedness", 
    "Answer Relevance", 
    "Context Relevance", 
    "Accuracy Compared to Ground Truth"
]

# Define the CSV file path
csv_file_path = "evaluation_results.csv"

# Open the CSV file in write mode
with open(csv_file_path, mode="w", newline="", encoding="utf-8") as file:
    writer = csv.writer(file)
    writer.writerow(csv_header)
    
    # Loop through the evaluation results and write each one to the CSV
    for result in evaluation_results:
        # Extract the scores and context from the evaluation result
        evaluation = result["evaluation"]
        
        # Parse the evaluation string to extract the individual scores
        # Assuming the evaluation string is structured in the format:
        # "Groundedness: X/1 Explanation: ... Answer Relevance: X/1 ..."
        scores = {}
        for line in evaluation.split("\n"):
            if line.startswith("Groundedness"):
                scores["Groundedness"] = line.split(":")[1].strip().split()[0]
            elif line.startswith("Answer Relevance"):
                scores["Answer Relevance"] = line.split(":")[1].strip().split()[0]
            elif line.startswith("Context Relevance"):
                scores["Context Relevance"] = line.split(":")[1].strip().split()[0]
            elif line.startswith("Accuracy Compared to Ground Truth"):
                scores["Accuracy Compared to Ground Truth"] = line.split(":")[1].strip().split()[0]
        
        # Write the row to the CSV
        writer.writerow([
            result["query"], 
            result["ground_truth"], 
            result["response"].content,  # assuming the response content is the string to be saved
            scores.get("Groundedness", ""),
            scores.get("Answer Relevance", ""),
            scores.get("Context Relevance", ""),
            scores.get("Accuracy Compared to Ground Truth", "")
        ])

print(f"Results saved to {csv_file_path}")
356/81:
from langchain_ollama import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from tiktoken import get_encoding
from langchain.schema import HumanMessage
# LLM iÃ§in OpenAI GPT 4O modelini tanÄ±mlÄ±yoruz
'''
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)
'''
# Memory
#memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# Retriever - VectorStore 


# Conversational Retrieval Chain
#conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

# DeÄerlendirme prompt Åablonu
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator for RAG systems. Evaluate the response using the following criteria:

1. Groundedness: 
   - Does the response rely on and accurately use the provided context? 
   - A high score (close to 1) means the response is fully grounded in the given context, while a low score (close to 0) indicates that the response includes information that is not present or contradicts the context. Your score should be between 0 and 1. Such as 0.45.

2. Answer Relevance: 
   - Is the response relevant to and directly answering the question?
   - A high score (close to 1) means the response is precise, well-aligned with the question, and provides a clear answer. A low score (close to 0) indicates the response is irrelevant or off-topic.Your score should be between 0 and 1. Such as 0.45.

3. Context Relevance: 
   - Does the provided context contain information that is useful and directly related to answering the question?
   - A high score (close to 1) means the context fully supports answering the question, while a low score (close to 0) means the context is unrelated or insufficient for the question.Your score should be between 0 and 1. Such as 0.45.

4. Accuracy Compared to Ground Truth:
   - How close is the response to the ground truth answer provided? 
   - A high score (close to 1) means the response matches the ground truth closely, while a low score (close to 0) indicates significant deviations.Your score should be between 0 and 1. Such as 0.45.

---

Evaluate the following inputs:

Context:  
{context}

Question:  
{question}

Response:  
{response}

Ground Truth:  
{ground_truth}

---

Provide a float score between 0 and 1 such as 0.534 for each criterion along with a short justification for your score. The output should follow this format:

Groundedness: X/1
Answer Relevance: X/1
Context Relevance: X/1 

Groundedness: X/1  
Explanation: [Your explanation]

Answer Relevance: X/1 
Explanation: [Your explanation]

Context Relevance: X/1  
Explanation: [Your explanation]

Accuracy Compared to Ground Truth: X/1  
Explanation: [Your explanation]

Groundedness: X/1
Answer Relevance: X/1
Context Relevance: X/1 
"""

MAX_TOKENS = 4096

def truncate_to_token_limit(text, token_limit=MAX_TOKENS, encoding_name="cl100k_base"):
    encoding = get_encoding(encoding_name)
    tokens = encoding.encode(text)
    if len(tokens) > token_limit:
        truncated_tokens = tokens[:token_limit]
        return encoding.decode(truncated_tokens)
    return text


questions_answers = [
    {
        "question": "What are the differences between the RRC_IDLE and RRC_INACTIVE states in terms of UE behavior?",
        "answer": "In RRC_IDLE, the UE camps on a cell, monitors system information, and performs paging while not connected. In RRC_INACTIVE, the UE maintains a context with the network but does not actively transmit or receive, allowing faster resumption of connection. Both states share procedures like PLMN selection and cell reselection."
    }
]
'''
llm = ChatOpenAI(base_url="https://api.sambanova.ai/v1/",
                 api_key = os.environ['SAMBANOVA_API_KEY'],
                 streaming=True,
                 max_tokens=token_size,
                 model = "Meta-Llama-3.2-3B-Instruct")
'''

llm = ChatOllama(
                 model = "llama3.2")


# SonuÃ§larÄ± saklamak iÃ§in bir liste
evaluation_results = []

# Her bir input Ã¼zerinde iÅlem yapma

for qa_pair in questions_answers:
    question = qa_pair["question"]
    ground_truth = qa_pair["answer"]
    # similarity_search Ã§Ä±ktÄ±sÄ±nÄ± stringe dÃ¶nÃ¼ÅtÃ¼r
    retrieved_contexts = vectorstore.similarity_search(question, k=5)
    combined_context = " ".join([doc.page_content for doc in retrieved_contexts])

    truncated_context = truncate_to_token_limit((combined_context), token_limit=4000)
    input_message = HumanMessage(
        content=f"Context: {truncated_context}\n\nQuestion: {question}"
    )
    response = llm.invoke([input_message])
    # DeÄerlendirme prompt'unu hazÄ±rlama
    grader_prompt = GRADER_PROMPT_TEMPLATE.format(
        context=vectorstore.similarity_search(question),
        question=question,
        response=response.content,
        ground_truth=ground_truth
    )
    
    # DeÄerlendirme iÃ§in LLM Ã§aÄrÄ±sÄ±
    evaluation_result = llm.invoke(grader_prompt)
    
    # SonuÃ§larÄ± saklama
    evaluation_results.append({
        "query": question,
        "response": response,
        "ground_truth": ground_truth,
        "evaluation": evaluation_result.content
    })

# SonuÃ§larÄ± yazdÄ±r
for result in evaluation_results:
    print(f"Query: {result['query']}")
    print(f"Model Response: {result['response']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print("Evaluation:")
    print(result["evaluation"])
    print("-" * 50)
356/82:
import csv

# Prepare the header for the CSV file
csv_header = [
    "Question", 
    "Ground Truth", 
    "Context", 
    "Groundedness", 
    "Answer Relevance", 
    "Context Relevance", 
    "Accuracy Compared to Ground Truth"
]

# Define the CSV file path
csv_file_path = "evaluation_results.csv"

# Open the CSV file in write mode
with open(csv_file_path, mode="w", newline="", encoding="utf-8") as file:
    writer = csv.writer(file)
    writer.writerow(csv_header)
    
    # Loop through the evaluation results and write each one to the CSV
    for result in evaluation_results:
        # Extract the scores and context from the evaluation result
        evaluation = result["evaluation"]
        
        # Parse the evaluation string to extract the individual scores
        # Assuming the evaluation string is structured in the format:
        # "Groundedness: X/1 Explanation: ... Answer Relevance: X/1 ..."
        scores = {}
        for line in evaluation.split("\n"):
            if line.startswith("**Groundedness"):
                scores["Groundedness"] = line.split(":")[1].strip().split()[0]
            elif line.startswith("**Answer Relevance"):
                scores["Answer Relevance"] = line.split(":")[1].strip().split()[0]
            elif line.startswith("**Context Relevance"):
                scores["Context Relevance"] = line.split(":")[1].strip().split()[0]
            elif line.startswith("**Accuracy Compared to Ground Truth"):
                scores["Accuracy Compared to Ground Truth"] = line.split(":")[1].strip().split()[0]
        
        # Write the row to the CSV
        writer.writerow([
            result["query"], 
            result["ground_truth"], 
            result["response"].content,  # assuming the response content is the string to be saved
            scores.get("Groundedness", ""),
            scores.get("Answer Relevance", ""),
            scores.get("Context Relevance", ""),
            scores.get("Accuracy Compared to Ground Truth", "")
        ])

print(f"Results saved to {csv_file_path}")
356/83:
import csv

# Prepare the header for the CSV file
csv_header = [
    "Question", 
    "Ground Truth", 
    "Context", 
    "Groundedness", 
    "Answer Relevance", 
    "Context Relevance", 
    "Accuracy Compared to Ground Truth"
]

# Define the CSV file path
csv_file_path = "evaluation_results.csv"

# Open the CSV file in write mode
with open(csv_file_path, mode="w", newline="", encoding="utf-8") as file:
    writer = csv.writer(file)
    writer.writerow(csv_header)
    
    # Loop through the evaluation results and write each one to the CSV
    for result in evaluation_results:
        # Extract the scores and context from the evaluation result
        evaluation = result["evaluation"]
        
        # Parse the evaluation string to extract the individual scores
        # Assuming the evaluation string is structured in the format:
        # "Groundedness: X/1 Explanation: ... Answer Relevance: X/1 ..."
        scores = {}
        for line in evaluation.split("\n"):
            if line.startswith("**Groundedness"):
                scores["Groundedness"] = line.split(":")[1].strip("* ").split()[0]
            elif line.startswith("**Answer Relevance"):
                scores["Answer Relevance"] = line.split(":")[1].strip().split()[0]
            elif line.startswith("**Context Relevance"):
                scores["Context Relevance"] = line.split(":")[1].strip().split()[0]
            elif line.startswith("**Accuracy Compared to Ground Truth"):
                scores["Accuracy Compared to Ground Truth"] = line.split(":")[1].strip().split()[0]
        
        # Write the row to the CSV
        writer.writerow([
            result["query"], 
            result["ground_truth"], 
            result["response"].content,  # assuming the response content is the string to be saved
            scores.get("Groundedness", ""),
            scores.get("Answer Relevance", ""),
            scores.get("Context Relevance", ""),
            scores.get("Accuracy Compared to Ground Truth", "")
        ])

print(f"Results saved to {csv_file_path}")
356/84:
import csv

# Prepare the header for the CSV file
csv_header = [
    "Question", 
    "Ground Truth", 
    "Context", 
    "Groundedness", 
    "Answer Relevance", 
    "Context Relevance", 
    "Accuracy Compared to Ground Truth"
]

# Define the CSV file path
csv_file_path = "evaluation_results.csv"

# Open the CSV file in write mode
with open(csv_file_path, mode="w", newline="", encoding="utf-8") as file:
    writer = csv.writer(file)
    writer.writerow(csv_header)
    
    # Loop through the evaluation results and write each one to the CSV
    for result in evaluation_results:
        # Extract the scores and context from the evaluation result
        evaluation = result["evaluation"]
        
        # Parse the evaluation string to extract the individual scores
        # Assuming the evaluation string is structured in the format:
        # "Groundedness: X/1 Explanation: ... Answer Relevance: X/1 ..."
        scores = {}
        for line in evaluation.split("\n"):
            if line.startswith("**Groundedness"):
                scores["Groundedness"] = line.split(":")[1].strip("* ").split()[0]
            elif line.startswith("**Answer Relevance"):
                scores["Answer Relevance"] = line.split(":")[1].strip().split()[0]
            elif line.startswith("**Context Relevance"):
                scores["Context Relevance"] = line.split(":")[1].strip().split()[0]
            elif line.startswith("**Accuracy Compared to Ground Truth"):
                scores["Accuracy Compared to Ground Truth"] = line.split(":")[1].strip().split()[0]
        
        # Write the row to the CSV
        writer.writerow([
            result["query"], 
            result["ground_truth"], 
            result["response"].content,  # assuming the response content is the string to be saved
            scores.get("Groundedness", ""),
            scores.get("Answer Relevance", ""),
            scores.get("Context Relevance", ""),
            scores.get("Accuracy Compared to Ground Truth", "")
        ])

print(f"Results saved to {csv_file_path}")
   1: %history
   2: %history
   3: %history -g -f Untitled1.ipynb
